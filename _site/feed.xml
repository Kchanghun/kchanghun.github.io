<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://192.168.0.41:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://192.168.0.41:4000/" rel="alternate" type="text/html" /><updated>2023-01-05T21:22:29+09:00</updated><id>http://192.168.0.41:4000/feed.xml</id><title type="html">No Free Knowledge</title><subtitle>There is No Free Knowledge when we study something.</subtitle><author><name>Chang Hun Kang</name></author><entry><title type="html">Convolutional RNN: an Enhanced Model for Extracting Feauters from Sequential Data</title><link href="http://192.168.0.41:4000/paper%20review/2023/01/05/Ensemble-Adversarial-Training-Attacks-And-Defenses.html" rel="alternate" type="text/html" title="Convolutional RNN: an Enhanced Model for Extracting Feauters from Sequential Data" /><published>2023-01-05T15:20:27+09:00</published><updated>2023-01-05T15:20:27+09:00</updated><id>http://192.168.0.41:4000/paper%20review/2023/01/05/Ensemble-Adversarial-Training-Attacks-And-Defenses</id><content type="html" xml:base="http://192.168.0.41:4000/paper%20review/2023/01/05/Ensemble-Adversarial-Training-Attacks-And-Defenses.html"><![CDATA[<h1 id="abstract">Abstract</h1>
<p>전통적인 합성곱층들은 입력값에 affine 함수를 통해 비선형성을 적용해 데이터의 패치들로부터 특징을 추출한다.<br />
우리는 이러한 특징 추출 과정에서 데이터의 패치들을 RNN에 feeding하고 그것의 output이나 hidden state들을<br />
추출된 특징으로 사용하여 더 향상된 모델을 제안한다.<br />
그렇게 해서 우리는 sequential data가 가지고 있는 몇개의 프레임들이 포함된 window를 그 자체가 sequential<br />
인 것으로 간주하여 이러한 추가적인 구조가 데이터의 중요한 정보를 잃지 않을 것이라는 사실을 이용 할 것이다.<br />
추가적으로 우리는 비선형성이 도입된 affine 함수는 매우 간단한 특징만 얻을 수 있으므로 affine과 비슷한<br />
잠재적 이점을 가진 몇가지 추가적인 계산 과정을 허용 할 것이다.<br />
우리는 convolutional rnn을 사용하여 기존의 cnn을 통한 결과보다 더 좋은 성능을 two audio 분류 작업에서<br />
얻을 수 있었다. 해당 모델의 Tensorflow 코드는 다음에서 확인 할 수 있다.<br />
https://github.com/cruvadom/Convolutional-RNN</p>

<h1 id="i-introduction">I. Introduction</h1>
<p>지난 몇년 동안, CNN은 object classification, traffic sign recognition과 image caption generation<br />
등 computer vision의 광범위한 작업에서 가장 좋은 결과를 얻어 왔다.<br />
합성곱 층을 어떤 데이터에 적용하면 이것은 국소적인 패치들로부터 특징을 추출하고 종종 pooling 메카니즘을<br />
따라 이웃한 패치들끼리 값을 pool하도록 한다.<br />
추출된 특징들은 신경망의 다음 층의 입력이 될 수 있는데 다음 층은 또 다른 합성곱 층이 될 수도 있고 분류기가<br />
될 수도 있다.<br />
합성곱층을 raw data로부터 특징을 얻기 위해서 사용하는 모델들은 알고리즘을 직접 만들어서 특징을 추출하는<br />
모델과 비교하여 더 뛰어난 성능을 보일 수 있고 SOTA 결과를 달성할 수 있다.<br />
<br />
RNN은 보통 다양한 길이를 갖는 text나 sound 데이터와 같은 sequential 데이터를 처리하기 위한 모델이다.<br />
LSTM은 rnn의 특정한 종류로 gating 메카니즘을 통해 long-term dependencies를 잘 처리 할 수 있다.<br />
LSTM은 음성 인식이나 machine translation 분야에서 성공적으로 사용되어 왔다.<br />
<br />
데이터의 길이가 다양하고 프레임의 sequence 형태를 보일 때 합성곱 층과 recurrent 층을 모두 포함하는<br />
모델은 다음과 같이 만들어질 수 있다 : 합성곱층은 sequence 상에서 연속되는 프레임들로 구성된 윈도우와 같은<br />
패치들로부터 특징을 추춣하는 데에 사용된다. 윈도우를 통해 추출된 특징들은 다른 합성곱층의 입력이 되거나<br />
rnn의 입력이 될 수 있는 또 다른 sequence 이다.<br />
또 한가지 이점이 있는데 합성곱 층 다음에 pooling 메카니즘을 사용하면 recurrent 층의 입력 sequence를<br />
줄일 수 있다는 것이다. 그렇게 하면 recurrent layer는 적은 수의 프레임들 사이에서 temporal dependencies 
만 처리하면 된다.<br />
<br />
이전의 합성곱 층의 가능한 제약은 그들이 affine 함수를 적용해 데이터로부터 비선형적인 특징을 추출한다는<br />
것이다. 특히, 특징은 element-wise multiplication을 통해 weight matrix, summing all elements, adding bais,<br />
그리고 비선형 함수를 적용해 추출된다. 이론적으로, 데이터 패치를 scalar 특징 값으로 매핑하는 함수는<br />
복잡도가 일정하지 않고 입력 데이터를 더 잘 표현하기 위해서 더 복잡한 비선형 함수가 사용될 수 있다.<br />
단순히 커널의 크기가 1x1보다 큰 합성곱 층을 더 많이 stack하는 것은 이 문제를 해결 할 수 없다.<br />
왜냐하면 결과적으로 레이어는 다른 위치에 있는 이전 층의 결과를 섞을 것이고 이 패치 자체만을 사용하는 것은<br />
더 복잡한 특징을 추출하지 못할 것이기 때문이다.
<br />
Sequential data의 경우 몇가지 연속되는 데이터 프레임들의 윈도우들은 추가적인 특성을 갖는다.<br />
모든 위도우는 그자체로 몇가지 프레임들의 작은 sequence를 나타낸다. 이러한 특성은 잠재적으로<br />
윈도우로부터 더 좋은 특징을 추출하기 위해 보여질 수 있다. 이번 연구에서 우리는 Convolutional Recurrent Neural Network(이하 CRNN) 모델을소개할것이다. CRNN은 먼저 모든 윈도우를 frame by frame으로 recurrent 층에 feed하고 각 연속적인 윈도우로부터 추출한 특징으로써 recurrent 층으로부터 얻은 output이나 hidden state를 사용한다. 그 결과 우리가 각 윈도우로부터 일시적인 추가적인 정보를 사용하고 특징들이 다른<br />
윈도우로부터 convolutional layer와 비교해 더 복잡한 방법으로 만들어지기 때문에 기존의<br />
convolutional layer와 비교해 잠재적으로 더 좋은 특징을 추출할 수 있을것이라는 거다. 
<br />
우리는 기존의 convolutional layer와 비교해 더 향상된 분류 결과를 얻기 위해 다양한 방법으로 모델을 만들어<br />
오디오 분류를 수행했다. 논문의 나머지 부분에서는 비슷한 연구를 논하고 그다음 recurrent layer에서 어떻게<br />
특징을 추출했는지 설명하고 오디오 데이터 분류를 우리가 제안한 모델들에 대해 수행한 다음 관찰된 결과를<br />
어떻게 분석할 수 있는지 말한다음 마지막으로 연구 결과를 설명할 것이다.<br />
<br />
<br /></p>
<h1 id="ii-related-work">II. Related Work</h1>
<p>hi</p>]]></content><author><name>Gil Keren, Bjorn Schuller</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[Abstract 전통적인 합성곱층들은 입력값에 affine 함수를 통해 비선형성을 적용해 데이터의 패치들로부터 특징을 추출한다. 우리는 이러한 특징 추출 과정에서 데이터의 패치들을 RNN에 feeding하고 그것의 output이나 hidden state들을 추출된 특징으로 사용하여 더 향상된 모델을 제안한다. 그렇게 해서 우리는 sequential data가 가지고 있는 몇개의 프레임들이 포함된 window를 그 자체가 sequential 인 것으로 간주하여 이러한 추가적인 구조가 데이터의 중요한 정보를 잃지 않을 것이라는 사실을 이용 할 것이다. 추가적으로 우리는 비선형성이 도입된 affine 함수는 매우 간단한 특징만 얻을 수 있으므로 affine과 비슷한 잠재적 이점을 가진 몇가지 추가적인 계산 과정을 허용 할 것이다. 우리는 convolutional rnn을 사용하여 기존의 cnn을 통한 결과보다 더 좋은 성능을 two audio 분류 작업에서 얻을 수 있었다. 해당 모델의 Tensorflow 코드는 다음에서 확인 할 수 있다. https://github.com/cruvadom/Convolutional-RNN]]></summary></entry><entry><title type="html">Emotion Recognition From EEG Signal Focusing on Deep Learning and Shallow Learning Techniques</title><link href="http://192.168.0.41:4000/paper%20review/2022/10/05/Emotion-Recognition-from-EEG-signal-focusing-on-Deep-Learning-and-Shallow-Learning-Techniques.html" rel="alternate" type="text/html" title="Emotion Recognition From EEG Signal Focusing on Deep Learning and Shallow Learning Techniques" /><published>2022-10-05T06:43:27+09:00</published><updated>2022-10-05T06:43:27+09:00</updated><id>http://192.168.0.41:4000/paper%20review/2022/10/05/Emotion-Recognition-from-EEG-signal-focusing-on-Deep-Learning-and-Shallow-Learning-Techniques</id><content type="html" xml:base="http://192.168.0.41:4000/paper%20review/2022/10/05/Emotion-Recognition-from-EEG-signal-focusing-on-Deep-Learning-and-Shallow-Learning-Techniques.html"><![CDATA[<h1 id="abstract">Abstract</h1>
<p>최근 <strong>electoencephalogram 기반의 감정 인식</strong>은 <strong>Human-Computer Interaction(HCI)</strong> 시스템이<br />
더 지능화 되게 해주는 덕에 중요하게 평가되고있다.<br />
Person-based decision making, mind-machine interfacing, cognitive interaction, affect detection,<br />
feeling detection 등과 같이 감정 인식 기술의 놀라운 적용들 때문에<br />
감정 인식이 과대 평가로 최근 AI 기반 연구들을 끌어들이는데 성공했다.<br />
그러므로 많은 연구들은 어떤 작업에서 그것에 맞는 feature set들과 기술들로<br />
방법론적 시스템 상의 검토가 이루어지는 다양한 방법의 접근을 해보는 방법으로 많은 연구가 되고 있다.<br />
이것은 효과적인 감정 인식 시스템을 구성하는 지침서로 작용해 초보자들이 학습하도록 용이하게 쓰일 것이다.<br />
이 논문에서 우리는 최근 출판된 감정 인식에서 최신 기술에 대한 엄격한 검토를 수행하고<br />
적절한 프레임워크를 개발하도록 핵심적인 내용을 제공하도록 몇몇 공통된 감정인식의 단계들을<br />
관련된 정의와 관련된 이론과 관련된 분석들을 요약할 것이다.<br />
게다가 여기서 진행한 연구에는 두개의 카테고리로 이분화 된다.</p>
<ul>
  <li>Deep Learning Based Emotion Recognition system</li>
  <li>Shallow Machine Learning Based Emotion Recognition system</li>
</ul>

<p>검토된 시스템들은 방법, 분류기, 분류된 감정 수, 정확도 그리고 사용된 데이터셋을 기반으로 비교되었다.<br />
앞으로 연구자들에게 방향성을 제공하기 위해 유익한 비교, 최근 연구 경향 그리고 몇 추천들을 제공할 것이다.</p>

<p><br /></p>
<h1 id="index-terms">Index Terms</h1>
<p>Emotion, Electroencephalogram, human0computer interaction, deep learning, shallow learning</p>

<p><br /></p>
<h1 id="1-introduction">1. Introduction</h1>
<p>Emotion recognition은 현재 인간의 마음 상태 또는 정신의 상태를 이해하거나 추출하는 과정이다.<br />
최근 몇년동안 뇌 신호로부터 감정을 인식하는 연구들이 상당히 많이 진행되었다.<br />
<strong>Artificial Inelligence(AI)</strong>의 발전으로, 감정인식은<br />
neuroscience, computer science, cognitive science와 medical science에서 없어서는 안될 분야로 자리잡았다.<br />
비록 인간의 표정, 몸의 움직임, 제스처 등등에 대한 표현이 그 사람의 감정적 상태를 표현하지만,<br />
자연적으로 발생하는 뇌의 <strong>Electroencephalogram(EEG)</strong> 신호로부터 원시 감정을 추출하는 것도 중요하다.<br />
왜냐하면 인간의 생각, 사상, 꿈과 계획과 같은 타입들은 뇌의 신호에 대한 정보에서 의미를 가지고 지표를 나타낸다.<br />
게다가, 피험자(subject)는 자연적으로 발생한 EEG 신호를 조작할 방법이 없다.<br />
게다가, 음성, 제스쳐와 자세를 기반으로한 감정 인식은 생각을 잘 표현하기 어렵거나<br />
육체적으로 그런 것들이 불가능한 사람들에게는 사용될 수 없다.<br />
그러므로, EEG는 인간의 감정을 추출하는 적절한 수단이고 이미 인간 감정을 이해하는 많은 연구에 사용되고 있다.</p>

<p>EEG를 사용한 감정인식은 흥미롭고 빠르게 성장하는 분야이다.<br />
결과적으로, 매해 관련된 논문의 출판이 지속적으로 증가하고 있다.<br />
PubMed 웹사이트에서 ‘Emotion recognition from EEG’라고 검색하여 얻은 출판물의 수를 데이터로 사용했다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_0.png" /></p>

<p><br />
그러나 EEG는 진폭이 매우 작은 신호이기 때문에 EEG로부터 감정을 인식한다는 것은 매우 도전적인 일이다.<br />
그럼에도 많은 연구자들은 감정만 인식할 수 있도록 추출된 특징들의 조합이나 원시 신호에<br />
딥러닝과 얕은 기계학습 기반의 정교한 기술들을 적용해 이러한 문제들을 해결하고 있다.</p>

<p>이번 논문에서 많은 감정 인식 시스템들을 분석했다.<br />
기본적으로 EEG를 통한 감정 분석 시스템의 전체적인 시스템은 크게 아래 두가지로 나뉜다.</p>
<ul>
  <li>Deep machine learning-based system</li>
  <li>Shallow machine learning-based system</li>
</ul>

<p>CNN, DNN, DBN, RNN, BDAE, VEn 등등의 Deep learning-based system들은<br />
분류기로 사용된다.<br />
반면 SVM, kNN, RF, DT, ANN, PNN, MLP 등등의<br />
shallow learning-based system들도 분류기로 사용된다.<br />
이 연구는 두가지 타입 시스템들의 전반적인 성능을 증명할 것이다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_1.png" /></p>

<p>이번 review를 위해 문헌 검색은 아래와 같이 PRISMA(Preferred Reporting Items for Systematic Reviews and Meta-Analyses)전략을 따랐다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_2.png" /></p>
<p>먼저, identification step에서 ‘Google Scholar’와 ‘PubMed’에서<br />
‘Emotion recognition from EEG’를 2015~2020년 기간 안에서 검색하여 762개를 선별했다.<br />
다음에는 screening step에서 non-English와 review, database, letter type을 배제했다.<br />
그 다음 제목과 abstract를 하나 하나 확인하여 169개의 연관없는 것과 48개의 정확하지 않은 것들을 제외시켰다.<br />
그 결과 72개의 문서가 남았고 마지막으로 31개의 문서들이 정확한 감정 상태에 대한 기술이 없고<br />
feature 추출 방식이나 분류 알고리즘이 기술되어있지 않아서 배제되었다.<br />
이 과정들을 통해 최종적으로 41개의 문서가 남아 이번 논문에서 review를 수행했다.</p>

<p>이번 review에서 아래와 같은 내용을 기술할 것이다.</p>
<ul>
  <li>감정, EEG 신호 분석, 필요한 소프트웨어, 이용가능한 데이터셋, 유명한 feature들과 분류기에 대한 소개내용들을<br />
입문 연구자들에게 유용한 핵심 내용을 제공하기 위해 철저하게 묘사한다.</li>
  <li>Deep learning과 얕은 machine learning 기반의 분류기 알고리즘에 대한 성능 비교는<br />
중간 단계의 연구자들이 진보된 연구 방향을 찾도록 도움을 줄 수 있다.</li>
  <li>매우 연관성 있는 자료들을 그것들의 한계와 추천 목록을 제공하여 전문가 수준의 연구자들이<br />
실제 세상에 적용할 수 있는 감정 인식 시스템을 완벽하게 설계하도록 수월하게 한다.</li>
</ul>

<p>이 논문의 배열은 아래와 같다.<br />
감정, EEG, EEG data를 얻는 기술들과 분석하는 소프트웨어에 대한 짧은 소개를 “Overview” 부분에서 한다.<br />
단계별 과정, feature와 분류기 등등을 “General Framework for Emotion Recognition” 부분에서 소개한다.<br />
다음으로 deep learning과 shallow learning 기반 인식 시스템의 성능 비교를 “Discussion” 부분에서 묘사한다.<br />
“Observations and Recommendations” 섹션을 마지막으로 논문의 결론을 제시한다.</p>

<p><br /></p>
<h1 id="ii-overview">II. Overview</h1>
<h2 id="a-emotion">A. Emotion</h2>
<p>감정은 우리가 특정 상황에서 어떻게 행동할지를 함축하고 있는 자발적인 느낌이다.<br />
서로 다른 상황에서 사람은 행복, 두려움, 분노, 지루함 등등 서로 다른 감정을 느낄 것이다.<br />
사람의 특성은 현재의 마음 상태, 행동과 감정으로 정의 될 수 있는 생각을 나타낼 수 있고<br />
이것은 정확하게 양적으로 정의를 내릴 수 없다.<br />
그러나 최근 연구 자료들에 따르면 사람들의 가장 공통되는 감정 상태의 타입들은<br />
amusement, boredom, disgust, exitement, joy, satisfaction, sympathy, romance, horror,<br />
entrancement, confusion, awe, nostalgia, fear, empathetic, calmness, anxiety, admiration,<br />
awkwardness, triumph, sadness, interest, envy, craving, adoration 등등 이다.<br />
감정들을 분류하거나 조합하기 위해서 많은 심리학자들은 아래와 같이 많은 모델들을 제안했다.<br />
이들중 Russel의 Circumplex 2D model이 유명하다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_3.png" /></p>

<p><br />
차원에 따라 감정 모델은 2D 모델과 3D모델로 분류될 수 있다.<br />
2D 모델은 감정가(Valence, 특정 사건마다 고유하게 가지고 있는 이끌림, positive valence와 negative valence)<br />
와 각성 값을 기반으로 아래와 같이 감정을 분류한다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_4.png" /></p>

<p><br />
그러나 아래와 같은 3D 모델은 valence와 arousal과 dominance를 고려한다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_5.png" /></p>
<ul>
  <li><strong>Valence</strong>   : pleasure에 대한 지표</li>
  <li><strong>Arousal</strong>   : 자극에 대한 지표</li>
  <li><strong>Dominance</strong> : 자연적인 감정을 조절하는 지표</li>
</ul>

<p><br /></p>
<h2 id="b-electroencephalogrameeg">B. Electroencephalogram(EEG)</h2>
<p>Eletroencephalogram은 인간의 두피로부터 일정 시간동안 뇌의 전기적 활동을 기록하는 파형 기록 시스템이다.<br />
이것은 뇌의 뉴런들을 통해 이온들의 흐름으로 인해 발생한 microvolt 범위를 갖는 전압의 fluctuation을 측정한다.<br />
EEG 데이터를 적절하고 충분히 기록하기 위해서는 인간의 뇌 해부학을 알아야 한다.<br />
Central Nervous System(CNS)의 중심으로 여겨지는 뇌는 세개의 부분이 합쳐져있는데<br />
Cerebrum(대뇌), Cerebellum(소뇌) 과 Brainstem(뇌간, 뇌 줄기)이다.<br />
세개의 부분 중에 cerebrum은 가장 크고 우반구와 좌반구로 구성된다.<br />
그러나 대뇌 반구는 Frontal lobe, Parietal lobe, Temporal lobe과 Occipital lobe으로 구성된다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_6.png" /></p>
<p><br />
EEG 신호들은 5개의 하위 band 신호들로 구성되는데 delta, theta, alpha, beta와 gamma라고 부르며<br />
이것들은 mental 상태와 조건과 관련이 있다.<br />
이름과 그에 맞는 위치, 주파수 영역과 뇌의 활동은 아래와 같다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_7.png" /></p>

<p><br /></p>
<h2 id="c-eeg-data-acquisition-techniques">C. EEG Data Acquisition Techniques</h2>
<p>EEG data를 기록하기 위해 몇가지 표준적인 취득 시스템들이 있다.<br />
Biosemi ActiveTwo, Emotive Epoc+ headset, Brain Vision LLC, EEG module of Neuroscan,<br />
Mobita 32-channel wireless EEG sytem, etc.<br />
이것들 중에 Biosemi Active Two 시스템은 더 친숙하고 꽤 인기있다.<br />
게다가 최근 16.1%의 경우들이 착욕가능하고 휴대 가능하고 무선이고 비용이 적어서<br />
실무적으로 매혹적인 Emotive Epoc+ headset을 사용해 데이터를 취합했다.</p>

<p>EEG data 취합 과정은 electrode(channel)의 수, 두피의 electrode 위치, 자극의 종류,<br />
기록된 주파수의 종류, 신호를 취합하는 장치에 따라 특성을 갖게 된다.<br />
International 10-20 eletrode placement system은 감정 인식에서 매우 공통적으로 사용된다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_8.png" /></p>
<p><br />
감정적 사진, 음성, 영상, 시청각 영화 클립이나 모든 감정적 작업들 또는 사건들은<br />
EEG 신호를 기록할 때 자극으로 사용된다.<br />
감정과 관련된 연구에서 시청각 자료나 영화 클립들은 아주 좋은 자극이 된다.</p>

<p><br /></p>
<h2 id="d-software-for-eeg-signal-analysis">D. Software for EEG Signal Analysis</h2>
<p>EEG 신호 기반의 연구들은 날이 갈수록 더 매력적이게 된다.<br />
그러나, 입문 단계의 연구자들은 EEG 신호 분석을 하는 소프트웨어 패키지나 tool들을 찾는데에 많은 노력을 들여야 한다.<br />
그러므로 몇가지 잘 알려진 software 패키지나 툴들을 아래에 제공하겠다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_9.png" /></p>

<p><br /></p>
<h1 id="iii-general-framework-for-emotion-recognition">III. General Framework for Emotion Recognition</h1>
<p>이번 장에서는 전반적인 EEG 신호를 통한 감정 인식 구조를 살펴볼 것이다.<br />
그 과정에서 다음 내용들을 포함 할것이다.<br />
Data management, Preprocessing, Feature Extraction,<br />
Feature Selection, and Classification algorithms 등.<br />
이 과정들은 아래에서 단계별로 묘사했다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_10.png" /></p>
<p><br />
앞서 선별한 41개의 자료들중에 85%에 해당하는 35개의 자료들이 공공 Open data를 사용했고<br />
나머지 15%에 해당하는 6개의 자료들은 직접 만든 데이터셋을 사용했다.<br />
공공데이터를 사용한 경우는 그중에 61%가 DEAP, 7%가 SEED, 2%가 MAHNOB를 사용했으며 나머지는<br />
다른 데이터셋을 사용했다.<br />
Feature의 경우 25%에 해당하는 10개의 자료들이 time-domain feature를 사용하고<br />
51%에 해당하는 21개의 자료들이 frequency-domain feature를 사용했고,<br />
10%에 해당하는 4개의 자료들이 time-frequency domain feature를 사용했고,<br />
나머지 14%에 해당하는 6개의 자료들은 다른 feature를 사용했다.<br />
몇몇 학자들은 여러가지 종류의 feature를 사용했는데 주된 feature를 고려해서 통계를 냈다.</p>

<p>Feature Extraction을 하기 위해 5개의 작업에서 Short-Time Fourier Transform(STFT),<br />
2개의 작업에서 wavelet transform, 1개의 작업은 Discrete Cosine Transform(DCT),<br />
3개의 작업에서 Higher-Order Crossing(HOC), 2개의 작업에서 Hilbert-Huang Transform(HHT),<br />
나머지는 6개의 다른 방법을 사용했다.<br />
몇몇 저자들은 간단한 계산을 사용하거나 특정한 방법을 기술하지 않았다.<br />
마지막으로 분류 알고리즘으로 44%에 해당하는 18개의 자료들은 Deep learning기반을 사용했고<br />
56%에 해당하는 23개의 자료들은 shallow learning techniques를 기반으로 사용했다.<br />
전체의 44%인 Deep learning 기반의 분류기에는<br />
CNN 24%, DNN 5%, DBN 3%와 다른 나머지 방법이 12%를 차지했다.<br />
전체의 56%인 Shallow learning classifier는<br />
SVM 34%, kNN 7%, DT 3%, MPL 2%와 다른 나머지 방법이 10%를 차지했다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_11.png" /></p>

<p><br /></p>
<h2 id="a-eeg-dataset-management">A. EEG Dataset Management</h2>
<p>인간 감정은 2가지 방법으로 추출할 수 있다.</p>
<ul>
  <li>이미지, 오디오, 비디오, 시청작, 촉감, 냄새 등 자극을 통한 추출</li>
  <li>피험자들이 과거 살면서 직면했던 감정적 상태나 사건에 대한 질문</li>
</ul>

<p>최근에는 대부분의 연구자들이 첫번째 방법을 사용한다.<br />
첫번째 방법에서 26%는 시각 자극, 23.8%는 비디오 자극, 17.5%는 오디오 자극,<br />
22.2%는 생리학과 감정 데이터를 조합하여 만들어진 존재하는 데이터를 사용했다.<br />
나머지 10.5%는 감정 게임들 또는 라이브 공영 또는 생활에서 오는 자극과 관련해 감정을 얻었다.</p>

<p>감정 인식을 위해 가장 먼저 필요한 것은 EEG data를 수집하거나 기록하는 것이다.<br />
어떤 연구가 같은 raw data를 사용할 때마다 연구의 성능이나 정확도를 비교할 수 있다.<br />
결과적으로, 몇몇 학자들은 연구 단게의 데이터를 설계했고 그것들을 모든 사람들이 무료로 사용할 수 있게 만들었다.<br />
그것들 중에는 일반적이고 인기있는 데이터셋들이 있는데 아래에 정리했다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_12.png" /></p>

<p><br /></p>
<h2 id="b-preprocessing">B. Preprocessing</h2>
<p><strong>Preprocessing</strong>은 data collection과 analysis 사이에서 변환을 하거나 EEG data를 재조합 함으로써 발생한다.<br />
EEG의 전처리는 추가적인 분석을 위해 데이터의 일부를 조합하거나 추출한다면<br />
어떤 데이터도 바꾸지 않고 수행될 수 있다.<br />
또다른 전처리 단계의 형태인 안좋은 channel들의 제거는 EEG에서 artifact들을 육안 검사를 통해 수행한다.<br />
몇몇의 널리 사용되는 전처리 방법인 spatial transformations와 temporal filtering은 아래와 같다.</p>

<p><br /></p>
<h3 id="1-down-sampling">1) Down-Sampling</h3>
<p>Down-Sampling으로 EEG data의 크기는 줄어들 것이다.<br />
예를 들어 DEAP 데이터셋에서 EEG data는 먼저 512Hz에 기록되고, 나중에 128Hz로 down-smaple된다.<br />
다중 시간 처리, 저장 또는 무선 통신에서 down-sample된 줄어든 크기의 데이터를 선호한다.</p>

<p><br /></p>
<h3 id="2-re-referencing">2) Re-Referencing</h3>
<p>EEG electrode voltage는 다른 eletrode와 연관이 있거나 몇개의 electrode의 평균과 연관이 있다.<br />
Reference를 바꾸면 EEG 신호의 모양이 다양해진다.<br />
그러므로 초기 데이터를 적절한 reference를 따라 기록하지 못했을 때 re-reference가 중요하다.<br />
Matoid, Cz, earlobe(귓볼)의 평균 또는 electrodes의 평균 등등이 일반적인 referencing으로 선택된다.</p>

<p><br /></p>
<h3 id="3-artifacts-filtering">3) Artifacts Filtering</h3>
<p>일반적으로 raw EEG 신호들은 internal artifacts와 external artifacts 두가지 artifact에 의해 방해받는다.<br />
Internal artifact들은 눈 깜빡임, 안구 운동, 안면 근육 운동, 심장 박동, 호흡 등등으로 범주화 된다.<br />
반면 External artifact들은 cable frequency, 머리 움직임, electrode 위치 등등을 포함한다.<br />
주파수 관점에서 전력선 주파수로 인한 noise는 50Hz나 60Hz이고 근육 움직임으로 인한 noise는 40Hz 이상이고<br />
다른 internal artifact로 인한 noise는 4Hz 미만일 것이다.</p>

<p><br /></p>
<h4 id="a-bandpass-filtering">a: Bandpass Filtering</h4>
<p>Original EEG 신호에서 선택된 주파수 영역 밖의 주파수를 갖는 artifact는<br />
적절한 <strong>bandpass filter</strong>를 통해 제거할 수 있다.<br />
눈 깜빡임은 2~10초 사이의 간격으로 이루어져 0.5~0.1Hz를 갖는다.<br />
정상적인 성인의 심장 박동은 분당 60~100번 뛰어 1~1.67Hz를 갖는다.<br />
정상적인 호흡은 분당 12~20번 이루어지므로 0.2~0.33Hz 주파수 영역을 가리킨다.<br />
그러나 안면 근육 움직임이나 EOG로 인한 artifact는 0~200Hz 주파수 영역을 아우르고<br />
주로 20Hz 영역에서 EEG에 영향을 준다.<br />
결과적으로 EOG artifact를 제외하고 모든 internal type artifact들은<br />
bandpass filtering 기술을 통해 제거될 수 있다는 것이 명확하다.<br />
연구자들은 각자 다른 영역의 bandpass filter를 적용하고 주로 4~45Hz를 사용했다.<br />
1~100Hz는 적당히 사용되고 8~30Hz, 4~45Hz, 2~42Hz, 2~50Hz 등등은 드물게 적용되었다.</p>

<p><br /></p>
<h4 id="b-adaptive-filtering">b: Adaptive Filtering</h4>
<p>0~200Hz를 갖는 EOG는 실제 4~45Hz를 갖는 EEG신호의 범위를 벗어나지 않기 때문에,<br />
몇몇 학자들은 이런 유형의 artifact를 제거하기 위해 adpative filtering을 사용하기도 한다.<br />
그러나 adaptive filtering을 적용하기 위해서 추가적인 EOG를 기록해야 한다.<br />
일반적으로 adaptive filter는 EOG가 섞인 EEG신호와 EOG 신호 두개를 입력을 받고<br />
EEG 신호에서 EOG 신호를 빼기위해 original EOG 신호를 찾도록 계산한다.<br />
몇몇 학자들은 notch filter를 50과 60Hz에 적용한다.</p>

<p><br /></p>
<h4 id="c-blind-source-separation">c: Blind Source Separation</h4>
<p>Blind Source Separation algorithm은 raw EEG data로부터 original 신경 신호를 추출하고<br />
artifact를 제거하기 위해 사용되었다.<br />
알고리즘은 Independent Component Analysis(ICA),<br />
Algorithms for Multiple Unknown Signals extraction(AMUSE), Second-Order Blind Identification(SOBI),<br />
Joint Approximate Diagonalizations of Eigenmatrices(JADE) 등등을 포함한다.<br />
이러한 기술들은 EEG 신호에서 internal &amp; external artifact들을 제거하는데 사용된다.</p>

<p><br /></p>
<h4 id="d-independent-component-analysis">d: Independent Component Analysis</h4>
<p><strong>ICA</strong> 기술은 raw EEG처럼 합성된 신호의 독립 성분을 통계적으로 추출하는 것이다.<br />
이런 몇몇 독립 성분들로부터 artifact와 관련된 성분들이 제거되고 original data로 만들기 위해 다시 합성된다.<br />
ICA가 보통 연결된 채널의 수보다 작거나 같은 수의 소스들의 조합인 raw EEG 신호를 고려하기 때문에,<br />
그 소스들의 수만큼 분리시킬 수 있다.<br />
Fast ICA 알고리즘은 보통 신호 대 noise의 비율을 높이기 위해 artifact를 제거하는데에 사용된다.</p>

<p><br /></p>
<h2 id="c-feature-extraction">C. Feature Extraction</h2>
<p>인간의 감정을 인식하기 위해 가장 기본이 되고 도전적인 작업은 감정 상태의 변화를 다양하게 하는<br />
가장 연관성 있는 특징들을 찾아내는 것이다.<br />
Feature들은 주로 time domain, frequency domain과 time-frequency domain feature로 범주화된다.<br />
이러한 feature들의 타입과 장단점을 아래에 정리해 두었다.</p>
<p align="center"><img style="width: 90%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_13.png" /></p>

<p><br />
감정 인식에서 shallow와 deep learning 기반의 방법으로 추출된 feature들을 아래에 정리했다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_14.png" /></p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_15.png" /></p>

<p><br /></p>
<h3 id="1-time-domain-features">1) Time Domain Features</h3>
<p>Time domain featuere들은 통계학적 feature들이다.<br />
예를 들어 mean, median, standard deviation, mode, variance, minimum, maximum 등등.<br />
몇몇 연구자들은 이런 feature들을 다른 feature들이나 feature 셋들과 함께 사용한다.</p>

<p><br /></p>
<h3 id="2-frequency-domain-features">2) Frequency Domain Features</h3>
<p>Frequency domain feature는 이런 타입의 신호들과 더 연관성 있는 정보를 포함한다.<br />
Frequency domain에서 인기있는 방법들은 Power Spectral Density(PSD), Fast Fourier Transform(FFT),<br />
Short Time Fourier Transform(STFT) 등등이다.<br />
감정 인식에서 spectrum 분석도 Fourier transform을 이용한 유명한 분석법이다.<br />
몇 학자들은 영구적이지 않고 정적인 신호들에 적합한 Eigenvector 방법을 사용해 감정을 분류하려 했다.<br />
또 다른 몇몇 연구들은 좋은 frequency resolution을 제공하는 autoregressive model을 사용하지만<br />
이 방법은 spectral estimation이 어렵다.<br />
Autoregressive 분석은 sharp spectral feature를 갖는 유형의 신호들에 적용하기 완벽하다.</p>

<p><br /></p>
<h3 id="3-time-frequency-domain-features">3) Time-Frequency Domain Features</h3>
<p>연구에 따르면 Fast Fourier Transform과 Auto-Regressive model은 느린 계산력으로 인한 피해자가 되고<br />
Non-stationary 타입의 신호를 분석하는것이 불가능하다.<br />
Joint time-frequency domain은 wavelet transform으로 구성된다.<br />
기본적으로 EEG 신호는 non-stationary이고 non-linear 신호이다.<br />
이러한 유형의 신호를 분석하는 것은 도전적이고 복잡하다.<br />
최근에는 wavelet transform이 time과 frequency domain 에서 모두 좋은 성능을 보여주기 때문에<br />
매우 인기가 있는 분석법이 되었다.<br />
Wavelet transform은 아래처럼 두 타입으로 분류된다.</p>
<ul>
  <li>(i)  Continuous Wavelet Transform</li>
  <li>(ii) Discrete   Wavelet Transform</li>
</ul>

<p>일반적인 feature들의 수학적 표현은 아래와 같다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_16.png" /></p>

<p><br /></p>
<h2 id="d-features-and-feature-extraction-methods">D. Features and Feature Extraction Methods</h2>
<p>많은 방법들로 EEG 신호로부터 다양한 feature들을 추출할 수 있다.<br />
매개변수와 함께 몇가지 중요한 feature extraction 방법을 수식과 함께 간단하게 소개했다.</p>

<p><br /></p>
<h3 id="1-short-time-fourier-transform-stft">1) Short Time Fourier Transform (<strong>STFT</strong>)</h3>
<p>Time domain EEG는 Fourier transform을 이용해 쉽게 frequency domain으로 전송될 수 있다.<br />
많은 학자들은 이 방법을 사용해 EEG feature들을 추출했다.<br />
Power Spectral Density feature(PSD)를 평가하기 위해<br />
Short-Time Fourier Transform(STFT) 방법이 사용된다.<br />
인기있는 STFT를 사용함으로써, window는 특정 시간의 segment에서 Discrete Fourier Transform을 하기 위해<br />
time series data 위를 움직일 수 있다.<br />
Fast Fourier Transform(FFT) 알고리즘은 아래 식을 통해 DFT 계산을 한다.</p>

\[\begin{align}
X_k=\sum^{N-1}_{n=0}x_ne^{-2jnk/N},\ where\ k=0,\dots,N-1
\tag{17}
\end{align}\]

<p>여기서 $x_0, \dots,\ x_{N-1}$은 complex number(복소수)이다.</p>

<p>FFT는 DFT를 수행하기 위한 알고리즘이지만,<br />
반면에 STFT는 time series data에서 window를 슬라이딩 시켜 DFT를 수행하는 방법이다.<br />
STFT는 아래 수식으로 쓸 수 있다.<br />
\(\begin{align}
X_{STFT}=\int[x(t)w^*(t-f)e^{-j2\pi ft}]dt
\tag{18}
\end{align}\)</p>

<p>$w(t)$는 window function이다.</p>

<p><br /></p>
<h3 id="2-discrete-cosine-transformdct">2) Discrete Cosine Transform(DCT)</h3>
<p>Discrete Cosine Transform(DCT)는 time domain signal을 basic frequency components로 전환시켜준다.<br />
N개의 time domain signal의 DCT 출력값에서 왼쪽에 위치하는 저주파성분과 오른쪽에 위치하는 고주파 성분의<br />
N개의 계수로 구성된다.</p>

<p>고주파 성분의 계수들의 값이 0에 가까워 무시되기 때문에,<br />
처음 몇몇 계수들(저주파 성분의)만 감정인식을 위한 EEG feqture로 고려된다.<br />
그러므로 DCT는 데이터를 합축하고 고주파 필터링 과정을 없애주고 분류 알고리즘에서 time과 space를 고려한 계산 복잡도를 줄여준다.<br />
EEG data의 N개 점에 대한 DCT 계수들은 아래 식을 통해 계산된다.<br />
\(\begin{align}
Y(u)=\sqrt{\dfrac{2}{N}}\alpha(u)\sum^{N-1}_{x=0}f(x)\cos{\left[\dfrac{\pi(2x+1)}{2N} \right]}
\tag{19}
\end{align}\)</p>

<p>여기서 $u=0,1,\dots,N-1$이고 $\alpha(u)=\left[1/\sqrt{2}\text{(when u = 0)}; \mathit{1}\ \text{(when u ≠ 0)}\right]$이다.</p>

<p><br /></p>
<h3 id="3-wavelet-transformwt">3) Wavelet Transform(WT)</h3>
<p>EEG같은 non-stationary 신호에서 feature를 추출하기 위해서는 wavelet transform이 가장 적합한 방법이다.<br />
Wavelet transform은 저주파에서 더 정확한 frequency 정보를 제공하고<br />
고주파에서 더 정확한 time 정보를 제공한다.<br />
저주파 필터와 고주파 필터를 통해 신호를 multi-resolution time-frequency plane으로 전송한다.<br />
두가지 wavelet transform이 아래와 같이 나뉜다.</p>
<ul>
  <li>(i)  Continuous Wavelet Transform (CWT)</li>
  <li>(ii) Discrete   Wavelet Transform (DWT)</li>
</ul>

<p>EEG를 이용한 감정 인식 분야에서 DWT가 많이 사용된다.<br />
DWT는 noise를 제거하고 EEG 신호를 delta, theta, alpha, beta와 gamma로 구성된 sub-band로 분해한다.<br />
DWT를 이용하여 EEG 신호를 approximate coefficient와 detail coefficient로 나눌 수 있다.<br />
Time domain 신호는 DWT를 사용해 아래 식을 통해 분해할 수 있다.</p>

\[\begin{align}
\gamma(t)=\int\limits^{\infty}_{-\infty}x(t)\dfrac{1}{\sqrt{2^a}}\psi\left(\dfrac{t-b\times 2^a}{2^a} \right)dt
\tag{20}
\end{align}\]

<p>여기서</p>
<ul>
  <li>$\gamma(t)$ : $x(t)$의 모든 time-domain 신호의 DWT</li>
  <li>$\psi(t)$ : mother wavelet</li>
  <li>a : scale parameter</li>
  <li>b : shift parameter</li>
</ul>

<p>Approximate와 detail coefficients는 아래 식을 통해 계산된다.<br />
\(\begin{align}
x_{app}=\sum^{\infty}_{k=-\infty}x\left[b\right]g\left[2n-b \right]
\tag{21}\\
x_{det}=\sum^{\infty}_{k=-\infty}x\left[b\right]h\left[2n-b\right]
\tag{22}
\end{align}\)</p>

<p><br /></p>
<h3 id="4-higher-order-crossing-hoc">4) Higher-Order Crossing (HOC)</h3>
<p>Higher-Order Crossing은 the number of zero-crossing of a finite zero means time-series data로 구성된다.<br />
HOC를 추출하기 위해 ‘How many times does a signal cross the zero level?’이 게산 되어야 한다.<br />
Filter가 time-series 신호에 적용되는 경우에 zero-crossing하는 수가 변한다.<br />
결과적으로 filter들의 집합이 적용되기 위해서 몇개의 집합이 만들어져야 한다.<br />
이것이 filter의 집합을 위한 HOC sequence라 불린다.<br />
서로 다른 HOC sequence들은 적절한 filter design으로 계산되고<br />
이 HOC는 feature vector를 만드는데 사용된다.<br />
그리고 나서 EEG를 통한 감정 분류에 사용될 수 있다.<br />
HOC feature vector는 특정 값 k를 통해 아래 식으로 계산될 수 있다.<br />
\(\begin{align}
D_k=NZC\{\mathfrak{J}_k(Z_t)\};\quad k=1,2,3,\dots;\ t=1,\dots,T
\tag{23}
\end{align}\)</p>

<p>여기서 NZC는 Number of Zero Crossing을 평가하기 위한 것이다.</p>

\[\begin{align}
\nabla Z_t &amp;\equiv Z_t - Z_{t-1}
\tag{24}\\
\mathfrak{J}_k &amp;\equiv \nabla^{k-1};\quad k=1,2,3,\dots
\tag{25}
\end{align}\]

<ul>
  <li>$Z_t$ : finite zero means series data</li>
  <li>$\nabla$ : high pass filter</li>
  <li>$\mathfrak{J}_k$ : sequence of high pass filter</li>
</ul>

<p><br /></p>
<h3 id="5-hjorth-parameter">5) Hjorth Parameter</h3>
<p>Bo Hjorth가 1970년에 소개한 Hjorth parameter들은 통계적 특성을 나타내고<br />
EEG 신호에서 feature를 추출하기 위해 많이 사용된다.<br />
이 Parameter들은 activity, mobility와 complexity를 포함한다.<br />
Time domain EEG 신호 x(t)에서 parameter는 다래와 같이 계산된다.</p>

\[\begin{align}
\text{Activity,}&amp;\quad A=\text{var}(x(t))=\dfrac{1}{N}\sum^{N}_{n-1}\left[x(t)-\mu_x \right]^2
\tag{26}\\
\text{Mobility,}&amp;\quad M=\sqrt{\dfrac{\text{var}(\dfrac{d(x(t))}{dt})}{\text{var}(x(t))}}
\tag{27}\\
\text{Complexity,}&amp;\quad C=\dfrac{\text{Mobility}(\dfrac{d(x(t))}{dt})}{\text{Mobility}(x(t))}
\tag{28}
\end{align}\]

<p><br />
Activity, Mobility와 Complexity는 신호의 세기와 평균 주파수와 주파수 변형를 각각 나타낸다.<br />
Autoregression, PCA, ICA 등등 과 같은 방법들도 감정 인식을 위해 EEG 신호에 적용해<br />
feature를 추출하는 데에 사용된다.<br />
그러나 이것들은 별로 사용되지 않고 우리는 자료의 길이를 줄여야 해서 이 논문에서 설명하지 않았다.<br />
모든 관점을 고려했을 때, 그 어떤 고정된 feature나 feature set도 최고의 feature로 다뤄질 수 없다.<br />
그러므로 많은 연구자들은 감정 인식과 더 관련된 상위 feature를 찾으려고 노력했다.<br />
위에 Table 7에서 감정 인식 방법에서 추출된 feature와 추출하는 방법을 deep learning 기반으로 요약해 뒀다.<br />
Shallow machine learning 기반의 방법도 위에 Table 8에 요약했다.</p>

<p>Power Spectral Density(PSD)와 Differential Entropy(DE)의 feature들은<br />
일반적으로 감정 인식 작업을 검토하는 데에 적용되었다.<br />
많은 연구자들은 통게적 feature들을 사용했다.<br />
Menezes의 2017년 자료와 Lan의 2016년 자료에서는 HOC feature를 사용했다.<br />
Zhong의 2020년 자료에서는 sparse adjacency matrix를 사용했다.<br />
Cui의 2020 자료에서는 temporal, regional과 asymmetric feature를 사용했다.<br />
Mehmood의 2017년도 자료와 Atkinson과 Campos의 2016년도 자료에서는 Hjorth parameter를 사용했다.<br />
Pearson’s Correlation Coefficients(PCC)는 몇몇 학자들에 의해 사용되었다.<br />
Cimtay와 Ekmekcioglu의 2020년도 자료에서는 Raw EEG 데이터를 feature로 사용했다.<br />
적용된 기술들은 위에 Table 7과 8에서 묘사했고<br />
후에 Table 10과 11에서 이들의 성능을 기반으로 정확도를 보이겠다.<br />
어떤 feature가 EEG를 통한 감정 인식에서 가장 적합한가에 대한 하나의 정답은 존재하지 않는다.<br />
하나의 system에서 최적의 feature는 분류할 감정의 수나 채널의 수, 데이터 타입, 복잡한 정도,<br />
알고리즘의 유형과 감정의 모델 등에 따라 다르기 때문에 다른 system에서도 최적의 feature가 될 수 없다.</p>

<p><br /></p>
<h2 id="e-feature-selection">E. Feature Selection</h2>
<p>Feature selection 방법들은 매우 중요한 feature들의 수를 줄여 구성된<br />
feature들의 하위집합을 만드는 것을 목표로 한다.<br />
그들은 input variable과 target variable 사이에 깊은 연관성을 갖는 적절한 feature들을 추출하는데에 도움이 된다.<br />
Selection 알고리즘을 조합한 과정을 기반으로 features selection method들은 3개의 타입을 갖는다.</p>
<ul>
  <li>Filter method</li>
  <li>Wrapper method</li>
  <li>Embedded method</li>
</ul>

<p>많은 학자들은 이러한 방법들을 바탕으로 여러 알고리즘들을 사용했다.<br />
EEG 기반의 감정인식을 위해 적절한 feature를 선택하는 알고리즘을 찾는 것은 어렵다.<br />
그러나 많은 input variable과 categorical output variable을 다루는 EEG를 통한 감정인식 작업에서<br />
Correlation-based Analysis of Variance(ANOVA)와 Kendall’s rank coefficient들은 자주 사용된다.</p>

<p>최근에는 많은 학습 알고리즘들이 feature selection 작업들을 내부적으로 처리한다.<br />
예를들어, sparse regression과 LASSO는 L1 regularization 기술을 사용하여 feature selection을 한다.<br />
게다가, stochastic searching algorithm은 global minima를 찾음으로써 같은 일을 수행한다.<br />
최신의 학습 알고리즘이 feature selection을 수행하기 때문에,<br />
몇몇 학자들은 감정 인식을 위해 feature selection 알고리즘을 따로 사용한다.<br />
몇가지 알아둬야 할 feature selection 알고리즘들에는 Particle Swarm Optimization(PSO),<br />
Genetic Algorithm(GA), Minimum Redundancy Maximum Relevance(mRMR), Ant Colony(AC),<br />
Correlation Feature Selection(CFS), Simulated Annealing(SA) algorithm,<br />
Sequential Forward Selection(SFS)과 Welch’s t-test 등등이 있다.</p>

<p><br /></p>
<h2 id="f-classificatio-algorithms">F. Classificatio Algorithms</h2>
<p>많은 학자들이 많은 classification algorithm들 ㅅ용했다.<br />
그들 중에서 몇몇 일반적인 알고리즘들을 아래 표기해뒀다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_17.png" /></p>

<p><br /></p>
<h1 id="iv-discussion">IV. Discussion</h1>
<p>많은 학자들은 EEG를 사용한 감정인식 논문들에 대한 검토를 출판했다.<br />
어떤 논문에서는 학자들이 EEG를 통한 감정 인식의 적용과 분류를 검토했다.<br />
여기서 이것들을 적용한 내용들을 정교하게 다루지만<br />
시스템 개발, 실제 feature, feature extraction 방법, 성능 비교에 대한 정확한 정보가 부족했다.<br />
어떤 시스템적 검토에서는 연구자들이 feature와 classifier-wise advantages, disadvantages와<br />
어떤 feature가 좋았는지에 대한 비율을 묘사했다.<br />
그러나 언뜻 봤을 때 앞으로의 연구자들을 위한 직관적인 정보를 전달해주는 통찰과 추천이 없다.<br />
어떤 논문에서는 VR 자극 기반 연구가 이루어졌지만 이번 검토에서는 우리는 오디오, 비디오, 시청각, 사진 유형 자극 기반 감정 인식 시스템을 비교한다. 
어떤 논문에서는 서술적인 연구를 했지만 대부분의 자료(90%이상)에서는<br />
shallow machine learning 기반 분류기를 사용한다.<br />
그러나 우리는 shallow와 deep learning 기반 시스템을 최근에 개발된 deep learning model들의 적용을 포함하여<br />
적절하게 비교했다.<br />
그 뒤에 미래 연구를 위해 최신 정보와 가치있는 정보를 제공하는 8개의 각 feature들, 방정식에 대한 feature들,<br />
deep과 shallow 학습 방법에 대한 성능 비교와 통찰과 추천을 12개의 최신 데이터 소스에 대해 진행했다.</p>

<p>이번 논문은 감정 인식을 deep과 shallow machine learning 기반 기술에 집중하여 성능, 방법과 기술들을 요약했다.</p>

<p>많은 deep machine learning-based 검토 시스템들에 대해 대부분의 시스템들은 CNN 기반 또는<br />
변형된 CNN 기반 시스템으로 개발 되었다.<br />
변형된 CNN 방법들은 DE-CNN, HCNN, MC-CNN, PCRNN, RA-CNN이 있다.<br />
몇몇 학자들은 deep machine learning 기반 시스템의 성능을 SVM, LR, kNN 등등<br />
다른 분류 알고리즘을 기반으로하는 시스템과 비교했다.<br />
대부분의 연구는 ‘DEAP’에의해 수행되고 몇몇은 ‘SEED’에의해 그리고 소수는 ‘Self-Generated’ 데이터셋으로 진행 되었다.<br />
주목할 점은 같은 수의 감정들로 분류하는 같은 인식 시스템을 사용하는 것이다.<br />
‘DEAP’ 데이터에 해당하는 정확도는 ‘SEED’나 self-generated ‘MAHNOB’보다 낮았다.<br />
데이터가 정리된 아래 테이블을 통해 위 내용이 명확함을 알 수 있다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_18.png" /></p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_19.png" /></p>

<p><br />
감정 인식에서 학자들은 Valence와 Arousal을 통해 감정을 직간접적으로 분류한다.<br />
직접적인 분류를 위해서는 전반적인 정확도가 성취되고<br />
간접적인 분류를 위해서는 valence와 arousal의 정확도가 찾아져야 한다.<br />
Deep learning 기반 시스템의 다양한 검토로 얻은 전반적인 정확도는 Fig. 10(a)에 그려놨다.<br />
그래프에서 시스템의 대부분은 80% 이상의 정확도를 갖는다는 것이다.<br />
가장 높은 전반적인 정확도는 99.72%를 달성했다.<br />
Fig. 10(b)는 valence와 arousal 기반의 간접적인 감정 인식 시스템의 정확도를 보여준다.<br />
시스템들 중에서 RA-CNN 분류기를 사용해 DEAP 데이터셋에서 두가지 class로 분류하도록 한 것이<br />
valence(96.65%)와 arousal(97.11%)의 가장 높은 정확도를 달성했다.<br />
반면 한 학자는 DNN 분류기를 사용하여 62.5%와 61.25%의 가장 낮은 정확도를 valence와 arousal 각각에서 달성했다.<br />
두가지 시스템이 모든 면에 대해서 똑같다면<br />
‘가장 높은 정확도가 좋은 시스템을 뜻하고 더 낮은 정확도가 별로 좋지 않은 시스템을 의미한다’라는 말이 맞다.<br />
출력 class가 2개가 아니라 4개인 논문에서는 2개일 때의 정확도보다 분류에 대한 정확도가 더 낮다는것을 보인다.</p>

<p>연관성 있는 정보에 대한 Shallow machine learning 기반 감정인식 시스템과<br />
그의 정확도는 위에 Table 11에 표기했다.<br />
Table 11에서 대부분의 학자들은(23명중 15명, 65.2%) SVM 또는 LSSVM과 MCSVM 같은<br />
modified SVM 등을 분류기로 사용했다.<br />
몇몇 학자들은 Decision Tree(DT), Genetic Algorithms, k Nearest Neighbor, Multi-Layer Perceptron,<br />
Probabilistic Neural Network, Random Forest, regularized Graph Neural Network를<br />
분류 알고리즘으로 사용했다.<br />
어떤 저자는 CFS와 kNN을 동시에 사용하여 80.8%의 정확도를 얻었다.<br />
어떤 논문에서는 하위 네트워크를 통해 계층적 네트워크 scheme을 만들어 긍정적, 부정적, 중립적 감정에 대해<br />
93.26%의 정확도를 얻었다.<br />
이진 분류기는 매우 일반적이다.<br />
게다가 몇몇 시스템은 3개, 4개, 5개 심지어 6개의 출력 class를 표현했다.<br />
Shallow machine learning 기반의 검토 시스템들의 전반적인 정확도는 Fig. 10(c)에 표현했다.<br />
Fig 10에서 대부분의 학자들이 DEAP 데이터셋을 사용했다는 것을 알 수 있다.<br />
추가로 SEED와 self-generated 데이터의 정확도는 DEAP와 비교해서 더 높았따.<br />
정확도는 시스템 평가를 위한 한가지 요소이듯이 전반적인 성능은 precision, recall, AUC, 출력 class의 갯수 등등도 펴악 요소가 될 수 있다.<br />
감정 인식 시스템 기반의 valence와 arousal이 성능은 Fig. 10(d)에 표현했다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_20.png" /></p>

<p>Table 10과 11의 데이터로부터 얻은 deep과 shallow learning 기반 감정 인식 시스템들의 정확도는 아래와 같다.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
      <th>2018</th>
      <th>2019</th>
      <th>2020</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Deep</td>
      <td>86.08%</td>
      <td>85.20%</td>
      <td>82.40%</td>
      <td>99.72%</td>
      <td>87.55%</td>
      <td>87.23%</td>
    </tr>
    <tr>
      <td>Shallow</td>
      <td>73.10%</td>
      <td>76.93%</td>
      <td>76.49%</td>
      <td>69.66%</td>
      <td>62.46%</td>
      <td>87.53%</td>
    </tr>
  </tbody>
</table>

<p>이 데이터들은 아래 그려뒀다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_21.png" /></p>

<p><br />
이 내용들로부터 deep machine learning 기반 시스템 성능은 shallow machine learning<br />
기반 시스템보다 비교적 높다는 것을 알 수 있다.</p>

<p><br /></p>
<h1 id="v-obsevations-and-recommendations">V. Obsevations and Recommendations</h1>
<p>시스템적 검토를 하고나서 classifier, feature extraction, dataset 등등과 관련된 문제들이 발생했다.<br />
몇가지 중요한 문제점들과 연관된 통찰과 그에 따라 일치하는 추천들을 아래 정리했다.</p>
<p align="center"><img style="width: 100%" src="../../../../assets/img/Paper_Review/EEG_Emotion_Recognition/EEG_Emotion_Recognition_22.png" /></p>

<p><br />
추가로 우리가 발견한 앞으로 고려해야할 내용들을 연구했다.<br />
중요한 점은 아래와 같다.</p>

<p>1)<br />
많은 연구자들은 EEG data의 baseline을 고려하여 빼지 않고 감정으로 발생한 EEG 데이터만 고려했다.<br />
Baseline EEG 신호들은 인간으로부터 자발적으로 발생한 EEG data이다.<br />
몇몇 연구자들은 감정으로부터 발생한 EEG 신호와 자발적으로 발생한 EEG 신호의 차이점을 계산했다.<br />
다음에는 이것을 feature로 사용해 그들은 좋은 결과를 보였다.</p>

<p>2)<br />
감정은 복잡한 생리학적 현상이므로 가장 좋은 feature나 가장 좋은 feature extraction 방법을 제안하기는 어렵다.<br />
그러나 wavelet transform, PCA, ICA, Hjorth parameters 등등은 출판된 논문을 통해 잘 수행 된다는 것을 보였다.</p>

<p>3)<br />
광범위한 알고리즘들 중에서 CNN, DBN, RNN 등등과 같은 deep learning 기술들은<br />
kNN, NB와 RF 같은 shallow learning 기반 알고리즘들과 비교해 더 효과적이다.<br />
그러나 SVM은 shallow learning 기술들 중에서 EEG 기반 감정 추출을 잘 수행한다.</p>

<p>4)<br />
휴대성과 단순함이 필요하지 않을 때는 ECG, EOG, EMG, fMRI와 다른 생리학적 신호들이 포함된 multimodal data는 
감정 인식 시스템의 성능을 매우 향상시킨다.</p>

<p><br />
최근 EEG microstate가 분석되어 처리에 대한 정보와<br />
인간 뇌의 인지와 같은 뇌 활동이 포함된 연관성을 결정할 수 있다.<br />
그러나 앞으로 이러한 형상에 대한 많은 연구가 필요하다.</p>

<p><br /></p>
<h1 id="vi-conclusion">VI. Conclusion</h1>
<p>이번 review에서는 이미 많은 학자들이 감정인식에서 사용한<br />
feature, feature extraction technique, system performance, algorithms 들을 정리했다.<br />
이 논문의 개정은 다음 두가지 주된 내용에 의해 이루어졌다.</p>
<ul>
  <li>Deep machine learning-based emotion recognition systems</li>
  <li>Shallow machine learning-based emotion recognition systems</li>
</ul>

<p>Comparison table과 performance graph를 보여주고 공적으로 사용 가능한 데이터셋의 정보를 알게하고<br />
분석을 위한 소프트웨어나 tool들의 이름을 알게하는것으로 당신의 열정에 도움이 되었으면 좋겠다.<br />
실생활에 적용하기 위해 기계학습을 사용한 감정 인식 시스템을 효과적으로 설계하기 위해<br />
앞으로 연구자들에게 완벽한 방향을 제시해 주었다.</p>]]></content><author><name>Md. Rabiul Islam, Mohammad Ali Moni, Md Rashed-Al-Mahfuz, Md. Saiful Islam, etc.</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[Abstract 최근 electoencephalogram 기반의 감정 인식은 Human-Computer Interaction(HCI) 시스템이 더 지능화 되게 해주는 덕에 중요하게 평가되고있다. Person-based decision making, mind-machine interfacing, cognitive interaction, affect detection, feeling detection 등과 같이 감정 인식 기술의 놀라운 적용들 때문에 감정 인식이 과대 평가로 최근 AI 기반 연구들을 끌어들이는데 성공했다. 그러므로 많은 연구들은 어떤 작업에서 그것에 맞는 feature set들과 기술들로 방법론적 시스템 상의 검토가 이루어지는 다양한 방법의 접근을 해보는 방법으로 많은 연구가 되고 있다. 이것은 효과적인 감정 인식 시스템을 구성하는 지침서로 작용해 초보자들이 학습하도록 용이하게 쓰일 것이다. 이 논문에서 우리는 최근 출판된 감정 인식에서 최신 기술에 대한 엄격한 검토를 수행하고 적절한 프레임워크를 개발하도록 핵심적인 내용을 제공하도록 몇몇 공통된 감정인식의 단계들을 관련된 정의와 관련된 이론과 관련된 분석들을 요약할 것이다. 게다가 여기서 진행한 연구에는 두개의 카테고리로 이분화 된다. Deep Learning Based Emotion Recognition system Shallow Machine Learning Based Emotion Recognition system]]></summary></entry><entry><title type="html">Basics of Electroencephalography for Brain-Computer Interface Developer</title><link href="http://192.168.0.41:4000/paper%20review/2022/09/27/Basics-of-Electroencephalography-for-Brain-Computer_interface-Developer.html" rel="alternate" type="text/html" title="Basics of Electroencephalography for Brain-Computer Interface Developer" /><published>2022-09-27T18:37:17+09:00</published><updated>2022-09-27T18:37:17+09:00</updated><id>http://192.168.0.41:4000/paper%20review/2022/09/27/Basics-of-Electroencephalography-for-Brain-Computer_interface-Developer</id><content type="html" xml:base="http://192.168.0.41:4000/paper%20review/2022/09/27/Basics-of-Electroencephalography-for-Brain-Computer_interface-Developer.html"><![CDATA[<p><strong>요약</strong><br />
뇌파에 관한 많은 기술적 발전에도 불구하고 정확한 뇌파의 측정은 오늘날에도 쉽지 않다.<br />
더욱이 비침습형 뇌파인식기를 이용한 외파의 측정은 정확도가 떨어진다.<br />
하지만 실용적인 뇌-컴퓨터 인터페이스 활용 시스템 개발을 위해서는 사용의 편의성 때문에 비침습형 뇌파인식기를 이용하는 경우가 많다.<br />
특히, 가장 사용하기 편한 비침습형 건식 최파인식기인 경우 뇌파 측정의 정확도가 가장 떨어진다.<br />
본 논문에서는 임상 응용보다는 뇌-컴퓨터 인터페이스 시스템 개발 시 뇌파 측정을 통한 사용자 의도 파악의 정확도를 높이는데 필요한 뇌파에 대한 기초 지식과 특성에 대해 알아본다.<br />
뇌파 측정을 통한 사용자 의도 파악이 정해지면 다양한 분야에서 뇌파의 활용도가 높아지고 새로운 응용 분야도 개척될 것이다.</p>

<p><strong>주제어</strong> : 차세대, 뇌파, 뇌-컴퓨터 인터페이스, 뇌파 측정</p>

<h1 id="1-서론">1. 서론</h1>
<p><strong>뇌파</strong>(Electroencephalography, <strong>EEG</strong>)는 두뇌에 있는 다수의 신경세포가 활성화 되어 발생하는 전기적인 신호인다.<br />
동시에 활성화되는 신경세포의 수가 많고 이들에서 발생하는 전류의 방향이 같다면 전류의 흐름이 커져 머리덮게 부분에서도 측정할 수 있게 되는데 이를 <strong>머리덮게 뇌파</strong>(Scalp EEG)라고 한다.<br />
1924년 Hans Burger가 뇌파를 기록하고<br />
1925년 머리덮개 부분에서 측정한 전기현상을 바탕으로 뇌파도(Electroencephalogram)를 작성하면서<br />
본격적으로 뇌파연구가 시작되어 1929년 최초의 인간 되파 기록 보고서가 발표되었다.</p>

<p>인간의 뇌파를 연구하기 전에는 동물들을 대상으로 뇌파 연구를 했다.</p>

<p>Hans Berger가 놰파를 발견한 후로 병원이나 실험실에서 신경 장애를 평가하거나, 
뇌 기능을 조사하는데 사용되었고 몇몇 연구는 치료 가능성을 탐구했다.<br />
이러한 연구가 발전하면서 뇌파를 해석하여 다른 사람의 생각을 읽어내고 이를 이용하여 주변 장치를 조정하거나 다른 사람과 의사소통할 수 있도록 하는 연구로 발전하고 있다.<br />
뇌 신호를 미리 정의 된 명령으로 변환하여 다른 사람들과 의사소통하거나 외부 장치를 제어하기 위해서는<br />
<strong>Brain-Computer Interface(BCI)</strong> 또는 Machine-Computer Interface(MCI)라고 하는<br />
뇌-컴퓨터 인터페이스 기술이 필요하다.<br />
초기 BCI는 해상도와 신뢰성이 제한적이었지만 최근에 컴퓨터 기술이 발전해<br />
다채널 뇌파의 정교한 온라인 분석이 가능해지고,<br />
뇌파 관련 임상 연구 결과가 쌓여 뇌파 관련 기술에 대한 과학적, 상업적 관심이 크게 향상되었다.</p>

<p>BCI 응용 시스템 개발시</p>
<ul>
  <li>뇌파를 이용한 정확한 사용자 의도 파악</li>
  <li>뇌파의 정확한 측정</li>
  <li>측정된 뇌파를 어떻게 해설할 것인지</li>
</ul>

<p>가 매우 중요하다.<br />
뇌파의 측정과 해석의 신뢰도를 높이는데 필요한 것들은 정확도 높은 뇌파측정기,<br />
신호처리 및 변환 프로그램 등 여러가지가 있으나 기본적으로 뇌파의 특성을 잘 알아야 한다.<br />
본 논문에서 BCI 시스템 개발시 뇌파 측정을 통한 사용자 의도 파악의 정확도를 높이는데 필요한<br />
<strong>뇌파에 대한 기본 지식과 특성을 임상 연구자보다는 BCI 시스템 개발자 관점에서 알아보겠다.</strong></p>

<p><br /></p>
<h1 id="2-뇌파eeg의-구성과-사건관련전위erp">2. 뇌파(EEG)의 구성과 사건관련전위(ERP)</h1>
<p>특정 시점에 뇌의 특정 위치에서 측정된 전압값과 기준전극과의 전압 차는 시간의 흐름과 함께 파형으로 나타나는데 이를 뇌파라 한다.<br />
뇌파는 진폭, 주파수, 파형 등의 조합으로 구성되며 BCI 시스템 개발을 위해서는 파형보다는 주파수와 진폭에 주목할 필요가 있다.<br />
뇌파는 주파수에 따라 다양하게 구분할 수 있는데, 본 논문에서는 6가지로 나누어 살펴본다.</p>

<p>인간의 뇌에서 나오는 뇌파의 파장은 기본적으로 0~30Hz의 주파수가 나오며 약 20~200µV의 진폭을 보인다.<br />
뇌파의 주파수 영역을 임의로 아래와 같이 분류한다.</p>
<ul>
  <li>주파수 4Hz미만 : 델타파</li>
  <li>주파수 4~7Hz  : 세타파</li>
  <li>주파수 8~13Hz : 알파파</li>
  <li>주파수 13~30Hz: 베타파</li>
  <li>주파수 30Hz~  : 감마파</li>
</ul>

<p>여기서 12~15Hz의 주파수를 갖는 것은 <strong>SMR파</strong>(Sensory Motor Rhythm Wave)라고 부르며<br />
BCI 시스템 개발에 유용하게 사용될 수 있는 뇌파이다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_0.png" /></p>

<p><strong>사건관련전위</strong>(Event-related potential; <strong>ERP</strong>)는 특정 자극 정보(소리, 영상 등)에 의해 발생하는<br />
<strong>유발전이</strong>(Evoked Potential; <strong>EP</strong>) 중에서 인지적 처리가 요구되는 상황에서 발생하는 뇌파이다.<br />
ERP는 발생 시간을 통제할 수 있어서 잘 측정한다면 사용자의 의도를 파악하는데 유용하게 쓰일 수 있는 신호로<br />
반응시간과 극성을 기준으로 구분하여 명명한다.<br />
ERP 파형 성분의 극성은 P로 나타내는 양전위와 N으로 나타내는 음전위가 있다.<br />
ERP는 이러한 극성과 반응시간을 이용하여,<br />
(청각)자극 제시 후 약 50ms 후에 최곳값이 양전위인 경우 <strong>P50</strong>이라 부르고<br />
자극 제시후 약 100ms 후에 최곳값이 음전위인 경우 <strong>N100</strong>라 부른다.<br />
이런 ERP는 P50, P200, P300, N100, N170, N200등 다양한데<br />
<strong>P300</strong>은 대표적인 ERP로 개수를 세거나 버튼을 누르는 등의 행동이 요구될 때<br />
이마엽과 마루엽(두정엽) 사이에서 발생한다.<br />
<strong>N170</strong>은 얼굴과 관련된 자극으로 뒤통수엽(후두엽)에서 크게 나타난다.</p>

<p>운동관련전위(Movements Related Cortical Potentials; MRCPs)는 몸의 움직임 시작 1500~200ms 전부터<br />
나타나는 뇌전위로 근육이 수축하기 200~500ms전에 나타나는 Bereitschafts Potential(BP)와<br />
BP 발생후 500~50ms 사이에 운동의 계획과 관련하여 발생하는 Negative Slope(NS),<br />
그리고 운동 시작 50ms 후에 발생하는 Motor Potential(MP)로 구성된다.</p>

<p><br /></p>
<h1 id="3-bci-시스템-개발시-주목할-뇌파의-발생-위치">3. BCI 시스템 개발시 주목할 뇌파의 발생 위치</h1>
<p>뇌파는 뇌의 다양한 영역에서 발생하기 때문에 뇌의 영역을 구분하여 설명할 필요가 있다.<br />
뇌의 영역 구분과 이데 대한 설명은 브로드만이 구성한 브로드만 영역을 이용한다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_1.png" /></p>

<p>Brodmann area는 대뇌의 피질에 있는 영역으로,<br />
세포구축(Cytoarchitecture) 즉 세포의 구조와 구성에 따라<br />
겉질47개와 속질 5개 영역을 포함하여 총 52개 영역을 정의하고 위 그림과 같이 번호를 부여한다.</p>

<p>속질 5개의 영역은 뇌의 안쪽에 위치하며 기분, 사고, 운동 통합 등 종합 인지 기능인 집행기능을 담당한다.<br />
하지만 비침습형 뇌파인식기를 이용한 뇌파의 측정시 속질에서 발생하는 뇌파는 측정이 어려우므로<br />
BCI 시스템 개발시 주목할 뇌파의 발생 위치는 겉질 영역이다.<br />
겉질은 크게 보면 아래 그림과 같이 전두엽이라고 하는 이마엽(<strong>Frontal Lobe</strong>),<br />
두정엽이라고 하는 마루엽(<strong>Parietal Lobe</strong>), 측두엽이라고 하는 관자엽(<strong>Temporal Lobe</strong>),<br />
그리고 후두엽이라고 하는 뒤통수엽(<strong>Occipital Lobe</strong>) 이렇게 4부분으로 나누어 볼 수 있다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_2.jpeg" /></p>

<p>이마엽(Frontal Lobe)은 계획, 판단, 창의성, 추론 등을 포함한 고차원적 사회적 행동과 관련이 있다.<br />
이마엽은 브로드만 영역의 9, 44, 45, 46 등이 집행기능(Executive Function)과 관계가 있고,<br />
브로드만 영역의 8, 32, 44, 45번은 운동기능(Motor Function)과도 관계가 깊다.<br />
마루엽(Parietal Lobe)에 있는 1, 2, 3번 영역은 여러 감각정보들을 받아들이는 역할을 하는<br />
일차 몸감각영역이고 이를 단면으로 잘라 놓고 보면 뇌의 아래 쪽에서 위쪽으로 가면서<br />
혀, 입술, 코, 눈, 손, 몸통 순으로 대응되어 있으며 입술과 손가락은 넓은 대응 부분을 차지한다.</p>

<p>5, 7번 영역은 몸감각연합영역(Somesthetic Association Area)이다.<br />
미각은 43번 영역에서 담당한다.<br />
중요한 자극에 집중하고 선택하는 능력인 주의(Attention)는 7번과 39번 영역과 관계가 깊다.<br />
관자엽(Temporal Lobe)은 청각, 후각 등과 관련이 있다.<br />
브로드만 영역의 22, 41, 42 등이 청각정보를 받아들이고 해석하는 역할을 수행한다.<br />
뒤통수엽(Occipital Lobe)은 시각기능(Visual Function)과 관련이 깊다.<br />
17, 18, 19번 영역이 이에 해당한다.</p>
<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_3.png" /></p>

<p><br /></p>
<h1 id="4-bci-시스템-개발시-주목할-잡파">4. BCI 시스템 개발시 주목할 잡파</h1>
<p>두피에서 뇌파인식기를 이용하여 측정된 신호가 모두 뇌에서 발생한 신호가 아닐 수 있다.<br />
이처럼 두피에서 뇌파인식기를 이용하여 측정된 신호이지만 뇌가 아닌<br />
다른 곳에서 발생한 신호를 잡파(<strong>Aritifact</strong>)라 한다.<br />
잡파는 눈이나 혀움직임 또는 이 부딪치기와 같은 생리적인 요인에 의해서 발생할 수도 있고,<br />
주변 기계장치같은 비생리적 요인으로 인해 발생할 수도있다.</p>

<p>Artifact는 뇌파보다 상대적으로 전위가 강하므로 뇌파인식기를 이용해 측정하기 쉽다.<br />
따라서 BCI 시스템의 효율을 높이기 위해서는 비생리적인 요인의 잡파는 제거해야 하지만<br />
생리적인 요인의 잡파는 잘 활용할 필요가 있다.</p>

<ul>
  <li>
    <p>Sweat Artifact<br />
피부에 땀이 나 전극과 두피접촉면 사이에 저항이 변하면서 발생하는 땀잡파</p>
  </li>
  <li>
    <p>Pulse Artifact<br />
두피아래 있는 동맥에 혈액이 흐를 때 발생할 수 있는 맥박잡파</p>
  </li>
  <li>
    <p>Eyelid Flutter and Blinking Artifact<br />
눈을 깜빡일 때 발생하는 안검잡파</p>
  </li>
  <li>
    <p>Eyeball Movement Artifact<br />
안구가 상하, 좌우로 움직일 때 발생하는 안구운동잡파</p>
  </li>
  <li>
    <p>Glossokinetic Artifact<br />
혀가 움직일 때 발생하는 혀움직임잡파</p>
  </li>
  <li>
    <p>Chewing Artifact<br />
음식물을 씹을 때 발생하는 저작잡파</p>
  </li>
</ul>

<p><br /></p>
<h1 id="5-뇌파-측정을-위한-전극-부착-위치와-몽타주">5. 뇌파 측정을 위한 전극 부착 위치와 몽타주</h1>
<p>비촉침 EEG를 위해 머리에 전극을 부착해야 한다.<br />
전극 부착 위치는 국제 10-20 system을 따른다.<br />
10-20 system은 아래 그림과 같이 노드 사이의 거리를 10%, 20% 로 유지를 하기 때문에 붙여진 이름이다.</p>

<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_4.png" /></p>

<p><br />
요즘에는 아래와 같은 개선된 10-20 system을 사용하기도 한다.<br />
개선된 10-20system은 5% 간격을 이용하기도 하여<br />
더 정밀한 측정을 위한 것으로 5%system (10-5 system)라고 부른다.</p>

<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_5.png" /></p>

<p><br />
전극 위치는 영어 알파벳과 숫자를 사용하여 이름을 정한다.</p>
<ul>
  <li>C : 중앙</li>
  <li>F : 이마엽(전두엽, Frontal Lobe)</li>
  <li>P : 마루엽(두정엽, Parietal Lobe)</li>
  <li>T : 관자엽(측두엽, Temporal Lobe)</li>
  <li>O : 뒤통수엽(후두엽, Occipital Lobe)</li>
</ul>

<p>그리고 숫자는</p>
<ul>
  <li>홀수 : 좌측</li>
  <li>짝수 : 우측</li>
</ul>

<p>전극이 중앙선에서 멀어지면 숫자가 커진다.</p>

<p>뇌파 인식기는 서로 다른 두 전극 - 입력 채널 G1(Grid 1)과 G2(Grid 2) - 의 전압 차이를 측정한다.<br />
활성전극(Active Electrode) G1과 전기활동이 거의 없는 기준전극(Reference Electrode) G2<br />
사이의 전압 차(G1 - G2)를 기록하는 것을 기준전극 유도 또는 단극성유도(Monopolar Derivation)라 한다.<br />
이때 기준전극은 활성이 0에 해당하면 가장 좋지만<br />
실제로 이러한 경우는 거의 불가능하므로 전압이 비교적 일정한 귀나<br />
Cz(central midline placement of electrodes in electroencephalography)를 기준전극으로<br />
사용하는 경우가 많다.<br />
그리고 서로 다른 두 전극의 전압 차를 비교하여 기록하는 것을 양극성 유도(Bipolar Derivations)라 한다.<br />
양극성 유도의 경우 입력되는 두 전극 G1과 G2는 서로 인접해 있는 경우가 많다.<br />
이렇게 측정된 특성 시점의 전압 차는 G1-G2 값으로 기록되고 시간의 흐름과 함께 파형으로 나타나고 이를 뇌파라 한다.</p>

<p>몽타주는 뇌파 측정을 위한 전극 배열을 조합한 패턴으로<br />
뇌파 측정 영역의 공간적 구조를 나타내는 EEG 채널의 표현이다.<br />
몽타주는 크게 단극성유도를 이용하는 참조 몽타주(Reference Montage)와<br />
양극성 유도를 이용하는 비교 몽타주(Differential Montage)로 나눌 수 있다.<br />
예를 들어 Fp1, F3, C3등을 G1으로 하고 Cz를 G2로 하여 각각의 전압의 차이를 측정하면<br />
아래 a같은 Cz를 기준으로 하는 참조 몽타주가 되고<br />
b,c와 같은 귀를 기준으로 하는 참고 몽타주가 된다.</p>

<p align="center"><img style="width: 70%" src="../../../../assets/img/Paper_Review/Basic_BCI/Basic_BCI_6.png" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"><center>Reference Montage</center></th>
      <th style="text-align: center"><center>Differential Montage</center></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">장점</td>
      <td style="text-align: center">전체 위상관계를 파악하기 좋다</td>
      <td style="text-align: center">전극간 위치가 멀지 않기 때문에<br />신호 발생 위치를 비교적 정확히 측정할 수 있다</td>
    </tr>
    <tr>
      <td style="text-align: center">단점</td>
      <td style="text-align: center">미세한 국소적 위치에 따른 <br />전압의 차이나 정밀한 초점의 확인이 어렵다</td>
      <td style="text-align: center">이웃한 전극에서 비슷한 전압이 측정되면<br />신호 발생을 알아내기 어렵다</td>
    </tr>
  </tbody>
</table>

<p>BCI 시스템 개발을 위해서는 시스템의 목적에 따라 필요한 뇌파 측정을 위한 적절한 몽타주를 만들고<br />
전극 부착 위치를 정확히 결정해야 한다.<br />
예를 들어, 시각정보에 반응하는 뇌파를 측정하기 위해서는 Oz를 활성전극 G1으로 하고<br />
A1이나 A2를 기준전극 G2로 설정하는 것이다.<br />
그리고 사용자의 정확한 의도를 알아내기 위해서는 사용자의 의도에 따른 정확한 신호를 발견해야 한다.<br />
이러한 정확한 신호 발견을 위해서는 여러 몽타주에 따른 뇌파를 각각 기록하고 서로 비교하여<br />
사용자의 의도와 뇌파 간의 상관도가 높은 뇌파 내의 특징들을 발견해야 한다.</p>

<p><br /></p>
<h1 id="6-결론">6. 결론</h1>
<p>디지털시스템이 보편화되면서 몽타주의 변화, 주파수 필터링 등<br />
다양한 변화를 실시간으로 적용하면서 뇌파를 기록하고 비교 연구하기 편하게 되었다.<br />
앞으로 이러한 뇌파에 관한 연구는 더욱 활성화되고 임상적 응용뿐만 아니라<br />
BCI 등 그 적용 범위가 확대될 것으로 예측된다.<br />
본 논문에서는 뇌파에 관한 세부적 임상 관련 지식은 생략하고<br />
BCI 시스템 개발 시 필요한 뇌파에 대한 기초 지식과 특성을 알아보았다.<br />
이는 의학이 아닌 공학 기반의 연구자들이 뇌파를 기반으로하는 다양한 시스템 개발시 유용한 참고자료가 될 것이다.<br />
향후 연구에서는 다양한 환경에서 여러 영역의 뇌파를 실제 측정하고 서로 비교하여<br />
사용자의 의도와 뇌파 간의 상관도가 높은 뇌파 내의 특징들을 발견하여 효과적인 BCI 시스템 개발에 활용하도록 한다.</p>]]></content><author><name>Gi-Chul Yang</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[요약 뇌파에 관한 많은 기술적 발전에도 불구하고 정확한 뇌파의 측정은 오늘날에도 쉽지 않다. 더욱이 비침습형 뇌파인식기를 이용한 외파의 측정은 정확도가 떨어진다. 하지만 실용적인 뇌-컴퓨터 인터페이스 활용 시스템 개발을 위해서는 사용의 편의성 때문에 비침습형 뇌파인식기를 이용하는 경우가 많다. 특히, 가장 사용하기 편한 비침습형 건식 최파인식기인 경우 뇌파 측정의 정확도가 가장 떨어진다. 본 논문에서는 임상 응용보다는 뇌-컴퓨터 인터페이스 시스템 개발 시 뇌파 측정을 통한 사용자 의도 파악의 정확도를 높이는데 필요한 뇌파에 대한 기초 지식과 특성에 대해 알아본다. 뇌파 측정을 통한 사용자 의도 파악이 정해지면 다양한 분야에서 뇌파의 활용도가 높아지고 새로운 응용 분야도 개척될 것이다.]]></summary></entry><entry><title type="html">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</title><link href="http://192.168.0.41:4000/paper%20review/2022/09/11/Learning-Phrase-Representations-using-RNN-Encoder-Decoder-for-Statistical-Machine-Translation.html" rel="alternate" type="text/html" title="Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation" /><published>2022-09-11T17:17:23+09:00</published><updated>2022-09-11T17:17:23+09:00</updated><id>http://192.168.0.41:4000/paper%20review/2022/09/11/Learning-Phrase-Representations-using-RNN-Encoder%E2%80%93Decoder-for-Statistical-Machine-Translation</id><content type="html" xml:base="http://192.168.0.41:4000/paper%20review/2022/09/11/Learning-Phrase-Representations-using-RNN-Encoder-Decoder-for-Statistical-Machine-Translation.html"><![CDATA[<h1 id="abstract">Abstract</h1>
<p>이번 논문에서 새로운 신경망 모델을 소개한다.<br />
RNN Encoder-Decoder라는 이름으로 두개의 RNN으로 구성되어있다.</p>

<p>첫 번째 RNN은 symbol들의 sequnece를 고정된 길이의 vector 표현으로 인코딩하고,<br />
다른 RNN은 그 vector를 다른 symbol들의 sequnce로 디코딩한다.</p>

<p>새로운 모델의 인코더와 디코더는 source sequece가 주어졌을 때<br />
target sequence의 조건부 확률을 최대치로 만들도록 같이 학습된다.</p>

<p>Statistical machine translation 시스템의 성능은 경험적으로  이미 존재하는 log-linear model에서<br />
추가적인 feature처럼 RNN Encoder-Decoder를 통해 구문의 쌍에 대한 조건부 확률을 계산하고<br />
그것을 개선 시키는 것에 기반을 둔다.</p>

<p>질적으로 새로 고안된 모델이 의미와 문법적으로 의미있는 언어 구문들을 학습하는 것을 보여줄 것이다.</p>

<p><br /></p>
<h1 id="1-introduction">1 Introduction</h1>
<p>심층 신경망들은 객체 인식과 음성인식 같은 다양한 곳에 적용했을 때 매우 좋은 성과를 보였다.<br />
게다가 최근 많은 연구들은 신경망이 자연어처리(<strong>NLP</strong>) 작업들에서도 성공적으로 쓰일 수 있다는 것을 보였다.<br />
이것들은 language modeling, paraphrase detection과 word embedding extraction도 포함한다.</p>

<p>Statistical machine translation(<strong>SMT</strong>) 분야 에서는 심층 신경망이 좋은 결과를 도출하기 시작 되했다.<br />
Schwenk의 2012년도 논문에서는 feedforward 신경망이 구문 기만 SMT 시스템에서 잘 사용되었다고 발표했다.</p>

<p>SMT에 신경망을 사용한 Schwenk의 논문 내용과 같이,<br />
이번 논문에서는 전통적인 구문기반 SMT 시스템에 한 부분으로 사용될 수 있는 새로운 신경망 구조에 집중할 것이다.<br />
RNN Encoder-Decoder라 부르는 새로 고안된 신경망 구조는<br />
encoder와 decoder 역할을 하는 두 RNN의 쌍으로 구성된다.</p>

<p>Encoder는 다양한 길이의 sourch sequence를 고정된 길이의 vector로 mapping하고<br />
Decoder는 vector로 표현된 내용을 다시 다양한 길이의 target sequence로 mapping한다.<br />
두 네트워크들은 source sequence가 주어졌을 때<br />
target sequence의 조건부 확률을 극대화 시키도록 같이 학습된다.<br />
게다가 memory capacity와 쉬운 학습을 위해 꽤 정교한 은닉 unit을 사용할 것을 제안한다.</p>

<p>새로운 hidden unit이 있는 RNN Encoder-Decoder는 경험적으로 English를 French로 번역하는 작업을 통해 평가된다.<br />
영어 구문을 일치하는 프랑스어 구문으로 번역하도록 모델을 학습시킨다.<br />
그렇게 되면 그 모델은 각 구문 쌍을 구문 테이블에서 점수를 비교하면서<br />
기존 구문기반 SMT 시스템에 사용이 될 수 있다.<br />
경험적인 평가에서는 RNN Encoder-Decoder를 통해 구문 쌍에 점수를 부여하는 접근은 번역 성능을 향상시켜줬다.</p>

<p>학습된 RNN Encoder-Decoder의 구문 점수를 이미 존재하는 번역 모델의 것과 비교해 질적으로 분석한다.<br />
이러한 질적 분석으로 RNN Encoder-Decoder가 구문 테이블에서<br />
언어의 일반성을 인지하는데 좋다는 것을 알 수 있고<br />
이는 간접적으로 전반적인 번역 성능이 전량적으로 개선된다고 설명된다.<br />
모델에 대해 더 분석을 해보니 RNN Encoder-Decoder가 지속적으로 구문의 구조에서<br />
의미와 문법에 대한 정보를 가지고 있는 space representation을 학습하고 있다는 것을 알았다.</p>

<p><br /></p>
<h1 id="2-rnn-encoder-decoder">2 RNN Encoder-Decoder</h1>
<h2 id="21-preliminary-recurrent-neural-networks">2.1 Preliminary: Recurrent Neural Networks</h2>
<p>Recurrent neural network (<strong>RNN</strong>)은 hidden state <strong>h</strong>와<br />
다양한 길이의 sequence $\mathbf{x}=(x_1,\dots,x_T)$에 대해 선택적으로 갖는 output <strong>y</strong>로 구성된다.</p>

<p>각 time step $t$에서 RNN의 hidden state \(\mathbf{h}_{\left\langle t\right\rangle}\)가 아래 식을 통해 업데이트된다.</p>

<p>\(\begin{align}
\mathbf{h}_{\left\langle t\right\rangle}=
f\left(\mathbf{h}_{\left\langle t-1 \right\rangle},x_t \right)
\end{align}\)
여기서 f는 비선형 활성함수이다.<br />
f는 element-wise logistic sigmoid function 만큼 단순하거나<br />
LSTM unit만큼 복잡할 수도 있다.</p>

<p>RNN은 sequence의 다음 symbol을 예측하도록 학습되면서 sequence의 확률분포를 배울 수 있다.<br />
이러한 경우에 각 time step $t$ 별로 출력값은 조건부 확률분포 (e.g, $p(x_t\ |\ x_{t-1},\dots,x_1)$)이다.<br />
예를들어 multinomial distribution (1-of-K coding)은 softmax 활성함수를 이용해 출력값을 얻는다.</p>

<p>\(\begin{align}
p(x_{t,j}=1\ |\ x_{t-1},\dots,x_1)=
{\dfrac{\exp(\mathbf{w}_j\mathbf{h}_{\left\langle t\right\rangle})}{\sum\limits^K_{j\ '=1}\exp\left(\mathbf{W}_{j\ '}\mathbf{h}_{\left\langle t\right\rangle}\right)}}
\end{align}\)
가능한 모든 symbol $j=1,\dots,K$에 대하여, 
$\mathbf{w}_j$는 가중치 매트릭스 $\mathbf{W}$의 row vector이다.</p>

<p>이 확률들을 조합하면 sequence $\mathbf{x}$의 확률을 구할 수 있다.</p>

\[\begin{align}
p(\mathbf{x})=\prod^T_{t=1}p(x_t\ |\ x_{t-1},\dots,x_1)
\end{align}\]

<p>이렇게 학습한 분포로부터 각 time step에서 반복적으로 symbol을 샘플링함으로써 새로운 sequence를 제대로 샘플링 할 수 있다.</p>

<p><br /></p>
<h2 id="22-rnn-encoder-decoder">2.2 RNN Encoder-Decoder</h2>
<p>이 논문에서 다양한 길이의 sequence를 고정된 길이의 vector 표현으로 encode하고<br />
그 고정된 길이의 vector 표현을 다시 다양한 길이의 sequence로 decode하도록<br />
학습한 새로운 구조의 신경망을 설명한다.</p>

<p>확률적인 관점에서 이 새로운 모델은 어떤 가변길이 sequence로부터 조정된<br />
새로운 가변길이 sequence의 조건부 확률분포를 배우는 일반적인 방법이다.<br />
e.g. $p(y_1,\dots,y_{T’}\ |\ x_1,\dots,x_T)$<br />
여기서 입력 sequence의 길이인 $T$와 출력 sequence의 길이인 $T’$의 값은 다를 수 있다.</p>

<p>Encoder는 입력 sequence $\mathbf{x}$의 각 symbol을 순차적으로 읽는 RNN이다.<br />
Encoder가 각 symbol을 읽듯이 RNN의 hidden state는 아래 식을 통해 변한다.</p>

\[\begin{align*}
\mathbf{h}_{\left\langle t\right\rangle}=
f\left(\mathbf{h}_{\left\langle t-1 \right\rangle},x_t \right)
\end{align*}\]

<p>Sequence의 마지막(end-of-sequence symbol로 표시됨)까지 읽고 나면,<br />
RNN의 hidden state는 전체 입력 sequence <strong>c</strong>의 요약본이 된다.</p>

<p>새로운 모델의 decoder는 또다른 RNN으로  주어진 hidden state \(\mathbf{h}_{\left\langle t \right\rangle}\) 를 가지고<br />
다음 symbol $y_t$를 예측함으로써 output sequence를 만들도록 학습된다.<br />
그러나 2.1절에서 말한 내용과 달리 $y_t$ 와 \(\mathbf{h}_{\left\langle t \right\rangle}\) 들은 $y_{t-1}$과 입력 sequence의 요약 <strong>c</strong>에의해 조정된다.</p>

<p>따라서 time t에서의 decoder의 hidden state는 아래와 같이 계산된다.</p>

\[\begin{align*}
\mathbf{h}_{\left\langle t\right\rangle}=
f\left(\mathbf{h}_{\left\langle t-1\right\rangle},y_{t-1},\mathbf{c} \right)
\end{align*}\]

<p>비슷하게 다음 symbol의 조건부 분포는 아래와 같다.</p>

\[\begin{align*}
P\left(y_t\ |\ y_{t-1},y_{t-2},\dots,y_1,\mathbf{c} \right)=
g\left(\mathbf{h}_{\left\langle t\right\rangle},y_{t-1},\mathbf{c} \right)
\end{align*}\]

<p>여기서 f와 g는 주어진 활성함수이고 g는 softmax같은 것을 사용해 반드시 합당한 확률값을 반환해야한다.</p>

<p>Fig 1을 보면 새로 고안된 모델의 구조를 그림으로 표현한 것을 볼 수 있다.<br />
<img src="/assets/img/Paper_Review/GRU/GRU_0.png" alt="GRU_0" /></p>

<p>RNN Encoder-Decoder에을 구성하는 두가지 RNN은<br />
conditional log-likelihood를 극대화 시키도록 함께 학습된다.</p>

\[\begin{align}
\max\limits_\theta \dfrac{1}{N}\sum\limits^{N}_{n=1}log\ p_\theta(\mathbf{y}_n\ |\ \mathbf{x}_n)
\end{align}\]

<p>$\theta$는 모델 파라미터들의 집합이고 각 $(\mathbf{x}_n,\mathbf{y}_n)$는 (input sequence, output sequence) 쌍으로<br />
training set에서 가져온다.<br />
여기서는 decoder의 출력으로써 입력 처음부터 미분 가능하기 때문에<br />
gradient 기반 알고리즘을 사용하여 모델 파라미터를 평가한다.</p>

<p>일단 RNN Encoder-Decoder가 학습된다면, 모델은 두가지 방법으로 사용될 수 있다.<br />
하나는 모델을 input sequence가 주어졌을 때 target sequence를 만들기 위해 사용하는 것이다.<br />
다른 하나는 모델을 식3,4를 이용해 간단하게 $p_\theta(\mathbf{y}\ |\ \mathbf{x})$를 점수로<br />
input과 output sequence의 쌍을 점수화하기 위해 사용하는 것이다.</p>

<p><br /></p>
<h2 id="23-hidden-unit-that">2.3 Hidden Unit that</h2>
<h2 id="adaptively-remembers-and-forgets">             Adaptively Remembers and Forgets</h2>
<p>새로운 모델 구조에 추가적으로 새로운 타입의 hidden unit (식1의 $f$)을 생각했다.<br />
새로운 hidden unit은 LSTM unit으로부터 영감을 받았지만 그보다 더 쉽게 계산되고 쉽게 구현할 수 있다.<br />
Fig2 에서는 hidden unit을 그림으로 묘사했다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_1.png" alt="GRU_1" /></p>

<p>j 번째 hidden unit이 어떻게 계산되는지 알아보겠다.<br />
먼저 reset gate $r_j$는 아래와 같이 계산된다.</p>

\[\begin{align}
r_f=
\sigma\left(\left[\mathbf{W}_r\mathbf{x} \right]_j+
\left[\mathbf{U}_r\mathbf{h}_{\left\langle t-1\right\rangle} \right]_j \right)
\end{align}\]

<p>여기서 $\sigma$는 logistic sigmoid 함수이고 \([.]_j\) 는 vector의 j 번째 원소를 뜻한다.<br />
$\mathbf{x}$는 입력이고 $\mathbf{h}_{t-1}$는 이전 hidden state이다.<br />
$\mathbf{W}$와 $\mathbf{U}$는 학습되는 가중치 매트릭스이다.</p>

<p>비슷하게 update gate $z_j$는 아래와 같이 계산된다.</p>

\[\begin{align}
z_j=
\sigma\left(\left[\mathbf{W}_z\mathbf{x} \right]_j+
\left[\mathbf{U}_z\mathbf{h}_{\left\langle t-1\right\rangle} \right]_j \right)
\end{align}\]

<p>그리고 unig $h_j$의 실제 활성은 아래를 통해 이루어진다.</p>

\[\begin{align}
h^{\left\langle t\right\rangle}_j=z_jh^{\left\langle t-1\right\rangle}_j+
(1-z_j)\tilde{h}^{\left\langle t\right\rangle}_j\\
\tilde{h}^{\left\langle t\right\rangle}_j=
\phi\left(\left[\mathbf{Wx}\right]_j+
\left[\mathbf{U}\left(\mathbf{r}\odot\mathbf{h}_{\left\langle t-1\right\rangle} \right) \right]_j \right)
\end{align}\]

<p>위 식들을 통해 reset gate가 0에 가까워지면 hidden state가 previous hidden state를 무시하고<br />
현재 입력 만으로 재설정된다.<br />
이러한 과정은 hidden state가 앞으로 관련성이 없는 정보를 drop시켜서 효과적으로 더 알찬 표현을 만들도록 한다.</p>

<p>다른 한편으로는 update gate가 이전 hidden state로부터 현재 hidden state로<br />
정보를 얼만큼 전달할 것인지 조절하는데<br />
이러한 활동이 LSTM의 memory cell이 하던 것과 비슷하고<br />
RNN이 long-term 정보를 기억하게 해준다.<br />
게다가 이러한 것은 leaky-integration unit의 다양성에 적응을 고려했다고도 볼 수 있다.</p>

<p>각 hidden unit이 분리된 reset gate와 update gate를 갖는 것처럼<br />
각 hidden unit은 다른 time scale들에서 의존성을 포착하도록 학습할 것이다.<br />
short-term 의존성을 포착하도록 학습된 이 unit들은 자주 활성화된는 reset gate를 갖는 경향을 보일 것이다.<br />
하지만 longer-term 의존성을 포착하는 것들은 자주 활성화되는 update gate를 갖게 될 것이다.</p>

<p>사전 실험에서 이 새로운 unit을 gating unit들과 사용하는 것이 중요하다는 것을 알아냈다.<br />
이런 gating unit 없이 자주 사용되는 tanh 만으로는 의미 있는 결과를 낼 수 없었다.</p>

<p><br /></p>
<h1 id="3-statistical-machine-translation">3 Statistical Machine Translation</h1>
<p>보통 사용되는 Statistical machine translation system(<strong>SMT</strong>)에서<br />
시스템(특히 decoder)의 목적은 아래 값을 극대화 시키며 주어진 source sentence <strong>e</strong>를 통해 번역된 <strong>f</strong>를 찾는 것이다.</p>

\[\begin{align*}
p(\mathbf{f}\ |\ \mathbf{e}) \propto
p(\mathbf{e}\ |\ \mathbf{f})\ p(\mathbf{f})
\end{align*}\]

<p>오른쪽 부분에서 첫 번째 항은 translation model이고 뒤에는 language model이다.<br />
하지만 사실 대부분의 SMT 시스템들은 추가적인 feature와 일치하는 가중치들로<br />
log-linear 모델처럼 $\log\ p(\mathbf{f}\ |\ \mathbf{e})$를 이용한다 :</p>

\[\begin{align}
\log p(\mathbf{f}\ |\ \mathbf{e})=
\sum\limits^N_{n=1}w_nf_n(\mathbf{f},\mathbf{e})+
\log Z(\mathbf{e})
\end{align}\]

<p>$f_n$은 n 번째 feature를 뜻하고 $w_n$은 n 번째 가중치를 뜻한다.<br />
$Z(\mathbf{e})$는 정규화 상수로 가중치의 영향을 받지 않는다.<br />
가중치들은 BLEU 점수를 최대화하기 위해 자주 최적화된다.</p>

<p>구문 기반 SMT 프레임워크에서는 번역 모델 \(\log p(\mathbf{e}\ \vert\ \mathbf{f})\)는<br />
source sentence와 target sentence의 번역 확률로 분해된다.<br />
이러한 확률들은 식9를 이용해 다시 한번 log-linear모델에서 추가적인 feature들을 고려한다.<br />
또한 BLEU 수치를 최대화하기 위해 가중치가 적용된다.</p>

<p>언어모델을 위한 신경망이 제안되었기 때문에,<br />
신경망들은 SMT 시스템에서 널리 사용되게 되었다.<br />
많은 경우에 신경망들은 번역을 하기 위한 가정들을 새로 점수를 게산한다. (n-best lists)
그러나 최근에는 source sentence의 표현을 추가적인 입력으로 사용하여<br />
구문 쌍이나 번역된 문장에 점수를 부여해 신경망을 학습 시키는 방법에 관심이 모이고있다.</p>

<p><br /></p>
<h2 id="31-scoring-phrase-pairs-with-rnn-encoder-decoder">3.1 Scoring Phrase Pairs with RNN Encoder-Decoder</h2>
<p>2.2절에서 말한 RNN Encoder-Decoder를 구문 쌍에 대한 테이블에서 학습시키고<br />
SMT decoder를 튜닝할 때 식9를 이용해 log-linear 모델에서 그 테이블의 점수들을 추가적인 feature로 사용한다.</p>

<p>RNN Encoder-Decoder를 학습 시킬 때,<br />
원래 글의 내용에서 각 구문 쌍의 정규화된 빈도수를 무시한다.<br />
이러한 방법은 정규화된 빈도수에 따라 큰 구문 테이블로부터 구문 쌍을 임의로 골라 계산하는 비용을 줄이고<br />
RNN Encoder-Decoder가 단순히 구문 쌍을 그것의 출현 횟수로만 순위를 매기는 것이 아님을 보장한다.</p>

<p>이러한 선택에 대한 기저에 깔린 한가지 이유는<br />
구문 테이블에서 번역 가능성의 존재는 이미 입력된 본문에서 구문쌍의 빈도수를 반영한 것이다.<br />
고정된 RNN Encoder-Decoder의 용량에서<br />
모델의 대부분의 용량은 언어의 규칙성을 학습하는데에 초점을 두려고 했다.<br />
언어의 규칙성이란 다시말해 잘 번역된것과 잘 번역되지 않은 것을 구별하는것 또는<br />
잘 번역된 다양한것을 배우는것을 뜻한다.</p>

<p>일단 RNN Encoder-Decoder가 학습되면 이미 존재하는 구문 테이블의 각 구문 쌍에 새로운 점수를 추가한다.<br />
이것은 새로운 점수들이 계산량을 조금 추가하여 존재하는 튜닝 알고릴즘에 적용되도록 한다.</p>

<p>Schwenk가 말하길 이미 존재하는 구문 테이블을 고안된 RNN Encoder-Decoder로 대체하는 것이 가능하다고 한다.<br />
그런 경우 주어진 source 구문에서 RNN Encoder-Decoder는<br />
좋은 target 구문의 리스트를 만들 필요가 있을 것이다.<br />
그러나 이것은 반복적으로 수행되는 샘플링 과정이 매우 expensive할 것이다.<br />
그러므로 이 논문에서 구문 테이블에서 구문 쌍들을 다시 점수를 매기는것만 고려하려고 한다.</p>

<p><br /></p>
<h2 id="32-related-approaches-neural-networks-in-machine-translation">3.2 Related Approaches: Neural Networks in Machine Translation</h2>
<p>경험적인 결과를 발표하기 전에, 우리는 문맥 SMT에서 신경망을 사용하도록 제안하는 수많은 연구들을 살펴봤다.</p>

<p>Schwenk는 2012년에 구문 쌍에 점수를 매기는 것과 비슷한 접근법을 제안했다.<br />
RNN 기반 신경망 대신에 그는 고정된 크기의 입력(그의 경우 짧은 구문은 zero-padding을 하면서<br />
7개의 단어를 받는다)을 받고 target 언어로 7개의 단어로 구성된<br />
고정된 크기위 출력을 갖는 feedforward 신경망을 사용했다.<br />
이 feedforward 신경망이 SMT 시스템에서 구문에 점수를 매기는데 사용되는 경우에<br />
가장 긴 구문은 종종 짧은 것으로 선택된다.<br />
그러나 구문의 길이가 길어지거나 다른 가변 길이의 sequence data에 신경망을 적용하기 때문에,<br />
신경망이 가변 길이의 입력과 출력을 다루는 것은 중요하다.<br />
여기서 제안된 RNN Encoder-Decoder는 이러한 적용에 잘 들어맞는다.</p>

<p>2012년도의 Schwenk와 비슷하게 2014년에 Devlin이 feedforward 신경망을 번역 모델을 만드는데 사용했고<br />
target 구문에서 한번에 한 단어를 예측하는 모델이었다.<br />
그들은 인상적인 개선을 보였지만 그들의 접근은 여전히 사전에 고정된 입력 구문의 최대 길이를 필요로 했다.</p>

<p>비록 그들이 학습시킨 것은 완전히 신경망은 아니지만,<br />
2013년의 Zou의 논문의 저자들은 단어나 구문 embedding에서<br />
두 언어를 자유롭게 구사하도록 학습시킬 것을 제안했다.<br />
그들은 SMT 시스템에서 구문 쌍의 점수들을 추가적으로 사용하여<br />
구문들의 쌍 사이에 거리를 계산해 embedding하도록 학습 시켰다.</p>

<p>2014년에 Chandar의 논문에서는 feedforward 신경망을<br />
입력 구문의 표현인 bag-of-words에서 출력 구문으로 mapping 하도록 학습시켰다.<br />
이 내용은 그들이 입력 구문의 표현을 bag-of-words로 사용한 것을 제외하면<br />
RNN Encoder-Decoder와 2012년에 Schwenk가 제안한 모델과 매우 유사했다.<br />
Bag-of-words를 사용한 비슷한 접근은 2013년 Gao의 논문에서 등장했다.<br />
이전에 2011년에 Socher가 encoder-decoder 모델과 비슷하게 두가지 RNN을 사용했었지만<br />
그들의 모델은 한가지 언어에 대한 설정(모델이 input sequence를 다시 만드는 방식)으로 한정되어 있었다.<br />
더 최근에는 또다른 RNN을 사용하는 encoder-decoder 모델이 2013년에 Auli의 논문에서 제안되었다.<br />
여기서 decoder는 source sentence와 source context에 의해 조정되었다.</p>

<p>RNN Encoder-Decoder와 두가지 논문(2013년의 Zou의 논문과 2014년의 Chandar의 논문)<br />
사이에 한가지 중요한 차이점은<br />
source와 target 구문의 단어들의 순서를 고려한다는 것이다.<br />
앞서 말한 접근들은 순서에 대한 정보를 효과적으로 무시하는 반면에<br />
RNN Encoder-Decoder는 자연스럽게 같은 단어를 갖는 문장이더라도 순서가 다른 문장들을 서로 구분한다.<br />
여기서 제안하는 RNN Encoder-Decoder와 가장 비슷한 접근법은<br />
2013년에 Kalchbrenner와 Blunsom이 제안한 Recurrent Continuous Translation Model(Model 2)이다.</p>

<p>그들의 논문에서 그들은 encoder와 decoder로 구성된 비슷한 모델을 제안했다.<br />
우리 모델과의 차이점은 그들은 convolutional n-gram model (CGM)을 encoder로 사용하고<br />
CGM의 inverse 버전과 RNN을 합친것을 decoder로 사용했다.<br />
그러나 그들은 그들의 모델을 전통적인 SMT 시스템의 n-best list의 점수를 다시 매기면서 평가하도록 제안했고<br />
그 기준 번역의 PPL을 계산했다.</p>

<p><br /></p>
<h1 id="4-experiments">4 Experiments</h1>
<p>우리의 접근 방법을 WMT’14 workshop의 영어/프랑스어 번역 작업을 통해 평가했다.</p>

<h2 id="41-data-and-baseline-system">4.1 Data and Baseline System</h2>
<p>WMT’14 번역작업의 프레임워크에서는 영어/프랑스어 SMT시스템이 만들어지도록 많은 양의 데이터를 사용할 수 있다.<br />
두가지 언어를 자유롭게 구사하는 본문은 Europarl(61M 단어들), news comentary (5.5M), UN (421M) 그리고 두개의 crawl된 말들이 90M과 780M 만큼 포함되어있다.<br />
마지막 두개의 본문은 꽤 noisy하다.<br />
프랑스어 모델을 학습 시키기 위해,<br />
712M개의 신문 자료로부터 crawl된 단어들을 bitext의 target side에서 추가로 사용가능하다.<br />
모든 단어들의 횟수는 tokenization 후에 프랑스 단어를 가리킨다.</p>

<p>모든 데이터들을 연결해서 만든 데이터로 확률적 모델들을 학습시키는 것은 최적의 성능으로 무조건 이끌지 않고<br />
다루기 힘든 극도로 큰 모델들의 결과를 만든다.</p>

<p>대신에 주어진 작업에서 데이터의 가장 관련있는 하위집합에서 집중해야하는 것은<br />
2010년 Moore와 Lewis의 방법으로 데이터를 선택적으로 적용하고<br />
2011년 Axelrod처럼 bitext로 확장시켰었다는 것이다.<br />
이것은 우리가 RNN Encoder-Decoder를 학습 시키기 위해 850M 중 348M개의 단어를 갖는 하위 집합을 택하고<br />
2G중 418M개의 단어를 갖는 하위집합을 택했다는 것이다.<br />
Test set newtest2012와 newtest2013를 데이터 선택에 사용하고<br />
가중치 튜닝은 MERT로 하고 test set으로 newtest2014를 사용했다.<br />
각 집합은 7만개가 넘는 단어를 갖고 single reference translation도 갖는다.<br />
RNN Encoder-Decoder와 신경망을 학습하기 위해,<br />
우리는 source와 target vocabulary를 가장 많이 등장하는 15000개의 단어들로<br />
영어와 프랑스어에 모두 제한을 두었다.<br />
이 범위는 데이터셋의 93%를 차지하는 범위이다.<br />
vocabulary에 있지 않은 모든 단어들은 special token ([UNK])로 mapping했다.</p>

<p>구문 기반 SMT 시스템의 기반은 Moses를 통해 기본설정을 사용해 만들었다.<br />
이 시스템 구조는 개발과 test set에서 각각 BLEU 점수를 30.64와 33.3를 달성했다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_2.png" alt="GRU_2" /></p>

<p><br /></p>
<h3 id="411-rnn-encoder-decoder">4.1.1 RNN Encoder-Decoder</h3>
<p>실험에서 사용된 RNN Encoder-Decoder는 encoder와 decoder에서<br />
1000개의 hidden unit과 제안된 gate를 사용했다.<br />
입력 symbol $x_{\left\langle t\right\rangle}$와 hidden unit 사이의 입력 매트릭스는 두개의 low-rank matrices로 나뉘고<br />
output matrix도 비슷하다.<br />
rank-100 매트릭스를 사용했고 이는 각 단어를 100차원으로 임베딩하도록 학습하는 것과 같다.<br />
식8에서 $\tilde{h}$ 계산을 위해 사용된 활성 함수는 tanh함수이다.<br />
decoder에서 hidden state에서 출력으로 계산하는 것은 2014년 Pascanu의 심층 신경망 작업처럼<br />
단일 중간층을 가지는데 그 중간층에는 2개의 입력을 pooling하여 500개의 maxout unit을 통해 구현했다.</p>

<p>RNN Encoder-Decoder의 모든 가중치 파라미터들은 가우시안 분포를 따르는<br />
표준편차가 0.01이고 평균이 0인 상태에서 샘플링하여 초기화를 했고 recurrent weight parameters는 예외이다.<br />
Recurrent weight parameters의 경우에는 먼저 가우시안 분포에서 샘플링하고 남은 singular vectors matrix를 사용했다.</p>

<p>Adadelta와 SGD를 RNN Encoder-Decoder를 학습하기 위해 사용했고<br />
하이퍼파라미터는 $\epsilon=10^{-6}, \rho=0.95$를 사용했다.<br />
각 엄데이트에서는 348M개의 단어들로 만든 구문 테이블에서 구문쌍을 임의로 64개 골라서 사용했다.<br />
모델 학습에는 대략 3일이 걸렸다.</p>

<p>실험에서 사용된 구조의 자세한 내용은 보충 자료에 더 깊이있게 서술했다.</p>

<p><br /></p>
<h3 id="412-neural-language-model">4.1.2 Neural language Model</h3>
<p>RNN Encoder-Decoder가 구문 쌍에 점수를 매기는 것의 효과를 평가하기 위해,<br />
target language model을 학습하기 위해 신경망을 사용해 더 전통적인 접근 방법을 사용했다.(CSLM)<br />
특히, CSLM을 사용한 SMT 시스템과 RNN Encoder-Decoder를 이용해 구문에 점수를 매기는 방법을<br />
사용하는 것에 대한 비교는 여러개의 신경망이 SMT 시스템이 풍부해지도록<br />
다른 부분에 기여하는 정도에 따라 명확해질 것이다.</p>

<p>CSLM 모델을 target 본문의 7-grams에서 학습시켰다.<br />
각 입력 단어는 $\mathbb{R}^{512}$ 임베딩 공간으로 사영을 시켰고,<br />
그것들을 이어 붙여서 3072차원의 벡터를 형성했다.<br />
연결된 벡터는 두개의 rectified layer(1536과 1024 크기인)로 입력되었다.<br />
output layer는 간단한 softmax 층이다.<br />
모든 가중치 파라미터들은 똑같이 -0.01과 0.01 사이에서 초기화 시켰고<br />
모델은 validation perplexity가 10 epoch동안 더 개선되지 않을 때까지 학습시켰다.<br />
학습이 끝나고 언어 모델은 45.80 PPL을 달성했다.<br />
Validation set은 본문의 0.1%를 임의로 골라 사용했다.<br />
모델은 n-best list rescoring보다 BLEU 점수를 더 받도록 decoding 과정에서<br />
부분적 번역에 점수를 측정하는데 사용했다.</p>

<p>decoder에서 CSLM의 사용으로 인한 계산 복잡도 문제를 해결하기 위해<br />
buffer를 사용해 decoder가 stack-search를 수행하는 동안 n-grams를 합쳤다.<br />
buffer가 차거나 stack이 삐뚤어진 경우에만 n-grams가 CSLM에 의해 점수를 받는다.<br />
이것은 Theano를 사용하는 GPU에서 matrix-matrix 곱연산을 빠르게 하도록 해준다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_3.png" alt="GRU_3" /></p>

<p><br /></p>
<h2 id="42-quantitative-analysis">4.2 Quantitative Analysis</h2>
<p>아래와 같은 조합을 사용했다.</p>
<ol>
  <li>Baseline configuration</li>
  <li>Baseline + RNN</li>
  <li>Baseline + CSLM + RNN</li>
  <li>Baseline + CSLM + RNN + Word penalty</li>
</ol>

<p>결과는 아래와 같다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_2.png" alt="GRU_2" /></p>

<p>예상했던 대로, 신경망에 의해 계산된 feature들을 추가하는 것은<br />
꾸준히 baseline 성능을 넘어 개선되었다.</p>

<p>가장 좋은 성능은 CSLM과 RNN Encoder-Decoder를 모두 사용 했던 경우였다.<br />
이러한 내용은 CSLM과 RNN Encoder-Decoder의 기여도는 서로 지나치게 영향을 주지 않고<br />
독립적으로 각 방법을 개선함으로써 더 좋은 결과를 기대할 수 있다는 것을 말해준다.<br />
게다가 신경망이 모르도록 단어의 수를 제한했다.<br />
우리는 단순히 식9에서 log-linear모델이 추가적인 feature로 unknown 단어의 수를 추가했다.<br />
그러나 이런 경우에 우리들은 test set에서 좋은 성능을 얻지 못했지만<br />
development set에서는 얻을 수 있었다.</p>

<p><br /></p>
<h2 id="43-qualitative-analysis">4.3 Qualitative Analysis</h2>
<p>성능 개선이 어디서부터 이루어지는지 이해하기 위해,<br />
우리는 구문 쌍의 점수를 RNN Encoder-Decoder를 통해 translation model로부터 $p(\mathbf{f}\ \vert\ \mathbf{e})$를 계산했다.<br />
이미 존재하는 translation model은 오로지 본문의 구문 쌍의 통계에 의존하기 때문에,<br />
우리는 그것의 점수들을 많이 등장하는 구문에 좋은 평가가되고  <br />
드물게 등장하는 구문에는 안좋게 평가가 될것이라 예상했다.<br />
또한 앞서 3.1절에서 말한 것처럼 빈도에 대한 정보를 빼고 구문 쌍에 점수를 매기는 RNN Encoder-Decoder는<br />
본문에서 구문의 확률적 등장보다 언어의 규칙성에 기반을 둘것이라 생각한다.</p>

<p>source 구문이 3단어 이상으로 이루어진 긴 source 구문의 쌍과 자주 등장하는 쌍에 초점을 둔다.<br />
이러한 각 source 구문들인 경우 우리는 target 구문에서<br />
번역 확률 $p(\mathbf{f}\ \vert\ \mathbf{e})$ 또는 RNN Encoder-Decoder에서 높은 점수를 받는 것을 찾는다.<br />
비슷하게, 길지만 빈도가 낮은 source 구문의 구문 쌍에 대해서도 같은 처리과정을 수행한다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_4.png" alt="GRU_4" /></p>

<p>Table 2는 source 구문에 대하여 translation model과 RNN Encoder-Decoder가<br />
가장 선호하는 target 구문을 3개씩 고른 것이다.<br />
Source 구문들은 4개나 5개 이상의 단어들을 갖는 긴 구문들 중에서 임의로 고른 것이다.</p>

<p>대부분의 경우에 RNN Encoder-Decoder의 target 구문들을 선택하는 것은 실질적이거나 문헌적 번역과 비슷하다.<br />
우리는 일반적으로 RNN Encoder-Decoder가 짧은 구문을 선호하는 것을 관찰했다.</p>

<p>흥미로운 것은 많은 구문 쌍들이 translation model과 RNN Encoder-Decoder에 의해 비슷한 방법으로 매겨졌지만 근본적으로 다른 점수를 갖는 구문 쌍들도 있었다.(Fig3 참조)</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_3.png" alt="GRU_3" /></p>

<p>이것은 앞서 설명했듯이 RNN Encoder-Decoder가 단순히 본문에서 구문 쌍의 빈도수를 학습하지 않고 유일한 구문 쌍을 학습하는 접근 방식 때문에 생길 수 있다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_5.png" alt="GRU_5" /></p>

<p>게다가 Table3 에서는 RNN Encoder-Decoder로부터 반들어진 Table2 의 source 구문을 보인다.<br />
각 source 구문은 그들의 점수에 따라 50개의 샘플들을 만들어 가장 좋은 5개의 구문을 골랐다.<br />
여기서 RNN Encoder-Decoder가 실제 구문 테이블을 보지 않고<br />
target 구문을 잘 만들어 제안하는 것을 볼 수 있었다.<br />
중요한건 이렇게 만들어진 구문들은 구문 테이블의 target 구문과 전혀 겹치지 않는다는 것이다.<br />
이것은 우리가 구문 테이블의 일부나 전체를 앞으로 RNN Encoder-Decoder가 제안하는 것으로 대체할 가능성을 조사해볼만 하다고 느끼게 한다.</p>

<p><br /></p>
<h2 id="44-word-and-phrase-representations">4.4 Word and Phrase Representations</h2>
<p>RNN Encoder-Decoder는 기계 번역을 위해서만 설계된 것이 아니기 때문에,<br />
학습된 모델의 특성을 간단하게 살펴보겠다.</p>

<p>신경망을 사용하는 연속적인 공간의 언어 모델들은<br />
뜻에 대해 의미가 있는 embedding을 학습 할 수 있다고 알려져있다.<br />
RNN Encoder-Decoder가 projection을 하고<br />
단어들의 sequence로부터 다시 연속적인 벡터 공간으로 mapping하기 때문에,<br />
제안된 모델에서 비슷한 속성을 관찰할 것이라 예상했다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_6.png" alt="GRU_6" /></p>

<p>Fig4에서 왼쪽 부분은 RNN Encoder-Decoder로부터 학습한<br />
word embedding matrix를 사용해 만든 2-D 임베딩을 보여준다.<br />
Projection은 2013 van der Maaten의 논문에서 제안한 Barnes-Hut-SNE를 따라 했다.<br />
우리는 정확하게 의미적으로 비슷한 단어들이 서로 군집을 형성하는 것을 확인했다.(Fig4의 확대된 그림을 봐라)</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_7.png" alt="GRU_7" /></p>

<p>RNN Encoder-Decoder는 자연적으로 구문의 표현을 연속된 공간으로 만들어낸다.<br />
Fig1의 <strong>c</strong>의 표현은 1000차원의 벡터이다.<br />
단어의 표현도 비슷하게, 우리는 Fig5에서 Barnes-Hut-SNE를 사용해 4개 이상의 단어들로 구성된 구문의 표현을 시각화 했다.</p>

<p><img src="/assets/img/Paper_Review/GRU/GRU_9.png" alt="GRU_9" /></p>

<p>이 시각화로부터,<br />
RNN Encoder-Decoder가 구문의 의미적인 내용과 문법적인 내용의 구조를 파악한다는 것이 명확해졌다.<br />
예를 들어, 왼쪽 아래 그림에서, 문법적으로 비슷하고 대부분의 구문들은 시간에 대한 내용을 표현하는 것들끼리 군집을 형성하고 있다.<br />
오른쪽 아래 그림에서는 의미적으로 비슷한(국가나 지역) 구문의 군집을 보여주고 있다.<br />
다른 한편으로, 오른쪽 위 그림에서는 문법적으로 비슷한 구문들을 보여준다.</p>

<p><br /></p>
<h1 id="5-conclusion">5 Conclusion</h1>
<p>이번 논문에서, 우리는 새로운 신경망 구조를 제안했다.<br />
그것을 RNN Encoder-Decoder라 부르며 임의의 길이를 갖는 sequence로부터<br />
임의의 길이를 갖는 (다른 집합에 속한 것도 가능한)다른 sequence로 mapping 하도록 배울 수 있는 모델이다.<br />
RNN Encoder-Decoder는 조건부 확률적으로 sequence 쌍에 점수를 부여하는 것과<br />
주어진 source sequence로부터 target sequence를 만들어내는 것이 가능하다.<br />
새로운 구조를 통해 reset gate와 update gate를 통해 적응적으로<br />
sequence를 읽거나 만드는 동안에 각 hidden unit들을 얼만큼 기억할지 또는 얼만큼 잊어버릴지를 결정하도록<br />
새로운 hidden unit을 제안했다.</p>

<p>RNN Encoder-Decoder를 구문 테이블에서 각 구문 쌍에 점수를 매기는데 사용하여<br />
SMT 작업을 통해 새로 제안한 모델을 평가했다.<br />
질적으로, 새로운 모델이 구문 쌍에서 언어의 규칙성을 잘 파악하고<br />
RNN Encoder-Decoder가 target 구문을 잘 만들어서 제안하는 것을 관찰할 수 있었다.</p>

<p>RNN Encoder-Decoder에 의한 점수들은 BLEU 수치 를 통해 전반적인 번역 성능이 개선되는 것을 찾을 수 있었다.<br />
또한, RNN Enocder-Decoder의 기여도가 SMT 시스템에서 신경망을 이용한 접근과 꽤 독립적이라는 것을 알았다.<br />
따라서 우리는 RNN Encoder-Decoder와 신경망 언어모델을 같이 사용함으로써<br />
성능을 더 개선할 수 있었다.</p>

<p>우리의 학습된 모델에 대한 질적인 분석은<br />
그것이 실제로 다양한 수준(단어나 문장 수준)의 언어의 규칙성을 파악했다고 본다.<br />
이러한 내용은 RNN Encoder-Decoder로부터 이득을 얻을 수 있는 적용과 관련해<br />
더 많은 자연어가 있을 것이라 말해준다.</p>

<p>고안된 구조는 분석과 개선에 있어서 큰 잠재적 능력을 가지고 있다.<br />
여기서 조사한 것은 아니지만 어떤 접근에서는 RNN Encoder-Decoder가 제안한 target 구문을<br />
구문 테이블의 일부나 전체를 대체하는 방법을 사용했다.<br />
또한, 제안된 모델은 글로된 언어에만 제한되지 않고,<br />
음성 내용같은 곳에 이 구조를 적용하는 것이 앞으로 중요한 연구가 될 것이다.</p>]]></content><author><name>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, etc.</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[Abstract 이번 논문에서 새로운 신경망 모델을 소개한다. RNN Encoder-Decoder라는 이름으로 두개의 RNN으로 구성되어있다.]]></summary></entry><entry><title type="html">Long Short-Term Memory Based Recurrent Neural Network Architectures for large vocabulary speech recognition</title><link href="http://192.168.0.41:4000/paper%20review/2022/09/08/LSTM-for-large-vocabulary-speech-recognition.html" rel="alternate" type="text/html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for large vocabulary speech recognition" /><published>2022-09-08T18:54:43+09:00</published><updated>2022-09-08T18:54:43+09:00</updated><id>http://192.168.0.41:4000/paper%20review/2022/09/08/LSTM-for-large-vocabulary-speech-recognition</id><content type="html" xml:base="http://192.168.0.41:4000/paper%20review/2022/09/08/LSTM-for-large-vocabulary-speech-recognition.html"><![CDATA[<h1 id="abstract">Abstract</h1>
<p>Long Short-Term Memory (<strong>LSTM</strong>)은<br />
<strong>Vanilla RNNs</strong>의 <strong>vanishing/exploding gradien</strong>t 문제를 다루기 위해 고안된 RNN구조다.</p>

<p>feedforward 신경망과는 다르게 RNNs는 sequence 모델링이 잘 되도록 순환적인 연결을 갖는다.</p>

<p>RNNs는 sequence labeling과 sequence prediction( handwriting recognition, language modeling, phonetic labeling of acoustic frames, etc.)에 사용되어 왔다.</p>

<p>그러나, 심층 신경망과 다르게, speech recognition에서 RNNs의 사용은<br />
작은 규모의 phone recognition으로 제한되었다.</p>

<p>이 논문에서는 RNN구조를 기반으로 많은 단어의 음성 인식을 잘 학습하도록 새롭게 만든 LSTM을 소개한다.</p>

<p>LSTM과 RNN과 DNN 모델들을 다양한 파라미터들과 구성상태로 학습하고 비교한다.</p>

<p>결과적으로 LSTM은 빠르게 수렴하고 비교적 작은 크기의 모델에 최신 speech recognition 성능을 제공하는 것을 확인할 것이다.</p>

<p><strong>Index Terms– Long Short-Term Memory, LSTM,</strong><br />
                         
<strong>recurrent neural network, RNN, speech recognition</strong></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="1-introduction">1. Introduction</h1>
<p>Deep neural network(<strong>DNN</strong>) 같은 fedforward neural networks (<strong>FFNN</strong>)와 달리,<br />
Recurrent neural networks(RNNs)의 구조는 현재 input에 대해<br />
결정을 만들어내기 위해 이전 time step의 활동 또한 입력으로 받는<br />
순환적인 feeding 형태를 갖는다.</p>

<p>이전 time step의 활동은 네트워크의 내부 상태에 저장되고<br />
FFNN에서 입력으로 사용되는 고정된 맥락의 window와는 다르게<br />
절대적이지 않은 일시적인 맥락에 대한 정보를 제공한다.</p>

<p>그러므로, RNN은 전체 sequence에 걸쳐 고정된 정적인 크기의 window를 적용하지 않고<br />
역동적으로 변하는 크기의 contextual window를 모든 sequence에 적용한다.</p>

<p>이러한 능력은 RNN을 sequence prediction과 sequence labeling같은<br />
sequence modeling 작업에 더 적합하게 만든다.</p>

<p><br />
그러나, gradient-based 방법인 Back-propagation through time (<strong>BPTT</strong>) 기술을 이용해<br />
전통적인 RNN을 학습 시키는 것은 vanishing / exploding gradient problem 때문에 어렵다.</p>

<p>게다가 이러한 문제들은 RNN이 입력과 출력 사이에 5-10개의 이산적인<br />
tiem step을 갖는것 처럼 긴 시간에 걸친 모델이 될 능력을 제한한다.</p>

<p><br />
이러한 문제들을 다루기 위해 Long-Short-Term Memory (<strong>LSTM</strong>)이 고안 되었다.<br />
original LSTM의 구조는 recurrent hidden layer에 <strong>memory block</strong>이라 불리는 특별한 유닛이 포함되어있다.</p>

<p>Memory block들은 self-connection을 통해 <strong>memory cell</strong>을 포함하는데<br />
이 memory cell은 네트워크의 일시적인 상태에 대한 정보를 저장하고 있다.<br />
그 외에도 <strong>gate</strong>라 불리는 특정한 목표를 위해 늘어난 unit들로 구성된 구조를 추가해 정보의 흐름을 제어한다.</p>

<p>각 memory block은 memory cell에 흘러 들어오는 input activations를 제어하는 <strong>input gate</strong>와<br />
네트워크의 남은 부분에 흐르는 cell activation 출력을 제어하는 <strong>output gate</strong>를 포함한다.</p>

<p>후에, subsequence로 segment되지 않은 연속된 입력 스트림을 처리하지 못하는<br />
LSTM 모델의 약점을 해결하기 위해 memory block에 <strong>forget gate</strong>가 추가되었다.</p>

<p><strong>forget gate</strong>는 self recurrent connection을 하는 cell에 입력되기 전에 cell의 내부 state를 scaling해서<br />
cell의 memory를 forgetting하거나 resetting하도록 적응하는 역할을 한다.</p>

<p>게다가 최근 LSTM 구조는 cell 내부에서 해당 cell의 gate에 연결이 되는 <strong>peephole connections</strong>를 추가해<br />
결과의 정확한 시간을 학습한다.</p>

<p><br />
LSTM과 전통 RNN은 sequence prediction과 sequence labeling 작업에 잘 사용되어왔다.<br />
LSTM은 문맥 자유 언어와 문맥 의존 언어에서 RNN보다 더 좋은 성능을 보여왔다.</p>

<p><strong>Bidirectional LSTM</strong> 네트워크는 양방향 RNN과 비슷하고 현재 입력에 대한 결과를 계산하기 위해<br />
TIMIT 음성 데이터베이스의 acoustic 프레임에 음성 라벨링을 하기 위해 고안되었다.</p>

<p>온오프라인에서 필기체 인식은 분할되지 않은 sequence data를 학습하도록<br />
forward backward 타입의 알고리즘을 사용하는 connectionist temporal classification(<strong>CTC</strong>)를 포함하는<br />
양방향 LSTM 네트워크들이 HMM기반 최첨단 시스템의 성능을 능가하는 것을 확인했다.</p>

<p>최근에는 acoustic modeling에 대한 DNN의 성공을 뒤이어,<br />
LSTM층을 여러개 쌓은 구조인 deep LSTM RNN을<br />
CTC output layer와 음성 sequence를 예측하는 RNN 변환기를 결합한 것이<br />
TIMIT의 음성 인식 데이터에서 최신 기술의 결과를 보여줬다.</p>

<p>language modeling에서는 전통 RNN이 표준 n-gram 모델들 중에서 아주 작은 perplexity를 얻었다.</p>

<p><br />
DNN이 음성인식과 많은 단어들의 speech 인식에서 가장 좋은 성능을 보이는 반면,<br />
LSTM 네트워크의 적용은 TIMIT 데이터에서 음성인식에만 적용 가능했고<br />
DNN보다 더 좋은 결과를 얻기 위해 추가적인 기술과<br />
CTC와 RNN transducer 같은 추가적인 모델들의 사용이 요구되었다.</p>

<p><br />
이 논문에서, RNN 구조 기반의 LSTM이 많은 단어들의 speech 인식 시스템(수천개의 문맥 의존 상태를 갖는)에서 <br />
최신 기술의 성능을 뛰어넘는것을 확인할 것이다.</p>

<p>큰 네트워크에서 계산적 효율의 문제점을 해결하고 model parameter를 더 잘 사용하도록<br />
기존의 LSTM 구조를 변경하여 새로운 구조를 제안한다.</p>

<p><br />
<br /></p>
<h1 id="2-lstm-architectures">2. LSTM ARCHITECTURES</h1>
<p>기존 LSTM 네트워크의 구조에는 input layer와 recurrent LSTM layer와 output layer가 있다.<br />
input layer는 LSTM layer에 연결되어있고<br />
LSTM layer에서 recurrent 연결들은 cell output unit에서 나와서<br />
cell input, input gate, output gate와 forget gate로 바로 들어간다.<br />
cell output units는 네트워크의 output layer에 연결된다.</p>

<p>표준 LSTM 네트워크에서 각 memory block에 하나의 cell은 편향을 제외하고<br />
총 parameter W의 수가 아래와 같이 계산될 수 있다 :</p>

\[\begin{align*}
W=n_c\times n_c\times 4+n_i\times n_c\times 4+n_c\times n_o+n_c\times3
\end{align*}\]

<p>$n_c$는 memory cell의 수 (여기서는 memory block의 수)<br />
$n_i$는 input unit의 수<br />
$n_o$는 output unit의 수</p>

<p>LSTM 모델을 학습시키는데 각 가중치 당 time step 별로 SGD optimizer기술에 드는 계산 복잡도는 O(1)이다.<br />
따라서 time step당 학습에 드는 계산 복잡도는 O(W)가 된다.<br />
비교적 적은 수의 input을 받는 네트워크의 학습 시간은  $n_c\times(n_c+n_o)$ 항목에 의해 결정된다.<br />
많은 수의 output unit과 일시적 문맥의 정보를 저장하기 위해 많은 수의 memory cell을 필요로하는 작업에서는<br />
LSTM 모델을 학습시키는데 계산비용이 많이 든다.</p>

<p><br />
표준 구조에 대한 대안으로써 LSTM 모델을 학습시키는 데에 드는 계산 복잡도를 해결하기 위해<br />
두 개의 새로운 구조를 제안한다.<br />
두 구조들은 Figure1에서 보여준다.</p>

<p><img src="/assets/img/Paper_Review/LSTM/LSTM_0.png" alt="LSTM_0" /></p>

<p>구조들중 하나는 cell output unit을 cell input unit과 gate에 연결하고<br />
이외에도 예측 결과를 나타내는 network의 output unit에도 연결이 되어있는<br />
recurrent projection layer에 연결한다.</p>

<p>그러므로 이 모델에서 파라미터의 수는</p>

\[\begin{align*}
n_c\times n_r\times 4+n_i\times n_c\times 4+n_r\times n_o+n_c\times n_r+n_c\times 3
\end{align*}\]

<p>이고 $n_r$은 recurrent projection layer의 unit의 수이다.</p>

<p>다른 구조 하나는 recurrent projection layer와 더불어 output layer에 직접 연결되어있는<br />
또 다른 non-recurrent projection layer를 추가한다.<br />
이 구조의 모델은</p>

\[\begin{align*}
n_c\times n_r\times 4+n_i\times n_c\times 4+(n_r+n_p)\times n_o+n_c\times(n_r+n_p)+n_c\times 3
\end{align*}\]

<p>이고 $n_p$는 non-recurrent projection layer의 unit의 수이고<br />
이것은 recurrent connection $(n_c\times n_r\times 4)$ 에서 파라미터의 증가 없이<br />
projection layer의 unit의 수를 늘릴 수 있게 해준다.<br />
여기서 output unit과 과련해서 projection layer를 두개 사용하는 것은<br />
$n_r+n_p$개의 unit을 하나의 projection layer를 사용하는 것과 같은 효율을 보인다.</p>

<p><br />
하나의 LSTM 네트워크는 아래 식을 통해 네트워크 unit들을 계산하고 주된 목적은<br />
input sequence $x=(x_1,\dots,x_T)$를 output sequence $y=(y_1,\dots,y_T)$에 mapping시키는 것이다.</p>

\[\begin{align}
i_t=\sigma(W_{ix}x_t+W_{im}m_{t-1}+W_{ic}c_{t-1}+b_i)\\
f_t=\sigma(W_{fx}x_t+W_{mf}m_{t-1}+W_{cf}c_{t-1}+b_f)\\
c_t=f_t\odot c_{t-1}+i_t\odot g(W_{cx}x_t+W_{cm}m_{t-1}+b_c)\\
o_t=\sigma(W_{ox}x_t+W_{om}m_{t-1}+W_{oc}c_t+b_o)\\
m_t=o_t\odot h(c_t)\\
h_t=W_{ym}m_t+b_y
\end{align}\]

<p>W는 가중치 메트릭스를 나타내고
b는 편향 벡터를 나타내고<br />
sigma는 logistic sigmoid 함수를 나타내고<br />
i,f,o와 c는 각각 input, forget, output gate와 cell vector를 나타내고<br />
vector m과 같은 크기를 갖는다.<br />
$\odot$은 element-wise product를 나타내고<br />
g와 h는 cell input과 cell output activation 함수이고 일반적으로 tanh를 사용한다.</p>

<p><br />
두번째 구조인 recurrent와 non-recurrent projection layer를 모두 갖는 것의 식은 아래와 같다 :</p>

\[\begin{align}
i_t = \sigma(W_{ix}x_t + W_{ir}r_{t−1} + W_{ic}c_{t−1} + b_i)\\
f_t = \sigma(W_{fx}x_t +W_{rf}r_{t−1} +W_{cf}c_{t−1} +b_f)\\
c_t =f_t\odot c_{t−1} +i_t\odot g(W_{cx}x_t +W_{cr}r_{t−1} +b_c)\\
o_t = \sigma(W_{ox}x_t + W_{or}r_{t−1} + W_{oc}c_t + b_o)\\
m_t = o_t\odot h(c_t)\\
r_t = W_{rm}m_t\\
p_t = W_{pm}m_t\\
y_t =W_{yr}r_t +W_{yp}p_t +b_y
\end{align}\]

<p>r과 p는 recurrent와 optional non-recurrent unit activation을 뜻한다.</p>

<p><br /></p>
<h2 id="21-implementation">2.1. Implementation</h2>
<p>GPU를 사용하지 않고 하나의 기계에서 multicore CPU를 사용해서 새로 고안한<br />
LSTM 구조를 구현했다.<br />
이러한 결정은 CPU의 비교적 쉬운 구현 복잡도와 디버깅이 쉬운점을 기반으로 내려졌다.<br />
만약 큰 네트워크의 학습 시간중에 하나의 기계에서 병목을 일으킨다면<br />
CPU에서의 구현은 큰 군집의 기게에서 쉽게 구현되어 작동할 것이다.<br />
매트릭스 연산은 Eigen matrix library를 사용했다.<br />
이것은 C++라이브러리를 통해 CPU에서 벡터 명령어(SIMD - single instruction multiple data)를 통해<br />
효율적인 매트릭스 연산을 하도록 한다.<br />
활성함수와 gradient 계산의 구현은 SIMD 명령어를 통해 병렬적 구조의 이점을 얻어도록 구현했다.</p>

<p><br />
Asynchronous stochastic gradient descent(<strong>ASGD</strong>)를 최적화 기술로 사용했다.<br />
gradient에 의한 파라미터 업데이트는 비동기적으로 처리되고 multi-core 기계 상에서<br />
multiple threads를 통해 계산된다.</p>

<p>각 thread는 효율성을 위해 sequence의 batch상에서 병렬적으로 연산한다.<br />
예를 들어 vector-matrix 연산 보다 matrix-matrix 연산을 하고<br />
더 나아가 모델 파라미터가 여러 input sequence에 의해 동시에 업데이트 되기 때문에 stochasticity한 점도 있을 것이다.</p>

<p>이 외에도 하나의 thread에서 sequence를 batch로 처리하게되면,<br />
multiple thread를 동반한 학습은 더 큰 batch를 sequence에 적용할 때<br />
병렬적으로 처리되기 때문에 더 효율적이다.</p>

<p><br />
모델 파라미터를 업데이트하기 위해 truncated backpropagation through time(<strong>BPTT</strong>)를<br />
학습 알고리즘으로 사용한다.</p>

<p>고정된 time step $T_{bptt}$ 예를 들어 20의 값으로 사용해서<br />
activation을 forward-propagate 시키고 gradient를 backward-propagate 한다.<br />
학습 과정에서 input sequence를 $T_{bptt}$의 크기인 subsequence로 나눠서 vector로 만든다.<br />
말에 대한 subsequence들은 원래 자신의 순서로 처리된다.</p>

<p>우선 첫 time step부터 이전 time step까지 activation과 네트워크 입력을 가지고<br />
반복적으로 activation들을 계산하고 forward-propagate한다.<br />
그리고 네트워크의 error는 네트워크의 cost function을 이용하여 각 time step별로 계산한다.</p>

<p>그다음에 cross-entropy 기준으로 gradient를 back-propagate하는데<br />
각 time step별로 error값과 다음번의 time step을 이용해 gradient값을 구한다.<br />
이 때 time step은 $T_{bptt}$부터 시작한다.</p>

<p>마지막으로 네트워크의 가중치들의 graident값들은 $T_{bptt}$ time step동안 축적되고 그 가중치들이 업데이트된다.</p>

<p>각 subsequence를 처리하고 난 후 Memory cell의 상태는 다음 subsequence를 위해 저장한다.<br />
다른 입력 sequence들로부터 여러개의 subsequence를 처리하는 경우에는<br />
모든 sequence들의 마지막까지 도달해야 하기 때문에<br />
몇개의 subsequence들은 $T_{bptt}$보다 짧은 time step을 갖을 수 있다.<br />
subsequence는 다음 batch에서 새로운 입력 sequence의 subsequence로 대체되고<br />
cell state를 reset한다.</p>

<p><br /></p>
<h1 id="3-experiments">3. EXPERIMENTS</h1>
<p>Google English Voice Search라는 많은 단어가 쓰이는 speech recognition 작업에서<br />
DNN, RNN과 LSTM의 성능을 평가하고 비교한다.</p>

<h2 id="31-systems--evaluation">3.1. Systems &amp; Evaluation</h2>
<p>모든 네트워크들은 익명이고 손으로 번역된 Google voice search와 dictation traffic으로<br />
구성된 데이터셋에서 1900시간동안 300만개의 말을 학습했다.<br />
데이터셋은 10ms마다 계산되고 40차원을 가지는 25ms로 표현된다.<br />
말들은 14247개의 CD(문맥 의존) state를 갖는 9000만개의 가중치로 구성된 FFNN에 배치된다.<br />
출력 상태를 세개의 다른 목록으로 적용하며 네트워크들을 학습시켰다 : 126, 2000 and 8000<br />
이것들은 14247개의 state들을 동일한 클래스를 통해 각자의 작아진 목록으로 mapping시켜서 얻는다.</p>

<p>126 state set은 Context independent(CI,문맥 독립) state (3 x 42)이다.<br />
학습하기 전에 모든 네트워크의 가중치들은 임의로 초기화시켰다.<br />
learning rate는 각 네트워크와 구성에 맞게 안정한 수렴 결과를 얻기 위해 설정했다.<br />
learning rate는 학습하는 동안 기하급수적으로 줄었다.</p>

<p><br />
학습하는 동안 200000 프레임 set에서  frame accuracies를 평가했다.<br />
다시말해 phone state labeling accuracy acoustic frame 같은 것이다.</p>

<p>학습된 모델드른 23000 손으로 번역된 말들로 이루어진 음성인식 시스템에 대한 test set으로 평가했고<br />
word error rates(WERs)도 계산했다.<br />
언어 모델에서 decoding에 사용된 단어들의 크기는 260만개이다.</p>

<p><br />
DNNs들은 200개의 프레임으로 배치를 적용해 GPU환경에서 SGD를 통해 학습했다.<br />
각 네트워크는 logistic sigmoid 은닉층과 phone HMM state를 나타내는<br />
softmax output layer와 전결합을 이루고있다.<br />
LSTM 구조에서도 일관성을 위해 어떤 네트워크들은 low-rank projection layer를 갖는다.<br />
DNN의 입력은 5 frame을 오른쪽에 10이나 15 frame을 왼쪽에 쌓아(10w5, 15w5)로<br />
비대칭적인 window의 구성으로 이루어진다.</p>

<p><br />
LSTM과 전통적인 RNN의 다양한 구성들은 각 thread가 각 utterance에서 step별로<br />
4개나 8개의 subsequence의 gradient를 계산하도록하여<br />
24개의 thread로 ASGD 기법을 사용해 학습되었다.</p>

<p>20번째 time step ($T_{bptt}$)이 activations의 forward-propagate와 truncated BPTT 학습 알고리즘을<br />
gradient의 backward-propagate를 위해 사용되었다.</p>

<p>RNNs의 은닉층의 unit들은 logistic sigmoid activation function을 사용했다.<br />
Recurrent projection layer구조를 갖는 RNN은 projection layer에 선형 activation unit을 사용했다.<br />
LSTM은 <strong>tanh</strong>를 cell inpu unit, cell output unit과<br />
input, output forget gate의 logistic sigmoid 활성함수로 사용했다.<br />
LSTM에서 recurrent projection layer와 optional non-recurrent projection layer에서<br />
선형 activation unit을 사용했다.<br />
LSTM과 RNN에 들어가는 입력은 40차원의 25ms 프레임이다.<br />
앞으로 들어올 frame에 대한 정보는 현제 frame에 대해 더 좋은 결정을 하도록 돕기 때문에<br />
output state label을 5 frame 연기시킨다.</p>

<h2 id="32-results">3.2. Results</h2>
<p><img src="/assets/img/Paper_Review/LSTM/LSTM_1.png" alt="LSTM_1" />
<img src="/assets/img/Paper_Review/LSTM/LSTM_2.png" alt="LSTM_2" />
<img src="/assets/img/Paper_Review/LSTM/LSTM_3.png" alt="LSTM_3" /></p>

<p>Figure 2,3 과 4에서 각각 126, 2000과 8000 state output에 대해 frame accuracy를 보여준다.<br />
Figure에서 네트워크 구성의 이름은 네트워크 크기와 구조에 대한 정보를 갖고있다.<br />
cN에서 N은 LSTM에서는 memory cell의 수를 뜻하고 RNN에서는 은닉층의 unit 수를 뜻한다.<br />
rN은 LSTM과 RNN 모두 recurrent projection unit의 수를 뜻한다.<br />
pN은 LSTM에서 non-recurrent projection unit을 뜻한다.<br />
DNN 구성의 이름은 10w5처럼 왼쪽과 오른쪽의 context size를 뜻하고<br />
은닉층의 수와 각 은닉층의 unit 수<br />
그리고 optional low-rank projection layer의 크기를 뜻한다.</p>

<p>괄호 안에는 각 모델의 가중치의 수에 대한 정보가 담겨있다.<br />
RNN이 126개의 state output에서만 실험된 이유는<br />
126개의 state output에 대한 결과에서 이미 DNN과 LSTM보다 훨씬 뒤쳐지는 성능을 보였기 때문이다.</p>

<p>Figure 2에서 보이듯 RNN들은 매우 학습 초기에 매우 불안정한 모습이고<br />
수렴하도록 하기 위해 exploding problem 때문에 activation과 gradient를 제한했다.</p>

<p>LSTM은 빠르게 수렴하면서 RNN과 DNN보다 더 높은 frame accuracy를 보였다.<br />
Projected RNN구조를 통해 고안된 LSTM은 같은 수의 파라미터를 적용했을 때<br />
기존 RNN기반 LSTM보다 더 높은 accuracy를 보였다.<br />
(Figure3에서 LSTM_512 vs LSSTM_1024_256)</p>

<p>Recurrent와 non-recurrent projection layer를 포함하는 LSTM 네트워크는<br />
recurrent projection layer만 포함하는 LSTM보다 일반적으로 더 좋은 성능을 보였다.<br />
단, 2000개의 state를 적용한 경우는 learning rate를 너무 작게 설정했기 때문에 다른 결과가 나왔다.</p>

<p>Figure 5, 6과 7에서는 126, 2000과 8000 state output에 대해 같은 모델별로 WERs를 계산했다.<br />
아직 몇몇 LSTM 네트워크들이 수렴하지 않은 상태이지만, 논문의 마지막 개정에서는 완성 시키도록 할 것이다.<br />
음성 인식 실험에서는 문맥 의존적인(CI) 126 output state모델,<br />
CD 2000 output state embedded size model(mobile phone processor에서 작동하기 위해 제한됨)<br />
그리고 비교적 큰 8000 output state에서 LSTM 네트워크가 정확도가 개선되었다.</p>

<p>Figure 6에서 보듯, LSTM_c1024_r256과 LSTM_c512를 비교해보면<br />
새로 고안된 구조가 RNN보다 더 좋은 인식 정확도를 얻는데 필수적이라는 것을 알 수 있다.</p>

<p>또한 DNN의 깊이가 중요하다는 것을 보이기 위한 실험도 했다.<br />
(Figure 6에서 DNN_10w5_2_864_lr256과 DNN_10w5_5_512_lr256을 비교해라)</p>

<p><img src="/assets/img/Paper_Review/LSTM/LSTM_4.png" alt="LSTM_4" />
<img src="/assets/img/Paper_Review/LSTM/LSTM_5.png" alt="LSTM_5" />
<img src="/assets/img/Paper_Review/LSTM/LSTM_6.png" alt="LSTM_6" /></p>

<p><br /></p>
<h1 id="4-conclusion">4. CONCLUSION</h1>
<p>이미 알듯이 이번 논문에서 LSTM 네트워크를 large vocabulary speech recognition 작업에 적용해 보았다.</p>

<p>큰 수의 output unit을 사용하는 큰 네트워크에 LSTM을 적용할 수 있는 확장성을 위하여,<br />
기존 LSTM 구조보다 모델 파라미터를 더 효율적으로 사용할 수 있도록 새로운 두가지 구조를 만들어 소개했다.<br />
그중 하나는 LSTM layer와 output layer 사이에 recurrent projection layer이 들어간 구조이고<br />
다른 하나는 추가적인 recurrent 연결 없이 projection layer를 추가하기 위해 non-recurrent projection layer를 추가한 구조로<br />
이렇게 분리하면 유연성을 얻을 수 있다.</p>

<p>새로 고안된 구조들이 기존 LSTM에 비해 더 좋은 성능을 보인다고 확인했다.<br />
또한, large vocabulary speech recognition 작업에서 더 많은 output state를 사용할 때<br />
새로 고안한 LSTM 구조가 DNN보다 더 좋은 성능을 보이는 것을 확인했다.</p>

<p>LSTM을 single multi-core machine에서 학습 시키는 것은 더 큰 네트워크로 확장할 수 없다.<br />
후에 GPU와 분리된 CPU에서의 구현을 연구해 볼 것이다.</p>]]></content><author><name>Hasim Sak, Andrew Senior, Francoise Beaufays</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[Abstract Long Short-Term Memory (LSTM)은 Vanilla RNNs의 vanishing/exploding gradient 문제를 다루기 위해 고안된 RNN구조다.]]></summary></entry><entry><title type="html">Recurrent Neural Networks (RNNs): A gentle Introduction and Overview</title><link href="http://192.168.0.41:4000/paper%20review/2022/08/28/Recurrent-Neural-Networks-A-gentle-Introduction-and-Overview.html" rel="alternate" type="text/html" title="Recurrent Neural Networks (RNNs): A gentle Introduction and Overview" /><published>2022-08-28T16:44:13+09:00</published><updated>2022-08-28T16:44:13+09:00</updated><id>http://192.168.0.41:4000/paper%20review/2022/08/28/Recurrent-Neural-Networks:A-gentle-Introduction-and-Overview</id><content type="html" xml:base="http://192.168.0.41:4000/paper%20review/2022/08/28/Recurrent-Neural-Networks-A-gentle-Introduction-and-Overview.html"><![CDATA[<h1 id="abstract">Abstract</h1>
<p>“Language Modeling &amp; Generating Text”, “Speech Recognition”, “Generating Image Descriptions” or<br />
“Video Tagging” 분야에서 해결책을 위한 최신기술들은 RNN을 기반으로 하고있다.<br />
따라서 현재 또는 앞으로 제시될 해결책들에 대한 구조를 이해하고 따라잡으려면<br />
RNN에 대한 기본 개념을 이해하는 것이 매우 중요할 것이다.<br />
이 논문에서는 BPTT, LSTM 뿐만아니라 Attention Mechanism과 Pointer Networks에 대한 개념을<br />
독자가 쉽게 이해할 수 있도록 가장 중요한 RNN들을 살펴볼 것이다.<br />
그리고 이와 관련해 더 복잡한 주제를 읽어보는 것을 추천한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="1-introduction--notation">1 Introduction &amp; Notation</h1>
<p>RNNs는 sequence data에서 패턴을 찾기위해 주로 사용되는 신경막 구조이다.<br />
Sequence data로는 handwriting, genomes, text 또는 주식시장과 같은 산업에서 만들어지는 numerical time series가 될 수 있다.<br />
그러나, RNNs는 이미지가 패치들로 분해되고 sequence에 따라 적용이 되는 경우에도 적용 된다.<br />
더 높은 수준에서는 RNNs는 Language Modeling &amp; Generating Text, Speech Recognition,<br />
Generating Image Descriptions or Video Tagging 에도 적용된다.<br />
RNN은 MLP라 알려진 Feedforward Neural Networks와 정보가 네트워크를 통과하는 방법에 따라 구분된다.<br />
전방향 네트워크들은 cycle없이 네트워크를 통과시키는 반면,<br />
RNN은 cycle이 있고 정보를 자신에게 다시 전송한다.<br />
이런 방식으로 RNN은 전방향 네트워크의 기능을 확장시켜<br />
현재 입력값 $X_t$ 뿐만 아니라 이전 입력값들 $X_{0:t-1}$을 고려하게 한다.<br />
높은 수준에서 이 차이점을 시각화 한것이 Figure 1이다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_0.png" alt="RNNOverview_0" /></p>

<p>여기서 여러개의 hidden layer를 갖는 옵션은 하나의 Hidden Layer block H를 갖는 것으로 집약된다.<br />
이 block H는 여러개의 hidden layer로 확장될 수 있다.</p>

<p>\(\begin{align*}\end{align*}\)
이전 iteration에서 hidden layer로 정보를 넘기는 과정은 [24]에서 볼 수 있다.<br />
앞으로 time step $t$에 대해 hidden state와 input을</p>

\[H_{t}\in\mathbb{R}^{n\times h},\ X_t\in\mathbb{R}^{n\times d}\]

<p>로 표현하고 n은 sample의 개수<br />
d는 각 샘플 별 입력 수<br />
h는 hidden units의 수를 뜻한다.<br />
게다가 가중치 메트릭스로</p>

\[W_{xh}\in\mathbb{R}^{d\times h}\]

<p>hidden state to hidden state 메트릭스로</p>

\[W_{hh}\in\mathbb{R}^{h\times h}\]

<p>편향으로</p>

\[b_h\in\mathbb{R}^{1\times h}\]

<p>마지막으로, activation function을</p>

\[\phi\]

<p>로 표현하고 역전파를 사용해 gradient를 구하기 위해 sigmoid나 tanh를 사용한다.<br />
이 표현들을 모두 사용해 만든 식은 hidden variable을 나타내는 Equation 1과<br />
output variable을 나타내는 Equation 2가 있다.</p>

\[\begin{align}
H_t&amp;=\phi_h\left(X_tW_{xh}+H_{t-1}W_{hh}+b_h\right) \label{eq1} \\
O_t&amp;=\phi_o\left(H_tW_{ho}+b_o\right) \label{eq2}
\end{align}\]

<p>$H_t$가 재귀적으로 $H_{t-1}$을 포함하는 이 과정은 RNN의 모든 time step에서 발생하고<br />
모든 hidden state들을 trace 한다. ($H_{t-1}$과 그 이전 $H_{t-1}$까지 모두)</p>

<p>\(\begin{align*}\end{align*}\)
만약 RNN을 표기했던 방법으로 전방향 신경망을 표기하면<br />
이전 식들과 차이점을 분명하게 알 수 있다.
식3에서 hidden variable을 식4에서는 output variable을 보여준다.</p>

\[\begin{align}
H=\phi_h\left(XW_{xh}+b_h\right) \label{eq3} \\
O=\phi_o\left(HW_{ho}+b_o\right) \label{eq4}
\end{align}\]

<p>\(\begin{align*}\end{align*}\)
만약 당신이 Feedforward Neural Networks를 학습시키는 기술인 역전파를 잘 알고 있다면<br />
RNN에서 오차를 어떻게 역전파 시킬지에 대한 의문이 생길 것이다.<br />
여기, 이 기술을 Backpropagation Through Time(BPTT)라고 부른다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="2-backpropagation-through-timebptt--truncated-bptt">2 Backpropagation Through Time(BPTT) &amp;<br /> Truncated BPTT</h1>
<p>BPTT는 RNN에 적용된 역전파 알고리즘이다.<br />
이론상 BPTT는 우리가 역전파를 적용 가능하도록 RNN을 펼쳐 전통적인 Feedforward Neural Network와 같은 구조로 만든다.<br />
그러기 위해서, 우리는 이전에 말했던 표기법을 사용한다.</p>

<p>\(\begin{align*}\end{align*}\)
입력값 $X_t$를 네트워크에서 forward pass하는 경우<br />
hidden state $H_t$와 output state $O_t$를 한 step에 모두 계산한다.<br />
그리고 나서 우리는 output $O_t$와 $Y_t$의 차이를 Loss function $\mathcal{L}\left(O,Y\right)$로 아래 식5와 같이 정의할 수 있다.<br />
기본적으로 지금까지 모든 loss term $\ell_t$을 더하여 계산한다.<br />
이 loss term $\ell_t$은 특정 문제에 따라 다르게 정의될 수 있다.<br />
(e.g. Mean Squared Error, Hinge Loss, Cross Entropy Loss, etc.)</p>

\[\begin{align}
\mathcal{L}\left(O,Y\right)=\sum\limits^T_{t=1}\ell_t\left(O_t,Y_t\right) \label{eq5}
\end{align}\]

<p>우리는 세개의 가중치 메트릭스 $W_{xh}, W_{hh} and W_{ho}$를 사용하기 때문에<br />
각 가중치 메트릭스별로 partial derivative를 계산해야 한다.
평범한 역전파에도 사용되는 연쇄법칙(Chain rule)에 의해 식6로 $W_{ho}$를 구할 수 있다.</p>

\[\begin{align}
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{ho}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot\dfrac{\partial\ \phi_o}{\partial\ W_{ho}}=\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot\ H_t
\label{eq6}
\end{align}\]

<p>$W_{hh}$에 대한 partial derivative는 아래 식7을 통해 계산한다.</p>

\[\begin{align}
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{hh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot\dfrac{\partial\ \phi_o}{\partial\ H_t}\cdot\dfrac{\partial H_t}{\partial\ \phi_h}\cdot\dfrac{\partial\ \phi_h}{\partial\ W_{hh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot W_{ho}\cdot\dfrac{\partial H_t}{\partial\ \phi_h}\cdot\dfrac{\partial\ \phi_h}{\partial\ W_{hh}}
\end{align}\]

<p>$W_{xh}$에 대한 partial derivative는 아래 식8을 통해 계산한다.</p>

\[\begin{align}
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{xh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot\dfrac{\partial\ \phi_o}{\partial\ H_t}\cdot\dfrac{\partial H_t}{\partial\ \phi_h}\cdot\dfrac{\partial\ \phi_h}{\partial\ W_{xh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot W_{ho}\cdot\dfrac{\partial H_t}{\partial\ \phi_h}\cdot\dfrac{\partial\ \phi_h}{\partial\ W_{xh}}
\end{align}\]

<p>각 $H_t$가 이전 time step에 의존하기 때문에<br />
식8의 마지막 부분을 아래 식9와 식10으로 대체할 수 있다.</p>

\[\begin{align}
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{hh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot W_{ho}\sum\limits^t_{k=1}\dfrac{\partial\ H_t}{\partial\ H_k}\cdot\dfrac{\partial\ H_k}{\partial\ W_{hh}} \\
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{xh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot W_{ho}\sum\limits^t_{k=1}\dfrac{\partial\ H_t}{\partial\ H_k}\cdot\dfrac{\partial\ H_k}{\partial\ W_{xh}}
\end{align}\]

<p>개조된 부분은 아래와 같이 식11과 식12로 쓸 수 있다.</p>

\[\begin{align}
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{hh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot W_{ho}\sum\limits^t_{k=1}\left(W_{hh}^T\right)^{t-k}\cdot H_k\\
\dfrac{\partial\ \mathcal{L}}{\partial\ W_{xh}}=
\sum\limits^T_{t=1}\dfrac{\partial\ \ell_t}{\partial\ O_t}\cdot\dfrac{\partial\ O_t}{\partial\ \phi_o}\cdot W_{ho}\sum\limits^t_{k=1}\left(W_{xh}^T\right)^{t-k}\cdot X_k\\
\end{align}\]

<p>여기서 각 time step의 loss term인 $\ell_t$를 통해 매우 커질 수 있는 loss function $\mathcal{L}$을 구하기 위해<br />
$W^k_{hh}$를 저장해야한다.<br />
매우 큰 이 수를 위해 사용하는 이 방법은 매우 불안정하다.<br />
왜냐하면 만약 고유값이 1보다 작으면 gradient는 vanish 될거고<br />
만약 고유값이 1보다 크다면 gradient는 diverge할 것이기 때문이다.<br />
이 문제를 풀 수 있는 방법중 하나는 계산 가능한 수준에서 sum을 자르는 것이다.<br />
이걸 Truncated BPTT라고 하는데 이것은 기본적으로<br />
역전파로 돌아갈 수 있는 만큼 gradient의 time step을 제한하여 구현한다.<br />
여기서 Upper bound를 RNN의 window가 고려할 과거의 time step 수를 의미한다고 생각할 수 있을 것이다.<br />
BPTT는 기본적으로 RNN을 펼쳐 각 time step별로 새로운 layer를 만들기 때문에,<br />
이 과정을 hidden layers를 제한하는 것이라고 여길 수도 있을 것이다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="3-problems-of-rnns-vanishing--exploding-gradients">3 Problems of RNNs:<br /> Vanishing &amp; Exploding Gradients</h1>
<p>대부분의 신경망들처럼, vanishing 또는 exploding gradient들은 RNN의 주요한 문제점이다.
식9와 식10에서 본 잠재적으로 매우 긴 sequence에 걸친 matrix multiplication인 경우,<br />
gradient값이 1보다 작다면 점점 gradient가 작아져 결국 vanish될 것이고<br />
이것은 현재 time step으로부터 먼 초기 time step의 state가 주는 영향을 무시하게 된다.<br />
마찬가지로 gradient값들이 1보다 크다면 matrix multiplication을 할 때 exploding gradient 현상이 관찰될 것이다.</p>

<p>\(\begin{align*}\end{align*}\)
이 vanishing gradient 문제를 해결하기 위해 고안된 내용이 Long Short Term Memory units(LSTMs)가 된다.<br />
이 접근으로 Vanilla RNN을 뛰어넘는 성능이 다양한 작업에서 가능해졌다.<br />
다음 섹션에서는 LSTMs에 대해 더 깊게 알아보겠다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="4-long-short-term-memory-units-lstms">4 Long Short-Term Memory Units (LSTMs)</h1>
<p>LSTMs는 vanishing gradient 문제를 해결하기 위해 고안되었다.<br />
LSTMSs는 더 지속적인 error를 사용하기 때문에,<br />
RNNs이 긴 time step(1000번이 넘게)동안 학습을 가능하게 한다.
이것을 위해, LSTMs는 구조상 gated cell이라는 것을 사용하여<br />
전통적인 신경망 흐름 바깥에서 정보를 더 저장하도록 한다.
LSTM에서 이것이 작동하기 위해</p>
<ul>
  <li>output gate $O_t$ : cell의 입력을 읽음</li>
  <li>input gate $I_t$ : cell로 입력된 데이터를 읽음</li>
  <li>forget gate $F_t$ : cell 내용을 reset 함</li>
</ul>

<p>이 gate들의 계산을 아래 식13,14,15에 정리했다.</p>

\[\begin{align}
O_t=\sigma\left(X_tW_{xo}+H_{t-1}W_{ho}+b_o\right)\\ \notag \\
I_t=\sigma\left(X_tW_{xi}+H_{t-1}W_{hi}+b_i\right)\\ \notag \\
F_t=\sigma\left(X_tW_{xf}+H_{t-1}W_{hf}+b_f\right)
\end{align}\]

\[\begin{align*}\end{align*}\]

<p>위 식에서
\(\quad 
\begin{align*}
W_{xi},W_{xf},W_{xo}&amp;\in\mathbb{R}^{d\times h}\\
W_{hi},W_{hf},W_{ho}&amp;\in\mathbb{R}^{h\times h}\\
b_i,b_f,b_o&amp;\in\mathbb{R}^{1\times h}
\end{align*}
\quad\)
가 가중치와 편향으로 사용되었다.</p>

<p>\(\begin{align*}\end{align*}\)
게다가 sigmoid 함수를 activation 함수 $\sigma$로 사용하여 출력을 0~1로 만들어 결과적으로 0~1의 값을 갖는 벡터로 변환한다.</p>

<p>\(\begin{align*}\end{align*}\)
다음으로, 이전 gate와 비슷한 연산과정을 갖지만 활성함수로 tanh를 사용하여 결과를 -1~1로 만드는<br />
candidate memory cell $\tilde{C_t}\in\mathbb{R}^{n\times h}$이 필요하다.<br />
그리고 이 cell도 자신의 가중치와 편향 $W_{xc}\in\mathbb{R}^{d\times h},\ W_{hc}\in\mathbb{R}^{h\times h},\ b_c\in\mathbb{R}^{1\times h}$을 갖는다.<br />
아래 식16에서 증명하고 Appendix A에서 시각화 했다.</p>

\[\begin{align}
\tilde{C_t}=\tanh\left(X_tW_{xc}+H_{t-1}W_{hc}+b_c\right)
\end{align}\]

<p>\(\begin{align*}\end{align*}\)<br />
앞서 말한 gate들을 조합하기 위해 지난 메모리 내용인 $C_{t-1}\in\mathbb{R}^{n\times h}$를 사용한다.<br />
이전 메모리 내용 $C_{t-1}$은 우리가 새로운 메모리 내용 $C_t$에 얼마나 옛날 메모리 내용까지 보존시킬 것인지를 조절한다.<br />
이것은 식17에 정리하고 $\odot$은 element-wise multiplication을 뜻한다.</p>

\[\begin{align}
C_t=F_t\odot C_{t-1}+I_t\odot\tilde{C_t}
\end{align}\]

<p>마지막 단계는 hidden state $H_t\in\mathbb{R}^{n\times h}$를 프레임워크에 추가하는 것이고 아래 식18에 정리했다.</p>

\[\begin{align}
H_t=O_t\odot\tanh\left(C_t\right)
\end{align}\]

<p>tanh 함수를 통해 $H_t$의 각 원소들은 -1~1로 정의 될것이고<br />
전체 LSTM 구조는 아래와 같다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_1.png" alt="RNNOverview_1" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="5-deep-recurrent-neural-networks-drnns">5 Deep Recurrent Neural Networks (DRNNs)</h1>
<p>DRRNs는 아주 쉬운 개념이다.<br />
L개의 hidden layers를 갖는 DRNN을 만들려면<br />
아무 타입의 RNNs를 평범하게 쌓아 올리면 된다.<br />
각 hidden state $H_t^{\left(\ell\right)}\in\mathbb{R}^{n\times h}$는 현재 층의 다음 time step인 $H^{\left(\ell\right)}_{t+1}$으로 전달되고<br />
똑같이 현재 time step의 다음 층인 $H^{\left(\ell+1\right)}_t$으로 전달된다.<br />
첫 번째 층을 위해 이전 모델에서 보여준 hidden state 계산을 아래 식19에서 보여준다.<br />
그 다음 층의 경우는 이전 층의 hidden state를 input으로 인식하여 식20을 사용한다.</p>

\[\begin{align}
H_t^{\left(1\right)}&amp;=\phi_{1}\left(X_t,H^{\left(1\right)}_{t-1}\right)\\ \notag \\
H^{\left(\ell\right)}_t&amp;=\phi_\ell\left(H^{\left(\ell-1\right)}_{t},H^{\left(\ell\right)}_{t-1}\right)
\end{align}\]

<p>output $O_t\in\mathbb{R}^{n\times o}$에서 o는 output의 수인데<br />
output은 현재 times step의 마지막 층에 대한 hidden state값만 이용해 계산을 하고 식21에서 보여준다.</p>

\[\begin{align}
O_t=\phi_o\left(H^{\left(L\right)}_tW_{ho}+b_o\right)
\end{align}\]

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="6-bidirectional-recurrent-neural-networks-brnns">6 Bidirectional Recurrent Neural Networks (BRNNs)</h1>
<p>일단 language modeling에 대한 예시를 보자.<br />
현재 모델에서는 지금까지 봐온 내용에 근거하여 믿음이 가는 예측을 통해 다음 sequence element(i.e. the next word) 를 알아낸다.<br />
그러나, 문장의 사이 공간을 채우거나 공간 뒤에 문장의 어느 부분을 채우는 경우에 중요한 정보를 전달하게되는데<br />
이 정보는 이런 작업들이 잘 수행되도록 필수적인 역할을 한다.<br />
더 일반적인 수준에서 우리는 sequence의 특성을 미리 보고 지금과 통합하고 싶다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_2.png" alt="RNNOverview_2" /></p>

\[\begin{align*}\end{align*}\]

<p>이 미리보는 특성을 달성하기 위해 마지막 element부터 반대방향으로 sequence가 적용되는 hidden layer가 추가된 Bidirectional Recurrent Neural Networks (BRNNs)가 등장한다.<br />
구조를 미리보여주기 위해 위에 Figure2를 삽입했다.<br />
이제 forward hidden state $\overset{\rightarrow}{H_t}\in\mathbb{R}^{n\times h}$와 backward hidden state $\overset{\leftarrow}{H_t}\in\mathbb{R}^{n\times h}$ 를 소개하겠다.<br />
이 두 hidden state는 각각 식22와 식23에 정리했다.</p>

\[\begin{align}
\overset{\rightarrow}{H_t}&amp;=\phi\left(X_tW^{\left(f\right)}_{xh}+\overset{\rightarrow}{H_{t-1}}W^{\left(f\right)}_{hh}+b^{\left(f\right)}_h\right)\\ \notag \\
\overset{\leftarrow}{H_t}&amp;=\phi\left(X_tW^{\left(b\right)}_{xh}+\overset{\leftarrow}{H_{t+1}}W^{\left(b\right)}_{hh}+b^{\left(b\right)}_h\right)
\end{align}\]

<p>이전까지는 비슷한 가중치 메트릭스를 정의했지만,<br />
지금부터는 두개로 분리된 매트릭스를 정의할 것이다.<br />
하나는 forward hidden states를 위한 것으로 아래와 같이 정의한다.</p>

\[\begin{align*}
W^{\left(f\right)}_{xh},\ W^{\left(b\right)}_{xh}\in\mathbb{R}^{d\times h}\\
W^{\left(f\right)}_{hh},\ W^{\left(b\right)}_{hh}\in\mathbb{R}^{h\times h}\\
b^{\left(f\right)}_h,\ b^{\left(b\right)}_h\in\mathbb{R}^{1\times h}
\end{align*}\]

<p>이것을 통해 결과 o가 output의 수를 뜻하는 $O_t\in\mathbb{R}^{n\times o}$ 를 계산할 수 있게 된다.
여기서 $\frown$은 두 메트릭스를 axis 0으로 concate하는 것을 의미한다.(위 아래로 쌓음)</p>

\[\begin{align}
O_t=\phi\left(\left[\overset{\rightarrow}{H_t}\frown\overset{\leftarrow}{H_t}\right]W_{ho}+b_o\right)
\end{align}\]

<p>다시말하자면, 가중치 매트릭스 $W_{ho}\in\mathbb{R}^{2h\times o},\ b_o\in\mathbb{R}^{1\times o}$를 정의한다.<br />
두 방향은 서로 다른 수의 hidden units를 갖을 수 있다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="7-encoder-decoder-architecture---sequence-to-sequence-seq2seq">7 Encoder-Decoder Architecture &amp; <br /> Sequence to Sequence (seq2seq)</h1>
<p>Encoder-Decoder architecture는 네트워크가 두 부분으로 이루어진 신경망 구조이다.<br />
Encoder network는 input을 state로 encoding하고 Decoder network는 state를 output으로 decoding한다.<br />
state는 vector나 tensor의 형태를 보인다.<br />
구조는 Figure3에 있다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_3.png" alt="RNNOverview_3" /></p>

<p>이 Encoder-Decoder 구조에 근거하여<br />
Sequence to Sequence (seq2seq)라고 불리는 모델이<br />
sequence input에 대해 sequence output을 만들어 내도록 제안되었다.<br />
이 모델은 RNNs를 encoder와 decoder에 사용하고<br />
encoder의 hidden state가 decoder의 hidden state로 전달된다.<br />
seq2seq의 일반적인 응용프로그램으로는 Google Translate, voice-enabled devices or labeling video data가 있다.<br />
가장 초점을 두는 것은 고정된 길이의 input sequence size n을 고정된 길이의 output sequence size m으로 맞추는 것이다.<br />
여기서 n과 m은 서로 다른 값일 수도 있지만 반드시 다를 필요은 없다.</p>

<p>\(\begin{align*}\end{align*}\)
제안된 구조를 Figure4에서 시각화 해놨다.<br />
여기서 encoder는 RNN으로 구성되고 single element인 sequence data $X_t$를 사용한다.
$t$는 sequence element의 순서를 뜻한다.<br />
사용되는 RNNdms LSTM이나 GRU가 성능을 향상시키기 위해 사용된다.
게다가, hidden state $H_t$는 LSTM이나 GRU같은 RNN에 사용되는 hidden state계산과정과 같은 방법으로 계산된다.<br />
Encoder Vector(context)는 encoder network의 마지막 hidden state로 이전 input element들의 모든 정보를 통합하는 목적을 갖는다.<br />
Encoder Vector는 decoder network의 첫 번째 hidden state 역할을 하고<br />
decoder가 정확한 예측이 가능하게 한다.<br />
Decoder network는 RNN으로 설계되었고 time step $t$에서 output $Y_t$ 를 예측한다.<br />
만들어진 output은 또 다시 sequence이며 $t$에 대한 순차를 가진 $Y_t$가 된다.<br />
각 time step에서 RNN은 이전 unit 으로부터 hidden state를 수용하고<br />
자신으로부터 output과 new hidden state를 만들어낸다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_4.png" alt="RNNOverview_4" /></p>

<p>고정된 길이의 vector인 source sentence의 모든 정보를 포함할 필요가 있기 때문에<br />
Encoder Vector가 특히 long sequences를 입력 받게 되면 위와 같은 구조에서는 병목현상이 발생하게 된다.<br />
이러한 문제를 해결하기 위해 <strong>Attention</strong>을 사용해 해결을 하고있다.<br />
다음 장에서는, 그에 대한 해결책을 살펴보겠다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="8-attention-mechanism--transformer">8 Attention Mechanism &amp; Transformer</h1>
<p>RNNs을 위한 <strong>Attention Mechanism</strong>는 부분적으로 human visual focus와 Peripheral perception으로부터 영감을 받았다.<br />
이것은 사람이 특정지역에 초점을 두고 고해상도로 인식을 하게 하고<br />
주변 객체들은 저해상도로 인식하게 한다.<br />
이 focus points와 adjacent perception을 기반으로,<br />
사람은 자신이 focus point를 변경할 때 무엇을 인지 해야하는지 추측할 수 있게 된다고 한다.<br />
비슷하게, 이 방법을 단어들의 sequene로 관찰된 단어들 중에서 추론이 가능하도록 변형시킬 수 있다.<br />
예를들어, 만약 우리가 eating이라는 단어를 “She is eating a green apple”이라는 sequence에서 인지한다면,<br />
가까운 미래에 음식이라는 객체를 찾을 것이다.</p>

<p>\(\begin{align*}\end{align*}\)
일반적으로, Attention은 두 문장을 받고 그들을 단어가 row나 column의 요소로 이루어진 메트릭스로 변형시킨다.<br />
이 matrix layout을 기반으로 비슷한 맥락을 식별하거나 그들 사이에 연관성을 식별해 matrix를 채운다.<br />
이 내용에 대한 예제를 Figure5에서 볼 수 있다.<br />
Figure5는 높은 연관성을 흰색으로 낮은 연관성을 검은색으로 표현한다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_5.png" alt="RNNOverview_5" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="81-definition">8.1 Definition</h2>
<p>seq2seq 모델이 long sequences를 더 잘 다루도록 하기 위해 attention mechanism이 등장한다.<br />
encoder network의 마지막 hidden state의 결과로 Encoder Vector를 만드는 대신에,<br />
attention은 context vector와 전체 입력 source 사이에  shortcut을 사용한다.<br />
이 과정을 시각화 하면 Figure6가 된다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_6.png" alt="RNNOverview_6" /></p>

<p>여기서 n만큼의 길이를 가진 source sequence X를 갖고<br />
m의 크기를 갖는 target sequence Y를 결과로 얻으려고 한다.<br />
그런 점에서 그 식은 7장에서 앞서 말한 내용과 유사하다.<br />
식25처럼 forward와 backward 방향을 concate한 전반적인 hidden state $H_{t’}$ 를 갖는다.<br />
또한, decoder network의 hidden state는 $S_t$로 표기하고<br />
encoder vector는 $C_t$로 표기하며 이 둘은 식26과 식27에서 정리했다.</p>

\[\begin{align}
H_{t'}=\left[\overset{\rightarrow}{H_{t'}}\frown\overset{\leftarrow}{H_{t'}}\right]\\ \notag \\
S_t=\phi_d\left(S_{t-1},Y_{t-1},C_t\right)
\end{align}\]

<p>context vector $C_t$는 intput sequence의 hidden state들의 가중치 합이다.<br />
여기서 가중치인 alignment score $\alpha_{t,t’}$는<br />
$\sum\limits^T_{t’=1}\alpha_{t,t’}=1$을 만족한다.</p>

\[\begin{align}
C_t&amp;=\sum\limits^T_{t'=1}\alpha_{t,t'}\cdot H_{t'}\\ \notag \\
\alpha_{t,t'}=\text{align}\left(Y_t,X_{t'}\right)&amp;=
\dfrac{\exp(\text{score}\left(S_{t-1},H_{t'}\right))}{\sum\limits^T_{t'=1}\exp(\text{score}\left(S_{t-1},H_{t'}\right))}
\end{align}\]

<p>alignment $\alpha_{t,t’}$는 intput 위치 $t’$와 output 위치 $t$의 alignment score 연결한다.<br />
이 score는 가리키는 쌍이 얼마나 잘 맞는지를 나타낸다.<br />
모든 alignment score의 집합은 각 source hidden state가 각 output에 얼만큼 고려되었는지를 정의한다.<br />
Apeendix B에서 더 쉽고 시각화된 seq2seq에 대한 attention mechanism 설명을 보면 좋을것이다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="82-diffrent-types-of-score-functions">8.2 Diffrent types of score functions</h2>
<p>일반적으로, score function은 다양한 작업에서 사용될 때 여러가지 방법으로 구현된다.<br />
Table 1은 이름, 식과 사용에 따라 정리한것이다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_7.png" alt="RNNOverview_7" /></p>

<p>여기 alignmnet 모델에 두개의 학습 가능한 가중치 메트릭스 $\mathbf{v}_a$와 $W_a$가 있다고 하자.<br />
Scaled-Dot-Product 는 입력이 큰 경우 softmax 함수의 gradient가 매우 작아져서<br />
효율적인 학습에 문제가 생기는 점에서 영감을 얻어서 현재 순서의 단어의 글자수를 곱셈에 이용한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="83-transformer">8.3 Transformer</h2>
<p>Attentions Mechanism을 통합함으로써 <strong>Transformer</strong>가 등장한다.<br />
<strong>Transformer</strong>는 recurrence sequence를 attention으로 병렬화 하지만<br />
동시에 sequence에서 각 아이템의 위치를 encoder-decoder 구조를 통해  encoding한다.<br />
사실, 그러기 위해서는 RNNs를 쓰지 않고 성능 향상을 위해<br />
전적으로 self-attention을 사용한다.<br />
구조상 encoding하는 부분은 몇가지 encoder들로 만들어지고<br />
decoder 부분은 encoder 부분과 같은 수의 decoder들로 만든다.<br />
구조를 일반화 한 사진은 Figure7과 같다.</p>

<p><img src="/assets/img/Paper_Review/RNNOverview/RNNOverview_8.png" alt="RNNOverview_8" /></p>

<p>여기서, 각 encoder의 구성요소는 Self-Attention과 FeedForward Neural Network로 이루어진 2개의 하위 계층을 가진 구조로 이루어져있다.<br />
비슷하게, 두 하위 계층은 각 decoder 구성요소에서도 발견되지만<br />
Encoder-Decoder Attention에서 둘 사이에 하위 계층은 seq2seq에 사용된 Attention과 비슷한 일을 한다.<br />
배치된 Attention 계층들은 평범한 attention 계층들이 아니니라  Multi-Headed Attention으로 attention 계층의 성능을 향상시킨다.<br />
Multi-Headed Attention은 모델이 다른 위치에서 다르게 표현한 내용으로부터 정보를 읽게 한다.<br />
쉽게 말해 병렬 구조에서 다른 블락에서 작동하고 결과를 concate한다는 것이다.<br />
불행하게도, multi-head attention의 설계적 선택과 수학적 공식에 대한 설명을 포함하는 것은 이 논문에서 너무 지나친 내용일 수 있다.<br />
더 많은 내용을 원한다면 밑에 참고문서를 봐라.
Figure7에서 보인 구조는 encoder와 decoder 모두에 skip connections과 각 하위 계층에 normalisation 계층을 배치했다.<br />
한가지 중요한 것은 입력과 결과 모두 embedding되고<br />
positional encoding은 sequence element들의 가까운 정도를 표현하도록 적용되었다는 것이다.</p>

<p>\(\begin{align*}\end{align*}\)
마지막 linear와 softmax 층은 decoder stack을 단어로 결과를 만들어 실수 타입의 vector를 반환한다.<br />
이것은 linear 계층을 통해 vector를 훨씬 더 큰 logits vector로 변환함으로써 완성된다.<br />
이 logits vector는 각 cell이 고유한 단어에 대한 점수를 뜻하는 학습 데이터셋을 통해 학습된 어휘의 크기를 갖는다.<br />
softmax를 적용함으로써 그들의 점수를 확률값으로 반환하고<br />
특정 time step에 대해 가장 확률값이 큰 결과로써 cell(i.e. the word)을 선택할 수 있다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="9-pointer-networksptr-nets">9 Pointer Networks(Ptr-Nets)</h1>
<p>Pointer Networks(Ptr-Nets)는 결과 dictionary에 우선순위의 분산된 범주를 고치지 않음으로써 attention과 seq2seq를 적용해 개선시켰다.<br />
input sequence로부터 output sequence가 산출되는 대신에,<br />
pointer network는 input series의 요소들에 단계적으로 pointer가 적용되도록 한다.<br />
한 논문에서는 Pointer Network를 사용해 computing planar convex hulls,<br />
Delaunay triangulations and the symmetric planar Travelling Salesman Problem(TSP)문제를 해결할 수 있다고 증명했다.<br />
일반적으로 state사이에 추가적인 attention을 적용하고나서<br />
softmax를 통해 모델의 결과가 확률값을 갖도록 normalize한다(식29).</p>

\[\begin{align}
Y_t=\text{softmax}(\text{score}(S_t,H_{t'}))=
\text{softmax}(\mathbf{v}^T_a\tanh W_a[S_t;H_{t'}])
\end{align}\]

<p>Ptr-Net이 encoder state들을 attention weights가 적용된 output과 섞이지 않게 해서<br />
attention mechanism을 단순화시킨다.<br />
이 방법에서는 output이 위치에 대한 정보에만 반응하고 input content에 의해서는 반응하지 않는다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="10-conclusion--outlook">10 Conclusion &amp; Outlook</h1>
<p>이 논문에서는 기본적인 RNNs를 설명한다.<br />
일반적인 RNNs의 프레임워크, BPTT, RNNs의 고질적인 문제점, LSTM, DRNNs, BRNNs와 가장 최근 연구된 Encoder Decoder Architecture와 seq2seq 모델 그리고 Attention과 Transformer와 Pointer Network에 대한 내용을 포함한다.<br />
대부분의 주제들은 개념적으로만 다루고 구현적 특성을 다루지는 않았다.<br />
여기서 다룬 주제들에 대한 더 넓은 이해를 하려면 original 논문을 보도록 해라.<br />
추가적으로 가장 최근 출판된 논문에서는 최근 개념들을 다루니까 더욱 원래 논문을 보기를 권한다.</p>

<p>\(\begin{align*}\end{align*}\)
최근 출판된 논문들중 하나는 제시된 개념들을 많이 사용한다.
“Grandmaster level in StarCraft2 using multi-agent reinforcement learning”이라는 논문이고 Vinyals가 썼다.<br />
여기에서, 그들은 agent를 실시간으로 전략 게임 StarCraft2를 훌륭하게 학습시키는 접근 방법을 제시한다.<br />
만약 제시된 내용들이 다소 이론적이라면 실제 환경에 적합하도록 설정되어 배포된 LSTM, Transformer와 Pointer Networks에 관한 논문을 읽어보기를 권한다.</p>]]></content><author><name>Robin M. Schmidt</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[Abstract “Language Modeling &amp; Generating Text”, “Speech Recognition”, “Generating Image Descriptions” or “Video Tagging” 분야에서 해결책을 위한 최신기술들은 RNN을 기반으로 하고있다. 따라서 현재 또는 앞으로 제시될 해결책들에 대한 구조를 이해하고 따라잡으려면 RNN에 대한 기본 개념을 이해하는 것이 매우 중요할 것이다. 이 논문에서는 BPTT, LSTM 뿐만아니라 Attention Mechanism과 Pointer Networks에 대한 개념을 독자가 쉽게 이해할 수 있도록 가장 중요한 RNN들을 살펴볼 것이다. 그리고 이와 관련해 더 복잡한 주제를 읽어보는 것을 추천한다.]]></summary></entry><entry><title type="html">Ch7 합성곱 신경망(CNN)</title><link href="http://192.168.0.41:4000/deeplearning%20from%20scratch/2022/08/27/Ch7-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D.html" rel="alternate" type="text/html" title="Ch7 합성곱 신경망(CNN)" /><published>2022-08-27T20:05:47+09:00</published><updated>2022-08-27T20:05:47+09:00</updated><id>http://192.168.0.41:4000/deeplearning%20from%20scratch/2022/08/27/Ch7-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D</id><content type="html" xml:base="http://192.168.0.41:4000/deeplearning%20from%20scratch/2022/08/27/Ch7-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D.html"><![CDATA[<h1 id="71-전체-구조">7.1 전체 구조</h1>

<p>CNN = Convolutional Layer + Pooling Layer + Fully Connected Layer</p>

<p>(ex&gt; CONV_1 ⇒ RELU ⇒ CONV_2 ⇒ RELU ⇒ POOL_3 ⇒ DROP_4 ⇒ FLATTEN_5 ⇒ FC_6 ⇒ DROP_7 ⇒ FC_8 ⇒ SOFTMAX )</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="72-합성곱-계층">7.2 합성곱 계층</h1>

<p>패딩$\mathsf{^{padding}}$ : 필터 통과 후 데이터 사이즈 조절</p>

<p>스트라이드$\mathsf{^{stride}}$ : 필터가 이동하는 보폭</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="721-완전연결-계층의-문제점">7.2.1 완전연결 계층의 문제점</h2>

<p>완전연결 계층은 입력데이터가 다차원 이더라도 Flatten작업을 통해
그것의 형상을 무시하고 1차원 데이터로 만들어서 학습한다.</p>

<p>때문에 칼라 이미지 데이터처럼 다차원 형상의 데이터인 경우
인접한 데이터끼리 연관이 있을 가능성이 큰데
이를 무시해서 정확도가 더 낮을 수 있다.</p>

<p>반면 CNN은 입력 데이터의 형상을 보존하고
한 번에 데이터 전체를 보는 것이 아니라 부분 부분 필터를 적용하며
합성곱을 하기 때문에 인접한 데이터끼리의 연관성을 파악할 수 있다.</p>

<p>또한, 이러한 점 덕분에 음성 데이터의 처리에도 강점을 보이게 된다.</p>

<p>CNN에서 입출력 데이터를 특징맵$\mathsf{^{feature\ map}}$이라고 한다.</p>

<p>입력 특징 맵$\mathsf{^{input\ feature\ map}}$ : 합성곱 계층의 입력 데이터</p>

<p>출력 특징 맵$\mathsf{^{output\ feature\ map}}$ : 합성곱 계층의 출력 데이터</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="722-합성곱-연산">7.2.2 합성곱 연산</h2>

<p>데이터와 필터의 형상을 (높이, 너비)로 표기 한다.</p>

<p>문헌에 따라 필터를 커널이라 칭하기도 한다.</p>

<p>합성곱 연산은 필터의 윈도우를 일정 간격으로 이동해가며 단일 곱셈-누산$\mathsf{^{fused\ multiply-add,\ FMA}}$를 한다.</p>

<p>필터에 매개변수가 그동안의 가중치와 같은 역할을 한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled.jpeg" alt="Untitled" /></p>

<p>편향까지 포함한 흐름이다. 편향은 항상 (1,1)이다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled.png" alt="Untitled" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="723-패딩">7.2.3 패딩</h2>

<p>패딩 : 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채우는 것</p>

<p>아래는 zero padding이고 padding_size = 1</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%201.png" alt="Untitled" /></p>

<p>입력 데이터 형상 유지에 사용</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="724-스트라이드">7.2.4 스트라이드</h2>

<p>스트라이드 : 필터가 움직이는 간격</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%201.jpeg" alt="Untitled" /></p>

<p>필터 적용시 출력 크기 관계식</p>

<p>입력 크기 : (H, W)</p>

<p>필터 크기 : (FH, FW)</p>

<p>출력 크기 : (OH, OW)</p>

<p>패딩 : P</p>

<p>스트라이드 : S</p>

\[OH={H+2P-FH\over S}+1\\
OW={W+2P-FW\over S}+1\]

<p>단, 출력 크기는 모두 정수여야 함.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="725-3차원-데이터의-합성곱-연산">7.2.5 3차원 데이터의 합성곱 연산</h2>

<p>채널까지 고려한 3차원 데이터를 연산해 본다.</p>

<p>입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더해서 하나의 출력을 얻는다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%202.jpeg" alt="Untitled" /></p>

<p>주의할점</p>

<ul>
  <li>입력 데이터의 채널 수 = 필터의 채널 수</li>
  <li>모든 채널의 필터 크기가 같아야함</li>
</ul>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="726-블록으로-생각하기">7.2.6 블록으로 생각하기</h2>

<p>데이터 형상 = (채널, 높이, 너비)</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%203.jpeg" alt="Untitled" /></p>

<p>위에서의 결과는 채널이 1개다.</p>

<p>따라서 특징 맵을 여러장 얻기 위해서는 필터를 여러장 사용하면 된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%202.png" alt="Untitled" /></p>

<p>필터의 형상 = (필터 수 = 출력 채널 수, 채널 수 = 입력 채널 수, 높이, 너비)</p>

<p>필터를 FN개 만큼 만들어서 합성곱을 하면 출력 데이터의 형상에서 채널 수가 FN개가 된다.</p>

<p>다음은 편향을 적용한 그림 ( 편향은 브로드캐스팅으로 계산됨 )</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%203.png" alt="Untitled" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="727-배치-처리">7.2.7 배치 처리</h2>

<p>한번에 여러개의 데이터를 학습하기 위해 배치 처리를 지원하게 하려면</p>

<p>데이터의 차원을 하나 늘려줘야 한다. ( 3차원 ⇒ 4차원 )</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%204.jpeg" alt="Untitled" /></p>

<p>데이터 형상 = ( 데이터 수, 채널 수, 높이, 너비 )</p>

<p>정리 : 4차원 데이터가 하나 하른다 → N개에 대한 합성곱 연산이 이뤄진다 = N회 분의 처리를 한 번에 수행한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="73-풀링-계층">7.3 풀링 계층</h1>

<p>세로, 가로 방향의 공간을 줄이는 연산</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%205.jpeg" alt="Untitled" /></p>

<p>2X2 최대 풀링을 스트라이드 2로 처리 ( 보통 풀링 윈도우 크기와 스트라이드는 같은 값으로 설정 )</p>

<p>풀링의 종류는 최대 풀링 말고도 평균 풀링 같은 것이 있지만 이미지에서는 주로 최대 풀링 사용</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="731-풀링-계층의-특징">7.3.1 풀링 계층의 특징</h2>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="1-학습해야-할-매개변수가-없다">1. 학습해야 할 매개변수가 없다</h3>

<p>풀링 계층은 오히려 매개변수를 줄인다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="2-채널-수가-변하지-않는다">2. 채널 수가 변하지 않는다</h3>

<p>풀링 연산을 통해 각 채널의 높이와 너비는 작아져도</p>

<p>채녈마다 독립적으로 계산해서 채널 수는 그대로다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="3-입력의-변화에-영향을-적게-받는다--강건하다-">3. 입력의 변화에 영향을 적게 받는다 ( 강건하다 )</h3>

<p>입력 데이터가 조금 변해도 풀리의 결과는 잘 변하지 않는다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%206.jpeg" alt="Untitled" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="74-합성곱--풀링-계층-구현하기">7.4 합성곱 / 풀링 계층 구현하기</h1>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="741-4차원-배열">7.4.1 4차원 배열</h2>

<p>CNN에는 4차원 데이터가 흐른다.</p>

<p>ex&gt; (10, 1, 28, 28) ⇒ 높이 28, 너비 28, 채널 1개인 데이터가 10개</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="742-im2col로-데이터-전개하기">7.4.2 im2col로 데이터 전개하기</h2>

<p>합성곱 연산의 가장 간단한 구현 방법은 for 문을 겹겹이 쓰는 것이다.</p>

<p>그렇게하면 numpy의 퍼포먼스가 떨어지기 때문에 좋은 방법이 아니다.</p>

<p>대신 im2col을 사용해서 쉽게 구현 가능하다.</p>

<p>그림은 3차원을 2차원으로 변환한 것이지만</p>

<p>4차원 데이터도 2차원으로 변환시켜준다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%204.png" alt="Untitled" /></p>

<p>im2col은 필터링하기 좋게 입력 데이터를 전개한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%205.png" alt="Untitled" /></p>

<p>여기서는 스트라이드를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만,</p>

<p>실제 상황에서는 영역이 겹치는 경우가 대부분이다.</p>

<p>필터 적용 영역이 겹치게 되면 im2col로 전개한 후의 원소 수가 원래 블록의 원소 수보다 많아진다.</p>

<p>그래서 im2col을 사용해 구현하면 메모리를 더 많이 소비하는 단점이 있다.</p>

<p>하지만 행렬 계산에 고도로 최적화된 선형 대수 라이브러리 등을 통해 다시 계산 효율을 높일 수 있다.</p>

<p>입력 데이터도 전개했으므로 필터도 전개해서 행렬 곱을 하면 된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%207.jpeg" alt="Untitled" /></p>

<p>CNN은 4차원 데이터가 흐르므로 다시 reshape한다.</p>

<p>ex&gt; (1,3,7,7) — im2col(5,5,stride=1,pad=0) —&gt; (9,75)</p>

<p>결과 ⇒ (입력 데이터 수 * 채널 하나에 들어가는 필터 수) X (필터 사이즈 * 입력 데이터의 채널 수)</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%206.png" alt="Untitled" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Convolution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">pad</span>
        
        <span class="c1"># 중간 데이터（backward 시 사용）
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>   
        <span class="bp">self</span><span class="p">.</span><span class="n">col</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">col_W</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="c1"># 가중치와 편향 매개변수의 기울기
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">FN</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">FH</span><span class="p">,</span> <span class="n">FW</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">out_h</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="nb">int</span><span class="p">((</span><span class="n">H</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">pad</span> <span class="o">-</span> <span class="n">FH</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">out_w</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="nb">int</span><span class="p">((</span><span class="n">W</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">pad</span> <span class="o">-</span> <span class="n">FW</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">)</span>

        <span class="n">col</span> <span class="o">=</span> <span class="n">im2col</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FH</span><span class="p">,</span> <span class="n">FW</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="p">)</span>
        <span class="n">col_W</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">FN</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">T</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">col_W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">col</span> <span class="o">=</span> <span class="n">col</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">col_W</span> <span class="o">=</span> <span class="n">col_W</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">FN</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">FH</span><span class="p">,</span> <span class="n">FW</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">dout</span> <span class="o">=</span> <span class="n">dout</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">FN</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">col</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dW</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">FN</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">FH</span><span class="p">,</span> <span class="n">FW</span><span class="p">)</span>

        <span class="n">dcol</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">col_W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">col2im</span><span class="p">(</span><span class="n">dcol</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">FH</span><span class="p">,</span> <span class="n">FW</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="744-풀링-계층-구현하기">7.4.4 풀링 계층 구현하기</h2>

<p>합성곱과 다른점은 채널 독립적이라는 것.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch7/Untitled%208.jpeg" alt="Untitled" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Pooling</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool_h</span><span class="p">,</span> <span class="n">pool_w</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool_h</span> <span class="o">=</span> <span class="n">pool_h</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool_w</span> <span class="o">=</span> <span class="n">pool_w</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">pad</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arg_max</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">out_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_h</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">out_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_w</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">)</span>

        <span class="n">col</span> <span class="o">=</span> <span class="n">im2col</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_w</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="p">)</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">col</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_h</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">pool_w</span><span class="p">)</span>

        <span class="n">arg_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">,</span> <span class="n">C</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">arg_max</span> <span class="o">=</span> <span class="n">arg_max</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dout</span> <span class="o">=</span> <span class="n">dout</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">pool_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_h</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_w</span>
        <span class="n">dmax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dout</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="n">pool_size</span><span class="p">))</span>
        <span class="n">dmax</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">arg_max</span><span class="p">.</span><span class="n">size</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">arg_max</span><span class="p">.</span><span class="n">flatten</span><span class="p">()]</span> <span class="o">=</span> <span class="n">dout</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">dmax</span> <span class="o">=</span> <span class="n">dmax</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dout</span><span class="p">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">pool_size</span><span class="p">,))</span> 
        
        <span class="n">dcol</span> <span class="o">=</span> <span class="n">dmax</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dmax</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dmax</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dmax</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">col2im</span><span class="p">(</span><span class="n">dcol</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool_w</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<p>과정</p>

<ol>
  <li>입력 데이터 전개</li>
  <li>행별 최댓값 계산</li>
  <li>reshape</li>
</ol>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="75-cnn-구현">7.5 CNN 구현</h1>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="정리">정리</h2>

<ul>
  <li>데이터 준비
    <ul>
      <li>데이터 강화 ( flip, move, rotate )</li>
      <li>training data, validation data, test data
        <ul>
          <li>데이터의 분포가 고르게 나눈다.
            <ul>
              <li>분류를 하려면 3가지 데이터에 각 클래스가 차지하는 비율이 비슷해야함</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>data normalization</li>
    </ul>
  </li>
  <li>모델 정의
    <ul>
      <li>어떤 층을 조합할건지</li>
      <li>몇 층까지 쌓을건지</li>
      <li>합성곱층은 filter를 몇개로 할건지, filter size는 몇개로 할건지, padding과 stride는 어떻게 할건지, 활성화 함수는 뭘 쓸건지</li>
      <li>풀링층은 pooling size를 몇으로 할건지, padding과 stride는 몇으로 할건지</li>
      <li>Dropout은 몇%만큼 뉴런을 막을건지</li>
      <li>전결합층은 뉴런을 몇개 사용할건지</li>
      <li>Regularization은 뭘로 할건지, 얼마나 할건지</li>
    </ul>
  </li>
  <li>모델 학습
    <ul>
      <li>Optimizer는 뭘 사용할건지 learning rate는 몇으로 시작할건지</li>
      <li>batch size 는 몇으로 할지</li>
      <li>epochs는 몇으로 할건지, 조기종료 조건은 어떻게 할건지</li>
    </ul>
  </li>
</ul>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;DeepLearning from scratch&quot;]" /><summary type="html"><![CDATA[7.1 전체 구조]]></summary></entry><entry><title type="html">Ch6 학습 관련 기술들</title><link href="http://192.168.0.41:4000/deeplearning%20from%20scratch/2022/08/27/Ch6-%ED%95%99%EC%8A%B5%EA%B4%80%EB%A0%A8%EA%B8%B0%EC%88%A0%EB%93%A4.html" rel="alternate" type="text/html" title="Ch6 학습 관련 기술들" /><published>2022-08-27T19:40:27+09:00</published><updated>2022-08-27T19:40:27+09:00</updated><id>http://192.168.0.41:4000/deeplearning%20from%20scratch/2022/08/27/Ch6-%ED%95%99%EC%8A%B5%EA%B4%80%EB%A0%A8%EA%B8%B0%EC%88%A0%EB%93%A4</id><content type="html" xml:base="http://192.168.0.41:4000/deeplearning%20from%20scratch/2022/08/27/Ch6-%ED%95%99%EC%8A%B5%EA%B4%80%EB%A0%A8%EA%B8%B0%EC%88%A0%EB%93%A4.html"><![CDATA[<h1 id="61-매개변수-갱신">6.1 매개변수 갱신</h1>

<p>최적화$\mathsf{^{optimization}}$ : 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="611-모험가-이야기">6.1.1 모험가 이야기</h2>

<p>최적화 문제는 지도없이 눈을 가리고 광활한 산에서 가장 깊고 낮은 골짜기를 찾는 것이라고 비유 할 수 있다.</p>

<p>이때 단 하나의 단서로 발을 통해 땅의 기울기만 느낄 수 있다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="612-확률적-경사-하강법--sgd-stochastic-gradient-descent">6.1.2 확률적 경사 하강법 ( SGD, Stochastic Gradient Descent)</h2>

\[\mathrm{W}\leftarrow\mathrm{W}-\eta{\partial L\over\partial\mathrm{W}}\]

<p>$\mathbf{W}$ : 갱신할 가중치 매개변수</p>

<p>${\partial L\over\partial\mathbf{W}}$: $\mathbf{W}$에 대한 손실 함수의 기울기</p>

<p>$\eta$ : 학습률</p>

<p>$\leftarrow$ : 좌변의 값을 우변의 값으로 갱신</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SGD</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
	
	<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
			<span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</code></pre></div></div>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="613-sgd의-단점">6.1.3 SGD의 단점</h2>

<p>특정한 지표 없이 모든 매개변수를 검사하면서 손실 함수가 가장 낮은 지점을 찾는 것보다 SGD 처럼 기울기를 통해 방향성을 찾고 움직이는 것이 더 효율적인 방법이다.</p>

<p>SGD는 단순하고 구현도 쉽지만,</p>

<p>문제에 따라 비효율적일 때가 있다.</p>

\[f(x,y)={1\over20}x^2+y^2\]

<p>위 식을 3차원(좌)과 등고선(우)으로 그려보았다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled.png" alt="Untitled" /></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%201.png" alt="Untitled" /></p>

<p>몇개의 $f(x,y)$ 점에서 기울기를 그려보면</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%202.png" alt="Untitled" /></p>

<p>위와 같은데 $f(x,y)$ 의 실제 최소값은 $(0,0)$이지만</p>

<p>대부분의 기울기 방향이 $(0,0)$을 가리키지 않기 때문에 문제가 발생</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%203.png" alt="Untitled" /></p>

<p>lr = 0.9 &amp; iter_num = 40</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="614-모멘텀--momentum-">6.1.4 모멘텀 ( Momentum )</h2>

<p>모멘텀$\mathsf{^{momentum}}$은 ‘운동량’을 뜻하는 단어로, 물리와 관계가 있다.</p>

<p>수식은 다음과 같다.</p>

\[\mathbf{v}\leftarrow\alpha\mathbf{v}-\eta{\partial L\over\partial\mathbf{W}}\\
\mathbf{W}\leftarrow\mathbf{W}+\mathbf{v}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%204.png" alt="Untitled" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Momentum</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
            
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">momentum</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</code></pre></div></div>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%205.png" alt="Untitled" /></p>

<p>lr = 0.1 &amp; iter_num = 30</p>

<p>모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직인다.</p>

<p>x축 방향으로의 힘은 작지만 방향이 변하지 않아 한 방향으로 일정하지만
y축 방향으로의 힘은 크면서 방향이 일정하지 않아 매개변수가 위아래로 흔들리면서 안정적이지 못한 모습을 보여준다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="615-adagrad">6.1.5 AdaGrad</h2>

<p>학습률 값은 중요하다.</p>

<p>너무 크면 최적화 과정에서 발산하고</p>

<p>너무 작으면 최적화에 드는 시간과 계산 비용이 많이 든다.</p>

<p>학습률을 정하는 효과적 기술로 학습률 감소$\mathsf{^{learning\ rate\ decay}}$가 있다.</p>

<p>학습을 진행하면서 학습률을 점차 줄여가는 방법이다.</p>

<p>매개변수 전체의 학습률 값을 일괄적으로 조절하면 구현이 간단하지만</p>

<p>AdaGrad는 더 효율적으로 하기 위해</p>

<p>각각의 매개변수에 맞게 학습률을 조절한다.</p>

\[\mathbf{h}\leftarrow\mathbf{h}+{\partial L\over\partial\mathbf{W}}\odot{\partial L\over\partial\mathbf{W}}\\
\mathbf{W}\leftarrow\mathbf{W}-\eta\dfrac{1}{\sqrt{\mathbf{h}}}\dfrac{\partial L}{\partial\mathbf{W}}\]

<p>$\mathbf{h}$에는 기존 기울기 값을 제곱하여 계속 더해준다.</p>

<p>그리고 $\mathbf{W}$를 업데이트할 때 $\sqrt{\mathbf{h}}$로 나눠준다.</p>

<p>이렇게 해서 각 매개변수의 원소마다 각각 많이 움직인 원소는 학습률이 낮아지게 적용이 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdaGrad</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="o">+</span><span class="mf">1e-7</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%206.png" alt="Untitled" /></p>

<p>lr = 1.4 &amp; iter_num = 30</p>

<p>y축 방향은 기우리각 커서 처음에 크게 움직이지만,</p>

<p>그 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%207.png" alt="Untitled" /></p>

<p><a href="https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam">Optimizer 의 종류와 특성 (Momentum, RMSProp, Adam)</a></p>

<p><a href="https://ko.wikipedia.org/wiki/%EC%9D%B4%EB%8F%99%ED%8F%89%EA%B7%A0">이동평균 - 위키백과, 우리 모두의 백과사전</a></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="616-adam">6.1.6 Adam</h2>

<p>Adagrad는 SGD에서 개선된 방법이지만</p>

<p>이동할 수록 학습률이 낮아져 멈추게 된다.</p>

<p>이 문제를 해결하기 위해 RMSProp 이 제시되었는데</p>

<p>RMSProp은 지수이동평균( EMA, Exponential Moving Average )을 이용해 이전에 움직였던 정보보다 최근 움직인 크기에 높은 가중치를 부여해 학습률이 낮아져서 학습이 멈추는 문제를 해결했다.</p>

<p>그리고 RMSProp과 Momentum을 합쳐서 Adam을 만들었다.</p>

<p>Adam은 Momentum에서 관성계수 m과 함께 계산된 v로 매개변수를 업데이트 하고 기울기 값과 기울기의 제곱값의 EMA를 활용해 step 변화량을 조절한다.</p>

\[\mathbf{m}\leftarrow\beta_1\mathbf{m}+(1-\beta_1){\partial L\over\partial\mathbf{W}}\\
\mathbf{v}\leftarrow\beta_2\mathbf{v}+(1-\beta_2){\partial L\over\partial\mathbf{W}}\odot{\partial L\over\partial\mathbf{W}}\\
\mathbf{W}\leftarrow\mathbf{W}-\mathbf{m}{\eta\over\sqrt{\mathbf{v}+\epsilon}}\]

<p>$\beta_1, \beta_2$는 0.9, 0.999가 적절하다고 한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%208.png" alt="Untitled" /></p>

<p>lr = 0.3 &amp; iter_num = 30</p>

<p>모멘텀과 비슷한 패턴이지만 상하 흔들림이 적은 이유는</p>

<p>학습의 갱신 강도를 적응적으로 조정해서 얻은 혜택이다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="617-어느-갱신-방법">6.1.7 어느 갱신 방법?</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%209.png" alt="Untitled" /></p>

<p>사용한 기법에 따라 갱신 경로가 다르지만 그림만 보면 이 문제에서는 AdaGrad가 가장 좋아보인다.</p>

<p>하지만 어떤 optimizer를 쓸지는 풀어야 하는 문제가 무엇이냐에 따라 다르고 하이퍼파라미터를 어떻게 설정하느냐에 따라서도 결과가 다르니 잘 골라야 한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="618-mnist-데이터셋으로-본-방법-비교">6.1.8 MNIST 데이터셋으로 본 방법 비교</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2010.png" alt="Untitled" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="62-가중치의-초깃값">6.2 가중치의 초깃값</h1>

<p>가중치의 초깃값은 어떻게 설정하느냐에 따라 학습의 성패가 갈려서 특히 중요하다.</p>

<p>때문에 연구를 통해 권장하는 초깃값을 사용해 학습이 이뤄지는 것을 확인.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="621-초기값을-0으로">6.2.1 초기값을 0으로</h2>

<h3 id="가중치-감소mathsfweight-decay">가중치 감소$\mathsf{^{weight\ decay}}$</h3>

<p>오버피팅을 억제해 범용 성능을 높이는 테크닉으로</p>

<p>가중치 매개변수의 값이 작아지도록 학습하는 방법이다.</p>

<p>가중치를 작게 만들고 싶으면 초깃값을 작게 시작하는게 정공법이다.</p>

<p>지금까지는 초깃값을 만들 때 정규분포에서 생성되는 값을 0.01배 한 작은 값(표준편차가 0.01인 정규분포)를 사용했다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="그렇다면-초깃값을-모두-0으로-설정하면-어떻게-되나">그렇다면 초깃값을 모두 0으로 설정하면 어떻게 되나?</h3>

<p>실제로 가중치 초깃값을 모두 0으로 설정하면 학습이 올바로 이뤄지지 않는다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="초깃값을-모두-0으로-해서는-안-되는-이유는-정확히는-가중치를-균일한-값으로-설정-해서는-안되는-이유-">초깃값을 모두 0으로 해서는 안 되는 이유는( 정확히는 가중치를 균일한 값으로 설정 해서는 안되는 이유 )?</h3>

<p>오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문에 문제가 발생한다.</p>

<p>예를 들어 2층 신경망에서 첫 번째와 두 번째 층의 가중치가 0이면 순전파 때는 입력층의 가중치가 0이기 때문에 두 번째 층의 뉴런에 모두 같은 값이 전달된다.</p>

<p>두번째 층의 모든 뉴런에 같은 값이 입력된다는 것은 역전파 때 두 번째 층의 가중치가 모두 똑같이 갱신된다는 것이기 때문에</p>

<p>갱신을 거쳐도 여전히 같은 값을 유지하게 되고
이는 가중치를 여러 개 갖는 의미를 사라지게 한다.</p>

<p>따라서 초깃값을 무작위로 설정해야 한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="622-은닉층의-활성화값-분포">6.2.2 은닉층의 활성화값 분포</h2>

<p>sigmoid함수를 활성함수로 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그려보겠다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2011.png" alt="Untitled" /></p>

<p>histogram을 보면 각 층의 활성화값들이 0과 1에 치우쳐 분포한다.</p>

<p>초깃값 설정은 다음과 같이 했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">0</span>
</code></pre></div></div>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2012.png" alt="Untitled" /></p>

<p>초깃값은 표준편차가 1이고 평균은 0인 정규분포를 따르도록 (100,100) 모양으로 무작위로 골랐다. ( 입력값도 같은 정규분포를 따름 )</p>

<p>때문에 입력값과 가중치의 곱이 활성함수 sigmoid를 통과하게 되면 모든 층에서 출력값이 0과 1에 치우치게 된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2013.png" alt="Untitled" /></p>

<p>이런 경우는 역전파 계산시 미분 값이 0에 가까운수를 계속 곱해나가 기울기 소실$\mathsf{^{gradient\ vanishing}}$ 문제를 생긴다</p>

<p>이번에는 가중치의 표준편차를 0.01로 바꿔서 다시 한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2014.png" alt="Untitled" /></p>

<p>이전처럼 0과 1로 치우치진 않았으나 기울기 소실 문제는 일어나지 않는다.</p>

<p>하지만 앞에서 가중치를 균일하게 설정했을 때 다수의 뉴런이 거의 같은 값을 출력해서 뉴런을 여러 개 둔 의미가 없어진다는 문제를 말했었다.</p>

<p>뉴런들이 비슷한 값을 출력하면 여러 개를 사용한 의미가 없는 거고 그 의미는 표현력을 제한한다는 관점에서 문제가 된다.</p>

<p>각 층의 활성화값은 적당히 고루 분포해 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이뤄진다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="xavier-초깃값">Xavier 초깃값</h3>

<p>사비에르 그롤로트$\mathsf{^{Xavier\ Glorot}}$와 요슈아 벤지오$\mathsf{^{Yoshua\ Bengio}}$의 논문에서 권장하는 가중치 초깃값.</p>

<p>일반적인 딥러닝 프레임워크들이 표준적으로 이용하고 있다.</p>

<p>니 논문은 각 층의 활성화값들을 광범위하게 분포시키려면</p>

<p>앞 계층의 노드가 $n$개일 때 표준 편차가 $1\over\sqrt n$인 분포를 사용해야 한다는 결론을 말한다.</p>

<p>가중치 설정 코드</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">node_num</span><span class="p">,</span> <span class="n">node_num</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">node_num</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2015.png" alt="Untitled" /></p>

<p>지난 두차례 결과보다 확실히 넓게 분포된 것을 확인 했다.</p>

<p>하지만 1층의 결과는 종모양으로 제대로 분포되어있는데</p>

<p>층을 통과할 수록 모양이 일그러지고 있다.</p>

<p>이는 sigmoid함 수가 (0,0.5)에서 대칭인 S자 곡선이기 때문인데</p>

<p>원점에서 대칭인 S곡선인 tanh를 활성화 함수로 사용하여 이를 해결 한다고 한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="tanh">tanh</h3>

\[\mathsf{tanh}={e^x-e^{-x}\over e^x+e^{-x}}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2016.png" alt="Untitled" /></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2017.png" alt="Untitled" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="623-relu를-사용할-때의-가중치-초깃값">6.2.3 ReLu를 사용할 때의 가중치 초깃값</h2>

<p>Xavier 초깃값이 학습에 효율적인 이유는 활성함수로 sigmoid와 tanh를 사용했기 때문이다.</p>

<p>Xavier는 선형 함수인 활성함수를 상대로 효율이 좋은데 sigmoid와 tanh는 중앙 부근에서 선형이라 볼 수 있기 때문이다.</p>

<p>반면에 ReLu를 사용하려면 ReLu에 특화된 초깃값을 이용해야 한다.</p>

<p>카이밍 히$\mathsf{^{Kaiming\ He}}$의 이름을 따 ‘He 초깃값’ 이라 한다.</p>

<p>He 초깃값은 앞 계층의 노드가 n개일 때,</p>

<p>표준편차가 $\sqrt {2\over n}$ 인 정규분포를 사용한다.</p>

<p>Xavier 초깃값은 표준편차가 $1\over\sqrt n$ 이었는데
ReLu는 음의 영역에서 결과값이 0이라</p>

<p>뉴런의 출력값을 더 넓게 분포시키기 위해 2배의 계수를 적용했다고 해석할 수 있다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2018.png" alt="Untitled" /></p>

<p>ReLu 표준편차 0.01</p>

<p>층들의 활성함수값들이 아주 작아서 신경망에 데이터가 조금 흐르게 되는데</p>

<p>가중치 작게해서 오버피팅 피하려다</p>

<p>역전파에서 기울기가 작아서 학습이 안됨</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2019.png" alt="Untitled" /></p>

<p>ReLu Xavier</p>

<p>처음엔 괜찮은데 층이 깊어지면서 치우침이 커져 ‘기울기 소실’ 문제 발생</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2020.png" alt="Untitled" /></p>

<p>ReLu He</p>

<p>모든 층에서 균일하게 분포, 층이 깊어져도 분포가 균일하여 학습이 적절히 된다고 기대할 수 있다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="624-mnist-데이터셋으로-본-가중치-초깃값-비교">6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교</h2>

<p>5층신경망, 활성함수 : ReLu</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2021.png" alt="Untitled" /></p>

<ol>
  <li>
    <p>std = 0.01</p>

    <p>위에서 봤듯이 가중치가 너무 작아 기울기 소실이 발생해 학습이 되고있지 않음</p>
  </li>
  <li>
    <p>He</p>

    <p>ReLu에 잘 맞는 초깃값을 제공하는 방법으로 제일 학습 속도가 빠름</p>
  </li>
  <li>
    <p>Xavier</p>

    <p>학습이 잘 진행되지만 He 초깃값보다 학습 진도가 느림</p>
  </li>
</ol>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="63-배치-정규화">6.3 배치 정규화</h1>

<p>앞 절에서는 각 층의 가중치의 초깃값을 조절하여 활성화값의 분포를 관찰했는데</p>

<p>이번에는 각 층이 활성화값을 적당히 퍼뜨리도록 ‘강제’해보겠다.</p>

<p>배치 정규화$\mathsf{^{Batch\ Normalization}}$가 그런 아이디어에서 출발했다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="631-배치-정구화-알고리즘">6.3.1 배치 정구화 알고리즘</h2>

<p>배치 정규화의 장점</p>

<ol>
  <li>학습을 빨리 진행할 수 있따. (학습 속도 개선)</li>
  <li>초깃값에 크게 의존하지 않는다 ( 골치 아픈 초깃값 선택 장애 해결)</li>
  <li>오버피팅을 억제한다(드롭아웃 등의 필요성 감소)</li>
</ol>

<p>배치 정규화는 학습 시 미니배치를 단위로 정규화한다.</p>

<p>평균 0, 분산 1</p>

\[\mu_B\leftarrow{1\over m}\sum\limits^m_{i=1}x_i\\
\sigma^2_B\leftarrow{1\over m}\sum\limits^m_{i=1}(x_i-\mu_B)^2\\
\hat x_i\leftarrow{x_i-\mu_B\over\sqrt{\sigma^2_B+\epsilon}}\]

<p>위 식을 통해 배치 정규화를 하는데</p>

<p>이 단계를 활성화 함수의 앞이나 뒤에 삽입함으로써 데이터 분포를 고르게 만들 수 있다.</p>

<p>그리고 배치 정규화 계층마다 아래 수식을 통해 정규화된 데이터에 고유한 확대와 이동 변환을 수행한다.</p>

\[y_i\leftarrow\gamma\hat x_i+\beta\]

<p>gamma가 확대를, beta가 이동을 처리한다.</p>

<p>처음에는 $\gamma = 1,\ \beta=0$ 부터 시작하고,</p>

<p>학습하면서 적합한 값으로 조정해간다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="backpropagation-in-batch-normalization-layer">Backpropagation in Batch Normalization Layer</h2>

<p><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">Understanding the backward pass through Batch Normalization Layer</a></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2022.png" alt="Untitled" /></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="632-배치-정규화의-효과">6.3.2 배치 정규화의 효과</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2023.png" alt="Untitled" /></p>

<p>거의 모든 경우에서 배치 정규화를 사용할 때의 학습 진도가 빠르다.</p>

<p>실제로 배치 정규화를 이용하지 않는 경우엔 초깃값이 잘 분포되어 있지 않으면 학습이 전혀 진행되지 않는다.</p>

<p>따라서 배치 정규화를 사용하면 학습이 빠르고,</p>

<p>가중치 초깃값에 크게 의존하지 않아도 된다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="64-바른-학습을-위해">6.4 바른 학습을 위해</h1>

<p>오버피팅이 문제가 되는 일이 많다.</p>

<p>복잡하고 표현력이 높은 모델을 만들 수는 있지만,</p>

<p>그만큼 오버피팅을 억제하는 기술도 중요하다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="641-오버피팅">6.4.1 오버피팅</h2>

<p>오버피팅이 일어나는 경우</p>

<ol>
  <li>매개변수가 많고 표현력이 높은 모델</li>
  <li>훈련 데이터가 적음</li>
</ol>

<p>이번에는 일부러 오버피팅을 일으키기 위해</p>

<ol>
  <li>7층 네트워크를 사용</li>
  <li>60000개 MNIST 데이터셋의 훈련 데이터 중 300개만 사용</li>
</ol>

<p>해보겠다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2024.png" alt="Untitled" /></p>

<p>train의 정확도는 거의 100%가 되었고</p>

<p>test의 정확도는 train과 큰 차이를 보이고 있다.</p>

<p>이런 정확도를 보이는 모델를 overfitting 되었다고 한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="642-가중치-감소">6.4.2 가중치 감소</h2>

<p>오버피팅 억제용으로 오래된 해결방법으로는 가중치 감소$\mathsf{^{weight\ decay}}$가 있다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="가중치-감소-weight-decay">가중치 감소 weight decay</h3>

<p>오버피팅은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문에 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부과하여 오버피팅을 피하는 방법</p>

<p>신경망 학습의 목적은 손실 함수의 값을 줄이는 것이므로</p>

<p>손실함수에 가중치의 L2 norm을 더해서 학습을 진행하면</p>

<p>신경망은 가중치가 커지는 것을 억제할 것이다.</p>

<p>그리고 앞에 lambda를 곱해 가중치에 대한 패널티를 조절한다.</p>

\[L\leftarrow L+{1\over2}\lambda\mathbf{W}^2\]

<p>식을 위와 같이 구성하여 역전파 과정에서는 $\lambda\mathbf{W}$를 더하면 된다.</p>

<p>$\lambda$를 0.1로 적용해 가중치 감소를 사용해 학습을 하면 아래와 같은 결과가 나온다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2025.png" alt="Untitled" /></p>

<p>train과 test의 정확도 차이가 줄었고 train의 정확도가 100%에 도달하지 못한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="643-드롭아웃">6.4.3 드롭아웃</h2>

<p>가중치 감소는 구현이 간단하고 어느 정도 지나친 학습을 억제하지만</p>

<p>신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어려워진다.</p>

<p>이럴 때는 흔히 드롭아웃$\mathsf{^{Dropout}}$을 사용한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2026.png" alt="Untitled" /></p>

<p>드롭 아웃의 역전파는 ReLu와 같다.</p>

<p>순전파 때 신호를 통과시키는 뉴런은 역전파 때도 신호를 그대로 통과시키고,</p>

<p>순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dropout</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout_ratio</span> <span class="o">=</span> <span class="n">dropout_ratio</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train_flg</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">train_flg</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout_ratio</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout_ratio</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">dout</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span>
</code></pre></div></div>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2027.png" alt="Untitled" /></p>

<p>드롭아웃을 사용하니 데이터에 대한 정확도 차이가 줄고
훈련 데이터에 대한 정확도가 100%에 도달하지 않는다.</p>

<p>이처럼 드롭아웃을 이용하면 표현력을 높이면서도 오버피팅을 억제할 수 있다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h3 id="ensemble-learning">Ensemble Learning</h3>

<p>앙상블 학습은 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식이다.</p>

<p>신경망의 맥락에서 얘기하면, 가령 같은 구조의 네트워크를 5개 준비하여 따로따로 학습시키고, 시험 때는 그 5개의 출력을 평균 내어 답하는 것이다.</p>

<p>앙상블 학습을 수행하면 신경망의 정확도가 몇% 정도 개선된다는 것이 실험적으로 알려져 있다.</p>

<p>앙상블 학습은 드롭아웃과 밀접하다.</p>

<p>드롭아웃이 학습 때 뉴런을 무작위로 삭제하는 행위를</p>

<p>앙상블 학습은 다른 모델을 학습시키는 것으로 해석할 수 있기 때문</p>

<p>추론 때는 뉴런의 출력에 삭제한 비율을 곱함으로써 앙상블 학습에서 여러 모델의 평균을 내는 것과 같은 효과를 얻는 것이다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="65-적절한-하이퍼파라미터-값-찾기">6.5 적절한 하이퍼파라미터 값 찾기</h1>

<p>신경망에는 하이퍼파라미터가 많이 등장한다.</p>

<p>하이퍼파라미터를 적절히 설정하지 않으면 모델의 성능이 크게 떨어지기도 하기 때문에 조심해야한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="651-검증-데이터">6.5.1 검증 데이터</h2>

<p>데이터를 학습 데이터와 테스트 데이터로만 나눴는데 지금부터는 하이퍼파라미터를 검증하기 위해 검증 데이터를 추가한다.</p>

<p>테스트 데이터로 하이퍼파라미터 검증하면 하이퍼파라미터가 테스트 데이터에 오버피팅 되기 때문이다.</p>

<p>데이터셋에 따라서는 훈련, 검증, 시험 데이터를 미리 분리해둔 것도 있는데</p>

<p>지금은 훈련 데이터중 20%를 검증 데이터로 분리하겠다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="652-하이퍼파라미터-최적화">6.5.2 하이퍼파라미터 최적화</h2>

<p>하이퍼파라미터 최적화할 때의 핵심은 하이퍼파라미터의 최적 값이 존재하는 범위를 조금씩 줄여간다는 것이다. 
범위를 조금씩 줄이려면 우선 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸(샘플링) 후, 그 값으로 정확도를 평가.</p>

<p>정확도를 잘 살피면서 이 작업을 여러 번 반복하며 하이퍼파라미터의 최적 값의 범위를 좁혀가는 것이다.</p>

<p>신경망의 하이퍼파라미터 최적화에서는 그리드 서치같은 규칙적인 탐색보다는 무작위로 샘플링해 탐색하는 편이 좋은 결과를 낸다고 알려져 있다.</p>

<p>최종 정확도에 미치는 영향력이 하이퍼파라미터마다 다르기 때문</p>

<p>하이퍼파라미터의 범위는 대략적으로 로그 스케일로 지정하는것이 효과적이다.</p>

<p>딥러닝 학습에는 오랜 시간이 걸려서 나쁠 듯한 값은 일찍 포기하는 것이 좋다.</p>

<p>따라서 에폭을 작게 하여, 1회 평가에 걸리는 시간을 단축하는 것이 효과적이다.</p>

<ul>
  <li>0 단계
    <ul>
      <li>하이퍼파라미터 값의 범위 설정 ( 로그스케일 )</li>
    </ul>
  </li>
  <li>1 단계
    <ul>
      <li>설정된 범위에서 하이퍼파라미터의 값을 무작위로 추룰</li>
    </ul>
  </li>
  <li>2 단계
    <ul>
      <li>1 단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가한다. (에폭 작게)</li>
    </ul>
  </li>
  <li>3 단계
    <ul>
      <li>1, 2단계를 특정 횟수 ( 100회 등 ) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.</li>
    </ul>
  </li>
</ul>

<p>이 방법은 과학이라기 보다 수행자의 지혜와 직관에 의존한다.</p>

<p>베이즈 정리를 중심으로 한 베이즈 최적화는 수학 이론을 구사하여 더 엄밀하고 효율적으로 최적화를 수행한다.</p>

<p><a href="https://proceedings.neurips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf</a></p>

<p>\(\begin{align*}\end{align*}\)</p>
<h2 id="653-하이퍼파라미터-최적화-구현하기">6.5.3 하이퍼파라미터 최적화 구현하기</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2028.png" alt="Untitled" /></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch6/Untitled%2029.png" alt="Untitled" /></p>

<p>학습이 잘 진행될 때의 학습률은 0.001~0.01,</p>

<p>가주이 감소 계수는 $10^{-8}$~$10^{-6}$ 정도이다.</p>

<p>이처럼 잘될 것 같은 값의 범위를 관찰하고 범위를 좁혀서 같은 작업을 반복한다.</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;DeepLearning from scratch&quot;]" /><summary type="html"><![CDATA[6.1 매개변수 갱신]]></summary></entry><entry><title type="html">Hnadling long term dependencies</title><link href="http://192.168.0.41:4000/rnn%20note/2022/08/23/Handling-long-term-dependencies.html" rel="alternate" type="text/html" title="Hnadling long term dependencies" /><published>2022-08-23T21:27:13+09:00</published><updated>2022-08-23T21:27:13+09:00</updated><id>http://192.168.0.41:4000/rnn%20note/2022/08/23/Handling-long-term-dependencies</id><content type="html" xml:base="http://192.168.0.41:4000/rnn%20note/2022/08/23/Handling-long-term-dependencies.html"><![CDATA[<h1 id="vanilla-rnn의-한계">Vanilla RNN의 한계</h1>
<p>아래 Vanilla RNN 의 구조를 보자.<br />
각 timestep별로 결과에 얼마나 영향을 주는지 색으로 표현했을때<br />
색이 짙을 수록 역전파 과정에서 피드백 크기가 점점 작아져서 결과에 영향을 거의 안주게 된다.<br />
만약 앞에 입력이 결과에 영향을 줘야하는 경우라면 현재 timestep으로부터 먼 앞에 입력을 기억하고 있지 않다면<br />
<strong>Long Term Dependencies problem</strong> 이 발생하게 된다.</p>

<p align="center"><img style="width: 70%" src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_0.png" /></p>
<h5><center>출처: Wiki Docs</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="자주-사용되는-activation-functions">자주 사용되는 activation functions</h1>
<p>RNN에서 가장 많이 사용되는 activation functions들은 아래와 같다.</p>

<p align="center"><img src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_1.png" /></p>
<h5><center>출처: CS 230</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="vanishingexploding-gradient">Vanishing/exploding gradient</h1>
<p>RNN을 사용하다보면 Vanishing/exploding gradient 문제를 자주 만나게 된다.<br />
층의 수에 따라 기하급수적으로 커지거나 작아지는 기울기 때문에<br />
long term dependencies를 포착하기 어려워서<br />
Vanishing/exploding gradient 문제가 발생하게 된다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="gradient-clipping">Gradient clipping</h1>
<p>이 테크닉은 backpropagation 수행중에 가끔 마주치는<br />
exploding gradient 문제에 대응하기 위해 사용된다.</p>

<p align="center"><img style="width: 60%" src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_2.png" /></p>
<h5><center>출처: CS 230</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="grulstm">GRU/LSTM</h1>
<p>Gated Recurrent Unit(GRU)와 GRU의 일반화 버전인 Long Short-Term Memory units(LSTM)은 전통적인 RNNs를 통해 마주하는 vanishing gradient problem을 다루기 위해 사용한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="types-of-gates">Types of Gates</h1>
<p><strong>vanishing gradient</strong> 문제를 해결하기 위해 RNNs에서는 목적이 잘 정의된 특정 gate들을 사용한다.<br />
그것들을 보통 $\Gamma$ 로 표기한다.</p>

\[\Gamma=\sigma\left(Wx^{&lt;t&gt;}+Ua^{&lt;t-1&gt;}+b\right)\]

<p>$W,U,b$ 는 gate의 고유한 계수들이고 $\sigma$ 는 sigmoid function이다.<br />
아래는 주요 내용이다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Type of gate</th>
      <th style="text-align: center">Role</th>
      <th style="text-align: center">Used in</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Update gate $\Gamma_u$</td>
      <td style="text-align: center">과거가 현재에 얼마나 영향을 주는가</td>
      <td style="text-align: center">GRU,LSTM</td>
    </tr>
    <tr>
      <td style="text-align: center">Relevance gate $\Gamma_r$</td>
      <td style="text-align: center">이전 정보를 버릴지</td>
      <td style="text-align: center">GRU,LSTM</td>
    </tr>
    <tr>
      <td style="text-align: center">Forget gate $\Gamma_f$</td>
      <td style="text-align: center">cell을 지울지 말지</td>
      <td style="text-align: center">LSTM</td>
    </tr>
    <tr>
      <td style="text-align: center">Output gate $\Gamma_o$</td>
      <td style="text-align: center">cell의 어느정도를 내보낼지</td>
      <td style="text-align: center">LSTM</td>
    </tr>
  </tbody>
</table>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="lstm">LSTM</h1>
<p>LSTM의 cell의 구조는 아래와 같다.</p>
<p align="center"><img src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_3.png" /></p>
<h5><center>출처 : Explain LSTM &amp; GRU</center></h5>

\[\begin{align*}\\
\text{Input gate}\quad\rightarrow\quad i_t&amp;=\sigma\left(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}\right)\\ \\
\text{Forget gate}\quad\rightarrow\quad f_t&amp;=\sigma\left(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf} \right)\\ \\
\text{Cell(Gate) gate}\quad\rightarrow\quad g_t&amp;=\tanh\left(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg} \right)\\ \\
\text{Output gate}\quad\rightarrow\quad o_t&amp;=\sigma\left(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho} \right)\\ \\
\text{Cell state}\quad\rightarrow\quad c_t&amp;=f_t\odot c_{t-1}+i_t\odot g_t\\ \\
\text{Hidden state}\quad\rightarrow\quad h_t&amp;=o_t\odot\tanh\left(c_t\right)
\end{align*}\]

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="gru">GRU</h1>
<p>GRU의 cell의 구조는 아래와 같다.</p>

<p align="center"><img src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_4.png" /></p>
<h5><center>출처 : Explain LSTM &amp; GRU</center></h5>

\[\begin{align*}\\ 
\text{Reset gate}\quad\rightarrow\quad r_t&amp;=\sigma\left(W_{ir}x_t+b_{ir}+W_{hr}h_{t-1}+b_{hr}\right) \\ \\
\text{Update gate}\quad\rightarrow\quad z_t&amp;=\sigma\left(W_{iz}x_t+b_{iz}+W_{hz}h_{t-1}+b_{hz}\right) \\ \\
\text{New gate}\quad\rightarrow\quad n_t&amp;=\tanh\left(W_{in}x_t+b_{in}+r_t*\left(W_{hn}h_{t-1}+b_{hn}\right)\right) \\ \\
\text{Hidden state}\quad\rightarrow\quad h_t&amp;=\left(1-z_t\right)*n_t+z_t*h_{t-1}
\end{align*}\]

<p>\(\begin{align*}\end{align*}\)
\(\begin{align*}\end{align*}\)</p>
<h3 id="참고자료">참고자료:</h3>
<h4 id="cs-230---deep-learning"><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">CS 230 - Deep Learning</a>,</h4>
<h4 id="wiki-docs"><a href="https://wikidocs.net/22888">Wiki Docs</a>,</h4>
<h4 id="mit-6s191-recurrent-neural-networks-and-transformers"><a href="https://www.youtube.com/watch?v=QvkQ1B3FBqA">MIT 6.S191: Recurrent Neural Networks and Transformers</a>,</h4>
<h4 id="explain-lstm--gru"><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Explain LSTM &amp; GRU</a>,</h4>
<h4 id="pytorch-lstm"><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM">pytorch LSTM</a>,</h4>
<h4 id="pytorch-gru"><a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU">pytorch GRU</a></h4>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;RNN Note&quot;]" /><summary type="html"><![CDATA[Vanilla RNN의 한계 아래 Vanilla RNN 의 구조를 보자. 각 timestep별로 결과에 얼마나 영향을 주는지 색으로 표현했을때 색이 짙을 수록 역전파 과정에서 피드백 크기가 점점 작아져서 결과에 영향을 거의 안주게 된다. 만약 앞에 입력이 결과에 영향을 줘야하는 경우라면 현재 timestep으로부터 먼 앞에 입력을 기억하고 있지 않다면 Long Term Dependencies problem 이 발생하게 된다.]]></summary></entry><entry><title type="html">RNN Overview</title><link href="http://192.168.0.41:4000/rnn%20note/2022/08/22/RNN.html" rel="alternate" type="text/html" title="RNN Overview" /><published>2022-08-22T15:16:15+09:00</published><updated>2022-08-22T15:16:15+09:00</updated><id>http://192.168.0.41:4000/rnn%20note/2022/08/22/RNN</id><content type="html" xml:base="http://192.168.0.41:4000/rnn%20note/2022/08/22/RNN.html"><![CDATA[<h1 id="architecture-of-a-traditional-rnn">Architecture of a traditional RNN</h1>
<p><strong>RNN</strong>에서는 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 <strong>cell</strong>(메모리 셀, RNN 셀) 이라고 한다.</p>

<p>regular feed-forward network에서 hidden layer라고 부르던 뉴런은</p>

<p>RNN에서 <strong>hidden state</strong>라고 부른다.<br />
\(\begin{align*}\end{align*}\)</p>

<p><strong>hidden state</strong>에서는 이전 <strong>time step</strong>의 <strong>hidden state</strong> 로부터 얻은 출력값을 현재 <strong>time step</strong>의 <strong>hidden state</strong> 에서 입력값으로 사용한다.</p>

<p><img src="/assets/img/RNN%20Note/RNN_overview/RNN_0.png" alt="RNN_0" /></p>
<h5><center>출처: CS 230</center></h5>

<p><strong>timestep</strong> $t$ 에서 activation \(a^{&lt;t&gt;}\)와 output \(y^{&lt;t&gt;}\)는 아래와 같이 표현한다.</p>

\[\begin{align*}
a^{&lt;t&gt;}&amp;=g_1(W_{aa}a^{&lt;t-1&gt;}+W_{ax}x^{&lt;t&gt;}+b_a)\\
y^{&lt;t&gt;}&amp;=g_2(W_{ya}a^{&lt;t&gt;}+b_y)
\end{align*}\]

<p>$W_{ax},\ W_{aa},\ W_{ya},\ b_a,\ b_y$는 모든 <strong>timestep</strong>에서 변하지 않는 계수이고<br />
$g_1,\ g_2$는 활성함수들이다.<br />
\(\begin{align*}\end{align*}\)</p>

<p><img src="/assets/img/RNN%20Note/RNN_overview/RNN_1.png" alt="RNN_1" /></p>
<h5><center>출처: CS 230</center></h5>

<h2 id="rnn의-장점과-단점">RNN의 장점과 단점</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Advantages</th>
      <th style="text-align: left">Drawbacks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$\cdot$ 어떤 길이의 입력도 처리 가능</td>
      <td style="text-align: left">$\cdot$ 계산이 느림</td>
    </tr>
    <tr>
      <td style="text-align: left">$\cdot$ 입력 크기에 따라 모델 크기가 커지지 않음</td>
      <td style="text-align: left">$\cdot$ 오래된 <strong>timestep</strong>에 접근하기 어려움</td>
    </tr>
    <tr>
      <td style="text-align: left">$\cdot$ 시간에 대한 결과를 고려한 계산</td>
      <td style="text-align: left">$\cdot$ 현재 state에서는 미래의 입력을 고려하지 못함</td>
    </tr>
    <tr>
      <td style="text-align: left">$\cdot$ 전체 시간동안 가중치가 공유됨</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="applications-of-rnns">Applications of RNNs</h1>
<p>RNN model들은 NLP와 speech recognition 처럼 sequential data를 다루기 위해 사용된다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Type of RNN</th>
      <th style="text-align: center">Illustration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">One-to-one                              </td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_2.png" alt="RNN_2" /></td>
    </tr>
    <tr>
      <td style="text-align: center">One-to-many</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_3.png" alt="RNN_3" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Many-to-one</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_4.png" alt="RNN_4" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Many-to-many</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_5.png" alt="RNN_5" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Many-to-many</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_6.png" alt="RNN_6" /></td>
    </tr>
  </tbody>
</table>

<h5><center>출처: CS 230</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="loss-function">Loss function</h1>
<p>RNN의 경우 모든 timestep에서 loss function $\mathcal{L}$ 은 각 timestep 별 loss를 더하는 것으로 정의한다.</p>

\[\mathcal{L}(\hat y,y)=\sum\limits^{T_y}_{t=1}\mathcal{L}\left(\hat y^{&lt;t&gt;},y^{&lt;t&gt;}\right)\]

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="backpropagation-through-time">Backpropagation through time</h1>
<p>역전파는 각 시간별로 진행된다.<br />
timestep $T$ 인 경우, ${\partial\mathcal{L}^{(T)}\over\partial W}$ 는 아래와 같이 표현된다.</p>

\[{\partial\mathcal{L}^{(T)}\over\partial W}=\left.\sum\limits^{T}_{t=1}{\partial\mathcal{L}^{(T)}\over\partial W}\right|_{(t)}\]

<h2 id="how-to-do-bptt-">How to do <strong>BPTT</strong> ?</h2>
<p><img src="/assets/img/RNN%20Note/RNN_overview/RNN_7.png" alt="RNN_7" />
예를 들어 위와 같이 $S_t=W\cdot S_{t-1}$ 이고 $\mathcal{L}=W\cdot S_t$ 인 경우<br />
${\partial\ \mathcal{L}\over\partial\ W}$ 는 아래와 같이 구한다.</p>

\[\begin{align*}
{\partial\ \mathcal{L}\over\partial\ W}&amp;=
{\partial\ \mathcal{L}\over\partial\ S_t}\cdot{\partial\ S_t\over\partial\ W}\\ \\
{\partial\ S_t\over\partial\ W}&amp;=
\underbrace{\partial^+\ S_t\over\partial\ W}_{\mathsf{explicit}}+
\underbrace{\dfrac{\partial\ S_t}{\partial\ S_{t-1}}\cdot{\partial\ S_{t-1}\over\partial\ W}}_{\mathsf{implicit}}\\ \\
&amp;=
{\partial^+\ S_t\over\partial\ W}+{\partial\ S_t\over\partial\ S_{t-1}}\left[\underbrace{\partial^+\ S_{t-1}\over\partial\ W}_{\mathsf{explicit}}+\underbrace{\dfrac{\partial\ S_{t-1}}{\partial\ S_{t-2}}\cdot{\partial\ S_{t-2}\over\partial\ W}}_{\mathsf{implicit}}\right]=\dots\\ \\
&amp;=
{\partial\ S_t\over\partial\ S_t}\ {\partial^+\ S_t\over\partial\ W}+
{\partial\ S_t\over\partial\ S_{t-1}}\ {\partial^+\ S_{t-1}\over\partial\ W}+
{\partial\ S_t\over\partial\ S_{t-1}}\ {\partial\ S_{t-1}\over\partial\ S_{t-2}}\ {\partial^+\ S_{t-2}\over\partial\ W}+\dots+{\partial\ S_t\over\partial\ S_{t-1}}\dots{\partial^+\ S_1\over\partial\ W}\\ \\
&amp;=\sum\limits^t_{k=1}{\partial\ S_t\over\partial\ S_k}\ {\partial^+\ S_k\over\partial\ W}\\ \\
\therefore\dfrac{\partial\ \mathcal{L^{&lt;t&gt;}}}{\partial\ W}&amp;=
\dfrac{\partial\ \mathcal{L^{&lt;t&gt;}}}{\partial\ S_t}\ 
\sum\limits^t_{k=1}{\partial\ S_t\over\partial\ S_k}\ {\partial^+\ S_k\over\partial\ W}
\end{align*}\]

<p>위와 같은 개념으로 BPTT 진행하여 RNN이 학습을 한다.</p>

<p>\(\begin{align*}\end{align*}\)
\(\begin{align*}\end{align*}\)</p>

<h3 id="참고자료">참고자료:</h3>
<h4 id="michigan-online"><a href="https://www.youtube.com/watch?v=dUzLD91Sj-o&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=12">Michigan Online</a>,</h4>
<h4 id="cs-230"><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">CS 230</a>,</h4>
<h4 id="wiki-docs"><a href="https://wikidocs.net/22886">Wiki Docs</a></h4>
<h4 id="nptel-noc-iitm"><a href="https://www.youtube.com/watch?v=Xeb6OjnVn8g">NPTEL-NOC IITM</a></h4>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;RNN Note&quot;]" /><summary type="html"><![CDATA[Architecture of a traditional RNN RNN에서는 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 cell(메모리 셀, RNN 셀) 이라고 한다.]]></summary></entry></feed>