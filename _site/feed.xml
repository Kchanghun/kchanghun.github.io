<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-08-25T13:37:15+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">No Free Knowledge</title><subtitle>There is No Free Knowledge when we study something.</subtitle><author><name>Chang Hun Kang</name></author><entry><title type="html">Hnadling long term dependencies</title><link href="http://localhost:4000/rnn%20note/2022/08/23/Handling-long-term-dependencies.html" rel="alternate" type="text/html" title="Hnadling long term dependencies" /><published>2022-08-23T21:27:13+09:00</published><updated>2022-08-23T21:27:13+09:00</updated><id>http://localhost:4000/rnn%20note/2022/08/23/Handling-long-term-dependencies</id><content type="html" xml:base="http://localhost:4000/rnn%20note/2022/08/23/Handling-long-term-dependencies.html"><![CDATA[<h1 id="vanilla-rnn의-한계">Vanilla RNN의 한계</h1>
<p>아래 Vanilla RNN 의 구조를 보자.<br />
각 timestep별로 결과에 얼마나 영향을 주는지 색으로 표현했을때<br />
색이 짙을 수록 역전파 과정에서 피드백 크기가 점점 작아져서 결과에 영향을 거의 안주게 된다.<br />
만약 앞에 입력이 결과에 영향을 줘야하는 경우라면 현재 timestep으로부터 먼 앞에 입력을 기억하고 있지 않다면<br />
<strong>Long Term Dependencies problem</strong> 이 발생하게 된다.</p>

<p align="center"><img style="width: 70%" src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_0.png" /></p>
<h5><center>출처: Wiki Docs</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="자주-사용되는-activation-functions">자주 사용되는 activation functions</h1>
<p>RNN에서 가장 많이 사용되는 activation functions들은 아래와 같다.</p>

<p align="center"><img src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_1.png" /></p>
<h5><center>출처: CS 230</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="vanishingexploding-gradient">Vanishing/exploding gradient</h1>
<p>RNN을 사용하다보면 Vanishing/exploding gradient 문제를 자주 만나게 된다.<br />
층의 수에 따라 기하급수적으로 커지거나 작아지는 기울기 때문에<br />
long term dependencies를 포착하기 어려워서<br />
Vanishing/exploding gradient 문제가 발생하게 된다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="gradient-clipping">Gradient clipping</h1>
<p>이 테크닉은 backpropagation 수행중에 가끔 마주치는<br />
exploding gradient 문제에 대응하기 위해 사용된다.</p>

<p align="center"><img style="width: 60%" src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_2.png" /></p>
<h5><center>출처: CS 230</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="grulstm">GRU/LSTM</h1>
<p>Gated Recurrent Unit(GRU)와 GRU의 일반화 버전인 Long Short-Term Memory units(LSTM)은 전통적인 RNNs를 통해 마주하는 vanishing gradient problem을 다루기 위해 사용한다.</p>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="types-of-gates">Types of Gates</h1>
<p><strong>vanishing gradient</strong> 문제를 해결하기 위해 RNNs에서는 목적이 잘 정의된 특정 gate들을 사용한다.<br />
그것들을 보통 $\Gamma$ 로 표기한다.</p>

\[\Gamma=\sigma\left(Wx^{&lt;t&gt;}+Ua^{&lt;t-1&gt;}+b\right)\]

<p>$W,U,b$ 는 gate의 고유한 계수들이고 $\sigma$ 는 sigmoid function이다.<br />
아래는 주요 내용이다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Type of gate</th>
      <th style="text-align: center">Role</th>
      <th style="text-align: center">Used in</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Update gate $\Gamma_u$</td>
      <td style="text-align: center">과거가 현재에 얼마나 영향을 주는가</td>
      <td style="text-align: center">GRU,LSTM</td>
    </tr>
    <tr>
      <td style="text-align: center">Relevance gate $\Gamma_r$</td>
      <td style="text-align: center">이전 정보를 버릴지</td>
      <td style="text-align: center">GRU,LSTM</td>
    </tr>
    <tr>
      <td style="text-align: center">Forget gate $\Gamma_f$</td>
      <td style="text-align: center">cell을 지울지 말지</td>
      <td style="text-align: center">LSTM</td>
    </tr>
    <tr>
      <td style="text-align: center">Output gate $\Gamma_o$</td>
      <td style="text-align: center">cell의 어느정도를 내보낼지</td>
      <td style="text-align: center">LSTM</td>
    </tr>
  </tbody>
</table>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="lstm">LSTM</h1>
<p>LSTM의 cell의 구조는 아래와 같다.</p>
<p align="center"><img src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_3.png" /></p>
<h5><center>출처 : Explain LSTM &amp; GRU</center></h5>

\[\begin{align*}\\
\text{Input gate}\quad\rightarrow\quad i_t&amp;=\sigma\left(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}\right)\\ \\
\text{Forget gate}\quad\rightarrow\quad f_t&amp;=\sigma\left(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf} \right)\\ \\
\text{Cell(Gate) gate}\quad\rightarrow\quad g_t&amp;=\tanh\left(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg} \right)\\ \\
\text{Output gate}\quad\rightarrow\quad o_t&amp;=\sigma\left(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho} \right)\\ \\
\text{Cell state}\quad\rightarrow\quad c_t&amp;=f_t\odot c_{t-1}+i_t\odot g_t\\ \\
\text{Hidden state}\quad\rightarrow\quad h_t&amp;=o_t\odot\tanh\left(c_t\right)
\end{align*}\]

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="gru">GRU</h1>
<p>GRU의 cell의 구조는 아래와 같다.</p>

<p align="center"><img src="../../../../assets/img/RNN%20Note/Handling_long_term_dependencies/handling_4.png" /></p>
<h5><center>출처 : Explain LSTM &amp; GRU</center></h5>

\[\begin{align*}\\ 
\text{Reset gate}\quad\rightarrow\quad r_t&amp;=\sigma\left(W_{ir}x_t+b_{ir}+W_{hr}h_{t-1}+b_{hr}\right) \\ \\
\text{Update gate}\quad\rightarrow\quad z_t&amp;=\sigma\left(W_{iz}x_t+b_{iz}+W_{hz}h_{t-1}+b_{hz}\right) \\ \\
\text{New gate}\quad\rightarrow\quad n_t&amp;=\tanh\left(W_{in}x_t+b_{in}+r_t*\left(W_{hn}h_{t-1}+b_{hn}\right)\right) \\ \\
\text{Hidden state}\quad\rightarrow\quad h_t&amp;=\left(1-z_t\right)*n_t+z_t*h_{t-1}
\end{align*}\]

<p>\(\begin{align*}\end{align*}\)
\(\begin{align*}\end{align*}\)</p>
<h3 id="참고자료">참고자료:</h3>
<h4 id="cs-230---deep-learning"><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">CS 230 - Deep Learning</a>,</h4>
<h4 id="wiki-docs"><a href="https://wikidocs.net/22888">Wiki Docs</a>,</h4>
<h4 id="mit-6s191-recurrent-neural-networks-and-transformers"><a href="https://www.youtube.com/watch?v=QvkQ1B3FBqA">MIT 6.S191: Recurrent Neural Networks and Transformers</a>,</h4>
<h4 id="explain-lstm--gru"><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Explain LSTM &amp; GRU</a>,</h4>
<h4 id="pytorch-lstm"><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM">pytorch LSTM</a>,</h4>
<h4 id="pytorch-gru"><a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU">pytorch GRU</a></h4>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;RNN Note&quot;]" /><summary type="html"><![CDATA[Vanilla RNN의 한계 아래 Vanilla RNN 의 구조를 보자. 각 timestep별로 결과에 얼마나 영향을 주는지 색으로 표현했을때 색이 짙을 수록 역전파 과정에서 피드백 크기가 점점 작아져서 결과에 영향을 거의 안주게 된다. 만약 앞에 입력이 결과에 영향을 줘야하는 경우라면 현재 timestep으로부터 먼 앞에 입력을 기억하고 있지 않다면 Long Term Dependencies problem 이 발생하게 된다.]]></summary></entry><entry><title type="html">RNN Overview</title><link href="http://localhost:4000/rnn%20note/2022/08/22/RNN.html" rel="alternate" type="text/html" title="RNN Overview" /><published>2022-08-22T15:16:15+09:00</published><updated>2022-08-22T15:16:15+09:00</updated><id>http://localhost:4000/rnn%20note/2022/08/22/RNN</id><content type="html" xml:base="http://localhost:4000/rnn%20note/2022/08/22/RNN.html"><![CDATA[<h1 id="architecture-of-a-traditional-rnn">Architecture of a traditional RNN</h1>
<p><strong>RNN</strong>에서는 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 <strong>cell</strong>(메모리 셀, RNN 셀) 이라고 한다.</p>

<p>regular feed-forward network에서 hidden layer라고 부르던 뉴런은</p>

<p>RNN에서 <strong>hidden state</strong>라고 부른다.<br />
\(\begin{align*}\end{align*}\)</p>

<p><strong>hidden state</strong>에서는 이전 <strong>time step</strong>의 <strong>hidden state</strong> 로부터 얻은 출력값을 현재 <strong>time step</strong>의 <strong>hidden state</strong> 에서 입력값으로 사용한다.</p>

<p><img src="/assets/img/RNN%20Note/RNN_overview/RNN_0.png" alt="RNN_0" /></p>
<h5><center>출처: CS 230</center></h5>

<p><strong>timestep</strong> $t$ 에서 activation \(a^{&lt;t&gt;}\)와 output \(y^{&lt;t&gt;}\)는 아래와 같이 표현한다.</p>

\[\begin{align*}
a^{&lt;t&gt;}&amp;=g_1(W_{aa}a^{&lt;t-1&gt;}+W_{ax}x^{&lt;t&gt;}+b_a)\\
y^{&lt;t&gt;}&amp;=g_2(W_{ya}a^{&lt;t&gt;}+b_y)
\end{align*}\]

<p>$W_{ax},\ W_{aa},\ W_{ya},\ b_a,\ b_y$는 모든 <strong>timestep</strong>에서 변하지 않는 계수이고<br />
$g_1,\ g_2$는 활성함수들이다.<br />
\(\begin{align*}\end{align*}\)</p>

<p><img src="/assets/img/RNN%20Note/RNN_overview/RNN_1.png" alt="RNN_1" /></p>
<h5><center>출처: CS 230</center></h5>

<h2 id="rnn의-장점과-단점">RNN의 장점과 단점</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Advantages</th>
      <th style="text-align: left">Drawbacks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$\cdot$ 어떤 길이의 입력도 처리 가능</td>
      <td style="text-align: left">$\cdot$ 계산이 느림</td>
    </tr>
    <tr>
      <td style="text-align: left">$\cdot$ 입력 크기에 따라 모델 크기가 커지지 않음</td>
      <td style="text-align: left">$\cdot$ 오래된 <strong>timestep</strong>에 접근하기 어려움</td>
    </tr>
    <tr>
      <td style="text-align: left">$\cdot$ 시간에 대한 결과를 고려한 계산</td>
      <td style="text-align: left">$\cdot$ 현재 state에서는 미래의 입력을 고려하지 못함</td>
    </tr>
    <tr>
      <td style="text-align: left">$\cdot$ 전체 시간동안 가중치가 공유됨</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="applications-of-rnns">Applications of RNNs</h1>
<p>RNN model들은 NLP와 speech recognition 처럼 sequential data를 다루기 위해 사용된다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Type of RNN</th>
      <th style="text-align: center">Illustration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">One-to-one                              </td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_2.png" alt="RNN_2" /></td>
    </tr>
    <tr>
      <td style="text-align: center">One-to-many</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_3.png" alt="RNN_3" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Many-to-one</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_4.png" alt="RNN_4" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Many-to-many</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_5.png" alt="RNN_5" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Many-to-many</td>
      <td style="text-align: center"><img src="/assets/img/RNN%20Note/RNN_overview/RNN_6.png" alt="RNN_6" /></td>
    </tr>
  </tbody>
</table>

<h5><center>출처: CS 230</center></h5>

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="loss-function">Loss function</h1>
<p>RNN의 경우 모든 timestep에서 loss function $\mathcal{L}$ 은 각 timestep 별 loss를 더하는 것으로 정의한다.</p>

\[\mathcal{L}(\hat y,y)=\sum\limits^{T_y}_{t=1}\mathcal{L}\left(\hat y^{&lt;t&gt;},y^{&lt;t&gt;}\right)\]

<p>\(\begin{align*}\end{align*}\)</p>
<h1 id="backpropagation-through-time">Backpropagation through time</h1>
<p>역전파는 각 시간별로 진행된다.<br />
timestep $T$ 인 경우, ${\partial\mathcal{L}^{(T)}\over\partial W}$ 는 아래와 같이 표현된다.</p>

\[{\partial\mathcal{L}^{(T)}\over\partial W}=\left.\sum\limits^{T}_{t=1}{\partial\mathcal{L}^{(T)}\over\partial W}\right|_{(t)}\]

<h2 id="how-to-do-bptt-">How to do <strong>BPTT</strong> ?</h2>
<p><img src="/assets/img/RNN%20Note/RNN_overview/RNN_7.png" alt="RNN_7" />
예를 들어 위와 같이 $S_t=W\cdot S_{t-1}$ 이고 $\mathcal{L}=W\cdot S_t$ 인 경우<br />
${\partial\ \mathcal{L}\over\partial\ W}$ 는 아래와 같이 구한다.</p>

\[\begin{align*}
{\partial\ \mathcal{L}\over\partial\ W}&amp;=
{\partial\ \mathcal{L}\over\partial\ S_t}\cdot{\partial\ S_t\over\partial\ W}\\ \\
{\partial\ S_t\over\partial\ W}&amp;=
\underbrace{\partial^+\ S_t\over\partial\ W}_{\mathsf{explicit}}+
\underbrace{\dfrac{\partial\ S_t}{\partial\ S_{t-1}}\cdot{\partial\ S_{t-1}\over\partial\ W}}_{\mathsf{implicit}}\\ \\
&amp;=
{\partial^+\ S_t\over\partial\ W}+{\partial\ S_t\over\partial\ S_{t-1}}\left[\underbrace{\partial^+\ S_{t-1}\over\partial\ W}_{\mathsf{explicit}}+\underbrace{\dfrac{\partial\ S_{t-1}}{\partial\ S_{t-2}}\cdot{\partial\ S_{t-2}\over\partial\ W}}_{\mathsf{implicit}}\right]=\dots\\ \\
&amp;=
{\partial\ S_t\over\partial\ S_t}\ {\partial^+\ S_t\over\partial\ W}+
{\partial\ S_t\over\partial\ S_{t-1}}\ {\partial^+\ S_{t-1}\over\partial\ W}+
{\partial\ S_t\over\partial\ S_{t-1}}\ {\partial\ S_{t-1}\over\partial\ S_{t-2}}\ {\partial^+\ S_{t-2}\over\partial\ W}+\dots+{\partial\ S_t\over\partial\ S_{t-1}}\dots{\partial^+\ S_1\over\partial\ W}\\ \\
&amp;=\sum\limits^t_{k=1}{\partial\ S_t\over\partial\ S_k}\ {\partial^+\ S_k\over\partial\ W}\\ \\
\therefore\dfrac{\partial\ \mathcal{L^{&lt;t&gt;}}}{\partial\ W}&amp;=
\dfrac{\partial\ \mathcal{L^{&lt;t&gt;}}}{\partial\ S_t}\ 
\sum\limits^t_{k=1}{\partial\ S_t\over\partial\ S_k}\ {\partial^+\ S_k\over\partial\ W}
\end{align*}\]

<p>위와 같은 개념으로 BPTT 진행하여 RNN이 학습을 한다.</p>

<p>\(\begin{align*}\end{align*}\)
\(\begin{align*}\end{align*}\)</p>

<h3 id="참고자료">참고자료:</h3>
<h4 id="michigan-online"><a href="https://www.youtube.com/watch?v=dUzLD91Sj-o&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=12">Michigan Online</a>,</h4>
<h4 id="cs-230"><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">CS 230</a>,</h4>
<h4 id="wiki-docs"><a href="https://wikidocs.net/22886">Wiki Docs</a></h4>
<h4 id="nptel-noc-iitm"><a href="https://www.youtube.com/watch?v=Xeb6OjnVn8g">NPTEL-NOC IITM</a></h4>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;RNN Note&quot;]" /><summary type="html"><![CDATA[Architecture of a traditional RNN RNN에서는 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 cell(메모리 셀, RNN 셀) 이라고 한다.]]></summary></entry><entry><title type="html">Vector Differential</title><link href="http://localhost:4000/math%20note/2022/08/21/%EB%B2%A1%ED%84%B0%EC%9D%98-%EB%AF%B8%EB%B6%84.html" rel="alternate" type="text/html" title="Vector Differential" /><published>2022-08-21T15:08:13+09:00</published><updated>2022-08-21T15:08:13+09:00</updated><id>http://localhost:4000/math%20note/2022/08/21/%EB%B2%A1%ED%84%B0%EC%9D%98%20%EB%AF%B8%EB%B6%84</id><content type="html" xml:base="http://localhost:4000/math%20note/2022/08/21/%EB%B2%A1%ED%84%B0%EC%9D%98-%EB%AF%B8%EB%B6%84.html"><![CDATA[<h1 id="vector-differential">Vector Differential</h1>

<p><img src="/assets/img/Math_Note/VectorDifferential/vector_diff_0.jpg" alt="vector_diff_0" /></p>

<p><img src="/assets/img/Math_Note/VectorDifferential/vector_diff_1.jpg" alt="vector_diff_1" /></p>

<p><img src="/assets/img/Math_Note/VectorDifferential/vector_diff_2.jpg" alt="vector_diff_2" /></p>

<p><img src="/assets/img/Math_Note/VectorDifferential/vector_diff_3.jpg" alt="vector_diff_3" /></p>

<h1 id="watch-this-very-helpful-video-from-michigan-online"><a href="https://www.youtube.com/watch?v=dB-u77Y5a6A">Watch this Very Helpful Video from Michigan Online</a></h1>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;Math Note&quot;]" /><summary type="html"><![CDATA[Vector Differential]]></summary></entry><entry><title type="html">Ch5 오차역전파법</title><link href="http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html" rel="alternate" type="text/html" title="Ch5 오차역전파법" /><published>2022-08-21T14:55:27+09:00</published><updated>2022-08-21T14:55:27+09:00</updated><id>http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation</id><content type="html" xml:base="http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html"><![CDATA[<h1 id="ch5-오차역전파법">Ch5 오차역전파법</h1>

<p>수치 미분을 통해 기울기를 구하는 방법은</p>

<p>구현이 쉽다는 장접이 있지만 시간이 오래 걸린다는 단점도 있다.</p>

<p>오차역전파법을 사용해 효율적으로 계산하겠다.</p>

<h1 id="51-계산-그래프">5.1 계산 그래프</h1>

<h2 id="511-계산-그래프로-풀다">5.1.1 계산 그래프로 풀다.</h2>

<p>흐름</p>

<ol>
  <li>계산 그래프를 구성한다.</li>
  <li>그래프에서 계산을 왼쪾에서 오른쪽으로 진행한다. (순전파$\mathsf{^{forward\ propagation}}$)</li>
</ol>

<h2 id="512-국소적-계산">5.1.2 국소적 계산</h2>

<p>계산 그래프의 특징은 ‘국소적 계산’을 전파하여 최종 결과를 얻는다.</p>

<p>따라서 전체 계산이 복잡하더라도 각 단계에서 하는 일은 해당 노드의 ‘국소적 계산’이기 때문에</p>

<p>계산이 간단하지만 그것들이 모여 복잡한 계산을 해낸다.</p>

<h2 id="513-왜-계산-그래프">5.1.3 왜 계산 그래프?</h2>

<ol>
  <li>국소적 계산을 통해 문제를 단순화할 수 있다.</li>
  <li>중간 계산 결과를 모두 보관할 수 있다.</li>
  <li>역전파( back propagation )를 통해 ‘미분’을 효율적으로 계산할 수 있다.</li>
</ol>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_0.jpg" alt="ch5_0" /></p>

<h1 id="52-연쇄법칙">5.2 연쇄법칙</h1>

<h2 id="521-계산-그래프의-역전파">5.2.1 계산 그래프의 역전파</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_1.jpg" alt="ch5_1" /></p>

<p>신호 E에 노드의 국소적 미분 $\partial y\over \partial x$를 곱한 후 다음 노드로 전달한다.</p>

<h2 id="522-연쇄법칙이란">5.2.2 연쇄법칙이란</h2>

<p>합성 함수 : 여러 함수로 구성된 함수.</p>

\[z=(x+y)^2\\
\downarrow\\
z=t^2\\
t=x+y\]

<p>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</p>

\[{\partial z\over \partial x}={\partial z\over \partial t}{\partial t\over \partial x}\]

\[{\partial z\over \partial x}={\partial z\over \cancel{\partial t}}{\cancel{\partial t}\over \partial x}\]

<ol>
  <li>국소적 미분(편미분) 계산</li>
</ol>

\[{\partial z\over \partial t}=2t\]

\[{\partial t\over \partial x}=1\]

<ol>
  <li>연쇄법칙$\mathsf{^{chain rule}}$ 적용</li>
</ol>

\[{\partial z\over \partial x}={\partial z\over \partial t}{\partial t\over \partial x}=
2t\cdot1=2(x+y)\]

<h2 id="523-연쇄법칙과-계산-그래프">5.2.3 연쇄법칙과 계산 그래프</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_2.jpg" alt="ch5_2" /></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_3.jpg" alt="ch5_3" /></p>

<p>역전파 방향으로 국소적 미분을 곱하면 입력에 대한 출력의 미분값을 계산한것과 같다.</p>

<h1 id="53-역전파">5.3 역전파</h1>

<h2 id="531-덧셈-노드의-역전파">5.3.1 덧셈 노드의 역전파</h2>

\[z=x+y\]

\[{\partial z\over\partial x}=1\]

\[{\partial z\over\partial y}=1\]

<p>x, y : 덧셈노드의 입력</p>

<p>z : 덧셈노드의 출력</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_4.jpeg" alt="ch5_4" /></p>

<p>앞에 임의의 계산이 있더라도 연쇄법칙에 의해
덧셈노드의 상류에서 흘러온 미분 값은 $\partial L\over\partial z$가 된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_5.jpeg" alt="ch5_5" /></p>

<h2 id="532-곱셈-노드의-역전파">5.3.2 곱셈 노드의 역전파</h2>

\[z=xy\]

\[{\partial z\over\partial x}=y\]

\[{\partial z\over\partial y}=x\]

<p>곱셈노드의 각 입력에 의한 미분 값은 자신이 아닌 다른 입력값</p>

<p>따라서 입력 x에 의한 출력의 미분 값은 ${\partial L\over\partial z}\cdot y$</p>

<p>입력y에 의한 출력의 미분 값은 ${\partial L\over\partial z}\cdot x$</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_6.jpeg" alt="ch5_6" /></p>

<h2 id="533-사과-쇼핑의-예">5.3.3 사과 쇼핑의 예</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_7.jpeg" alt="ch5_7" /></p>

<h1 id="54-단순한-계층-구현하기">5.4 단순한 계층 구현하기</h1>

<p>계산 그래프를 파이썬으로 구현해보는 절</p>

<h2 id="541-곱셈-계층">5.4.1 곱셈 계층</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MulLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span>
        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span>
        
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</code></pre></div></div>

<h2 id="542-덧셈-계층">5.4.2 덧셈 계층</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span><span class="o">*</span><span class="mi">1</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span><span class="o">*</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</code></pre></div></div>

<h1 id="55-활성화-함수-계층-구현하기">5.5 활성화 함수 계층 구현하기</h1>

<p>활성함수 ReLu와 sigmoid 계층 구현</p>

<h2 id="551-relu-계층">5.5.1 ReLu 계층</h2>

\[y=\begin{cases}
x &amp; (x&gt;0)\\
0 &amp; (x\le0)
\end{cases}\]

\[{\partial y\over\partial x}=\begin{cases}
1 &amp; (x&gt;0)\\
0 &amp; (x\le0)
\end{cases}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_8.jpeg" alt="ch5_8" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReLu</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dout</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<p>ReLu 계층은 전기 회로의 ‘스위치’에 비유할 수 있다.</p>

<p>순전파 때 전류가 흐르고 있으면$(x&gt;0)$ 스위치를 ON으로 하고, 흐르지 않으면$(x\le0)$ OFF로 한다.</p>

<p>역전파 때는 스위치가 ON이라면 전류가 그대로 흐르고, OFF면 더 이상 흐르지 않는다.</p>

<h2 id="552-sigmoid-계층">5.5.2 Sigmoid 계층</h2>

\[y={1\over1+e^{-x}}\]

<p>아래 sigmoid 함수의 계산 그래프를 보면</p>

<p>덧셈 노드, 곱셈 노드, ‘exp’ 노드와 ‘ / ’ 노드가 사용된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_9.jpeg" alt="ch5_9" /></p>

<h3 id="sigmoid-함수의-역전파-과정">sigmoid 함수의 역전파 과정</h3>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_10.png" alt="ch5_10" /></p>

<h3 id="step-1-결과---partial-loverpartial-ycdot-y2">step 1 결과 : $-{\partial L\over\partial y}\cdot y^2$</h3>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;={1\over x}\\
{\partial y\over\partial x}&amp;=-{1\over\ x^2}\\
&amp;=-y^2
\end{align*}\]

<h3 id="step-2-결과---partial-loverpartial-ycdot-y2">step 2 결과 : $-{\partial L\over\partial y}\cdot y^2$</h3>

<p>덧셈 노드는 상류의 값을 여과 없이 하류로 내보낸다.</p>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;=x+1\\
{\partial y\over\partial x}&amp;=1
\end{align*}\]

<h3 id="step-3-결과---partial-loverpartial-ycdot-y2cdot-e-x">step 3 결과 : $-{\partial L\over\partial y}\cdot y^2\cdot e^{-x}$</h3>

<p>exp 노드의 편미분값은 원래 exp와 같다.</p>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;=e^x\\
{\partial y\over\partial x}&amp;=e^x
\end{align*}\]

<p>주의할 점은 step 3에서 exp노드의 입력값이 $e^{-x}$이므로</p>

<p>역전파 계산시 exp노드를 통과하면 국소적 미분값으로 $e^{-x}$를 곱해야 한다.</p>

<h3 id="step-4-결과--partial-loverpartial-ycdot-y2cdot-e-x">step 4 결과 : ${\partial L\over\partial y}\cdot y^2\cdot e^{-x}$</h3>

<p>곱셈 노드의 역전파 계산은 순전파 때의 값을 서로 바꿔 곱한다.</p>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;=x\cdot c\\
{\partial y\over\partial x}&amp;=c
\end{align*}\]

<h3 id="최종-결과">최종 결과</h3>

\[\begin{align*}
{\partial L\over\partial y}\cdot y^2\cdot e^{-x}=&amp;{\partial L\over\partial y}{1\over (1+e^{-x})^2}e^{-x}\\
=&amp;{\partial L\over\partial y}{1\over1+e^{-x}}{e^{-x}\over1+e^{-x}}\\
=&amp;{\partial L\over\partial y}\cdot y(1-y)\\
&amp;(\because y={1\over1+e^{-x}})
\end{align*}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_11.jpeg" alt="ch5_11" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h1 id="56-affinesoftmax-계층-구현">5.6 Affine/Softmax 계층 구현</h1>

<h2 id="벡터의-미분"><a href="http://kchanghun.github.io/math%20note/2022/08/21/벡터의-미분.html">*벡터의 미분</a></h2>

<h2 id="561-affine-계층">5.6.1 Affine 계층</h2>

<p>신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 어파인 변환$\mathsf{^{affine\ transformation}}$이라고 한다.</p>

<p>Affine 계층의 계산 그래프 : 변수가 행렬임에 주의, 각 변수의 형상을 변수명 위에 표기했다.</p>

<p>이제부터는 노드 사이에 벡터와 행렬도 흐른다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_12.png" alt="ch5_12" /></p>

<ol>
  <li></li>
</ol>

\[\begin{align*}{\partial L\over\partial X}=&amp;{\partial L\over\partial Y}{\partial Y\over\partial X}\\{\partial Y\over\partial X}=&amp;{\partial (X\cdot W+B)\over\partial X}={\partial (X^TW)\over\partial X}=W^T\\&amp;(\ \because Y=X\cdot W+B,\ X\mathsf{\ is\ vector,\ }W\ \mathsf{is\ matrix\ },\ B\ \mathsf{is\ constant\ })\\\therefore {\partial L\over\partial X}=&amp;{\partial L\over\partial Y}\cdot W^T\end{align*}\]

<ol>
  <li></li>
</ol>

\[\begin{align*}{\partial L\over\partial X}=&amp;
{\partial L\over\partial Y}
{\partial (X\cdot W)\over\partial X}={\partial (X^TW)\over\partial X}=
W^T\\&amp;
(\ \because Y=X\cdot W+B,\ X\mathsf{\ is\ vector,\ }W\ \mathsf{is\ matrix\ },\ B\ \mathsf{is\ constant\ })\\\therefore {\partial L\over\partial X}=&amp;{\partial L\over\partial Y}\cdot W^T\end{align*}\]

<p>dot노드의 역전파는 곱셈 노드의 역전파와 개념이 같은데 계산되는 변수가 다차원 배열이기 때문에 서로 도트곱을 할 수 있도록 차원을 맞춰줘야 한다.</p>

\[\begin{align*}
&amp;{\partial L\over\partial X}=&amp;
{\partial L\over\partial Y}\cdot &amp;W^T\\
&amp;(2,)&amp;(3,)\cdot&amp;(3,2)\\&amp;\\
&amp;{\partial L\over\partial W}=&amp;
X^T\cdot&amp;{\partial L\over\partial Y}\\
&amp;(2,3)&amp;(2,1)\cdot&amp;(,3)
\end{align*}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_13.jpg" alt="ch5_13" /></p>

<h2 id="562-배치용-affine-계층">5.6.2 배치용 Affine 계층</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_14.jpg" alt="ch5_14" /></p>

<p>순전파의 편향 덧셈은 각각의 데이터( 1 번째 데이터, 2번째 데이터, …)에 더해진다.</p>

<p>그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Affine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">original_x_shape</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">original_x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">original_x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h2 id="563-softmax-with-loss-layer">5.6.3 Softmax-with-Loss Layer</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_15.jpg" alt="ch5_15" /></p>

<p>손글씨 숫자 인식에서의 softmax 계층의 출력의 모습인데</p>

<p>0~9까지 10개의 숫자를 분류하기 때문에 입력도 10개고 출력도 10개다</p>

<p>softmax계층을 cross entropy error계층과 함께 구현한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_16.png" alt="ch5_16" /></p>

<h2 id="forward-propagation">Forward Propagation</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_17.png" alt="ch5_17" /></p>

<h3 id="1-softmax">1. Softmax</h3>

\[\begin{align*}
y_k&amp;={e^{a_k}\over\sum\limits_i^ne^{a_i}}\\
S&amp;=e^{a_1}+e^{a_2}+e^{a_3}\\
y_1&amp;={e^{a_1}\over S}\quad y_2={e^{a_2}\over S}\quad y_3={e^{a_3}\over S}
\end{align*}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_18.png" alt="ch5_18" /></p>

<h3 id="2-cross-entropy-error">2. Cross Entropy Error</h3>

\[L=-\sum\limits_kt_k\log(y_k)\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_19.png" alt="ch5_19" /></p>

<h2 id="back-propagation">Back Propagation</h2>

<h3 id="1-cross-entropy-error">1. Cross Entropy Error</h3>

\[y=\log x\\
{\partial y\over\partial x}={1\over x}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_20.png" alt="ch5_20" /></p>

<h3 id="2-softmax">2. Softmax</h3>

<p>step 1</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_21.png" alt="ch5_21" /></p>

<p>step 2</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_22.png" alt="ch5_22" /></p>

<p>step 3</p>

<p>순전파 때 여러 갈래로 나뉘어 흘렸다면 역전파 때는 그 반대로 흘러온 여러 값을 더한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_23.png" alt="ch5_23" /></p>

<p>step 4</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_24.png" alt="ch5_24" /></p>

<p>step 5</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_25.png" alt="ch5_25" /></p>

<p>step 6</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_26.png" alt="ch5_26" /></p>

<p>결과</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_27.png" alt="ch5_27" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SoftmaxWithLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">t</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h1 id="57-오차역전파법-구현하기">5.7 오차역전파법 구현하기</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>\
                    <span class="n">weight_init_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> \
                            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> \
                            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Relu1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ReLu</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span> <span class="o">=</span> <span class="n">SoftmaxWithLoss</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">accuracy</span>
    
    <span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">loss_W</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">grads</span>
    
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">dout</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">dout</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
        
        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">layers</span><span class="p">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">dout</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
            
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">].</span><span class="n">dW</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">].</span><span class="n">db</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">].</span><span class="n">dW</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">].</span><span class="n">db</span>
        
        <span class="k">return</span> <span class="n">grads</span>
</code></pre></div></div>

<h2 id="573-오차역전파법으로-구한-기울기-검증하기">5.7.3 오차역전파법으로 구한 기울기 검증하기</h2>

<p>기울기를 구하는 방법으로</p>

<ol>
  <li>수치 미분</li>
  <li>해석적 미준</li>
</ol>

<p>두가지를 알았다.</p>

<p>계산 그래프를 통해 해석적 미분을 계산하면서</p>

<p>느린 수치 미분보다 효율적 결과를 얻을 수 있다.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>수치 미분</th>
      <th>해석적 미분</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>속도</td>
      <td>느림</td>
      <td>빠름</td>
    </tr>
    <tr>
      <td>구현 난이도</td>
      <td>쉬움</td>
      <td>어려움</td>
    </tr>
  </tbody>
</table>

<p>때문에 수치 미분 값과 오차역전파법의 결과를 비교하여 제대로 구현 되었는지 확인한다. → 기울기 확인$\mathsf{^{gradient\ check}}$</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;DeepLearning from scratch&quot;]" /><summary type="html"><![CDATA[Ch5 오차역전파법]]></summary></entry><entry><title type="html">Ch4 신경망 학습</title><link href="http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch4-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5.html" rel="alternate" type="text/html" title="Ch4 신경망 학습" /><published>2022-08-21T01:10:34+09:00</published><updated>2022-08-21T01:10:34+09:00</updated><id>http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch4-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5</id><content type="html" xml:base="http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch4-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5.html"><![CDATA[<h1 id="ch4-신경망-학습">Ch4 신경망 학습</h1>

<h3 id="학습">학습</h3>

<p>훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻함</p>

<p>신경망이 학습하는 것을 나타내는 지표로 손실 함수를 사용한다.</p>

<p>이 때 손실 함수의 값이 작을수록 학습이 잘 된것이라 한다.</p>

<h1 id="41-데이터에서-학습한다">4.1 데이터에서 학습한다.</h1>

<p>신경망의 특징은 데이터를 보고 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 것이다.</p>

<h2 id="411-데이터-주도-학습">4.1.1 데이터 주도 학습</h2>

<p>학습 파이프라인의 전환</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch4/ch4_0.png" alt="ch4_0" /></p>

<ol>
  <li>첫 번째 방법에서는 알고리즘을 만들어내기 매우 어려움</li>
  <li>두 번째 기계학습을 통해 효율을 높였지만 여전히 사람이 특징을 적절하게 뽑아야함</li>
  <li>
    <p>세 번째 방법에서는 신경망이 직접 데이터를 통해 학습한다.</p>

    <p>따라서 숫자5를 인식하는 문제든 강아지를 인식하는 문제든 사람의 얼굴을 인식하는 문제든</p>

    <p>사람의 개입없이 문제를 해결할 수 있음 ( end-to-end machine learning )</p>
  </li>
</ol>

<h2 id="412-훈련-데이터와-시험-데이터">4.1.2 훈련 데이터와 시험 데이터</h2>

<p>학습에 사용하는 데이터는 훈련 데이터(training data)와 시험 데이터(test data)로 나뉜다.</p>

<p>좋은 모델은 새로운 데이터로도 문제를 올바르게 풀어내는 능력이 중요하기 때문에</p>

<p>모델을 평가할 때는 학습에 사용된 훈련 데이터말고 훈련에 사용되지 않은 시험 데이터로 평가한다.</p>

<p>갖고 있는 모든 데이터를 학습 데이터로 사용하면 데이터셋에 Overfitting이 일어나도 확인 할 수 없다.</p>

<h1 id="42-손실-함수--loss-function-">4.2 손실 함수 ( Loss Function )</h1>

<p>신경망의 성능을 나타내는 지표를 손실 함수라 하고</p>

<p>오차제곱합( Sum of Squares for error, SSE )과 교차 엔트로피 오차( Cross Entropy Error, CEE )가 일반적</p>

<h2 id="421-오차제곱합--sse-">4.2.1 오차제곱합 ( SSE )</h2>

\[\begin{align*}E=&amp;\frac{1}{2}\sum\limits_k(y_k-t_k)^2\\
y_k\ &amp;:\ \mathsf{output}\\
t_k\ &amp;:\ \mathsf{answer\ \ label}\\
k\ &amp;:\ \mathsf{dimension\ \ of\ \ data}\end{align*}\]

<h3 id="one-hot-encoding">*one-hot encoding</h3>

<p>한 원소만 1로하고 그 외는 0으로 나타내는 표기법</p>

<h2 id="422-교차-엔트로피-오차--cee-">4.2.2 교차 엔트로피 오차 ( CEE )</h2>

\[\begin{align*}E=&amp;-\sum\limits_kt_k\log(y_k)\\
y_k\ &amp;:\ \mathsf{output}\\
t_k\ &amp;:\ \mathsf{answer\ \ label\ (one-hot\ \ encoding)}\\
k\ &amp;:\ \mathsf{dimension\ \ of\ \ data}\end{align*}\]

<h2 id="423-미니배치-학습">4.2.3 미니배치 학습</h2>

<p>기계학습은 훈련 데이터를 사용해 학습한다.</p>

<p>훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.</p>

<p>따라서 모든 훈련 데이터를 대상으로 오차를 구하고 그 합을 지표로 삼는다.</p>

<p>( 훈련 데이터가 1000개면 1000번의 손실 함수를 실행해야함 )</p>

<p>빅데이터 수준에서는 데이터의 수가 수백만개 수천만개가 넘기 때문에</p>

<p>데이터 전체에 대한 손실 함수를 계산하기 어렵기 때문에 일부(미니배치$\mathsf{^{mini-batch}}$)만 골라서 학습.</p>

<p>이렇게 학습하는 방법을 <strong>미니배치 학습</strong> 이라고 한다.</p>

<h2 id="424-배치용-교차-엔트로피-오차">4.2.4 (배치용) 교차 엔트로피 오차</h2>

<h3 id="평균-손실-함수">평균 손실 함수</h3>

\[\begin{align*}
E&amp;=-\frac{1}{N}\sum\limits_n\sum\limits_kt_{nk}\log y_{nk}\\
\mathsf{N}\ &amp;:\ \mathsf{the\ \ number\ \ of\ \ Data}\\
t_{nk}\ &amp;:\mathsf{\ k_{th}\ answer\ of \ \ n_{th}\ \ data}\\
y_{nk}\ &amp;:\mathsf{\ k_{th}\ output\ of\ \ n_{th}\ \ data}
\end{align*}\]

<p>N으로 나눠서 정규화하면</p>

<ol>
  <li>범위를 0~1로 조절</li>
  <li>훈련 데이터의 개수와 관계없이 통일된 지표를 얻음</li>
</ol>

<h2 id="425-왜-손실-함수를-설정하는가">4.2.5 왜 손실 함수를 설정하는가?</h2>

<p>신경망 학습의 궁극적인 목표는 높은 정확도 이지만</p>

<p>학습 방법으로 정확도를 지표로 사용하지 않고 손실 함수를 사용한다.</p>

<p>학습을 통해 신경망에 사용되는 매개변수를 조절할 때 미분 값을 사용하는데</p>

<p>정확도를 지표로 삼는 함수의 미분 값은 대부분의 장소에서 0이되어 매개변수 조절이 불가능하기 때문이다.</p>

<p>또한, 정확도는 매개변수의 변화에 의해 값이 이산적으로 변하는데</p>

<p>그것은 매개변수의 변화가 주는 변화를 정확도가 무시하고 있다는 것입니다.</p>

<p>반면 손실 함수는 미분 값이 0인 경우 학습이 종료되고 매개변수의 변화에 의해 손실 함숫값은 연속적으로 변해</p>

<p>매개변수 변화에 민감하게 반응해 최적의 매개변수를 구하기 좋다.</p>

<p>같은 맥락에서 step 함수를 활성 함수로 쓰지 않는다.</p>

<p>step 함수는 대부분의 장소에서 미분 값이 0이고 그 함숫값이 이산적이어서</p>

<p>모델의 학습 지표를 손실 함수로 삼는다 하더라도 학습이 되지 않기 때문이다.</p>

<h1 id="43-수치-미분">4.3 수치 미분</h1>

<p>경사법에서는 기울기(미분 값)을 기준으로 매개변수를 조절한다.</p>

<h2 id="431-미분">4.3.1 미분</h2>

<p>미분이란 한순간의 변화량</p>

<h3 id="전방-차분--forward-difference-">전방 차분 ( Forward Difference )</h3>

\[\begin{align*}
\dfrac{d\ f(x)}{dx}=\lim_{h\rightarrow 0}\dfrac{f(x+h)-f(x)}{h}
\end{align*}\]

<p>위와 같은 미분 방법을 전방 차분이라 하고 수치 미분법 중 하나이다.</p>

<p>또 다른 수치 미분법으로는</p>

<h3 id="중앙-차분-central-difference-">중앙 차분( Central Difference )</h3>

\[\begin{align*}
\dfrac{d\ f(x)}{dx}=\lim_{h\rightarrow 0}\dfrac{f(x+h)-f(x-h)}{2h}
\end{align*}\]

<h3 id="후방-차분--backward-difference-">후방 차분 ( Backward Difference )</h3>

\[\begin{align*}
\dfrac{d\ f(x)}{dx}=\lim_{h\rightarrow 0}\dfrac{f(x)-f(x-h)}{h}
\end{align*}\]

<p>사람이 직접 수학 문제를 풀 때 사용하는  전개해서 미분하는 방법을 해석적 미분이라고 하는데</p>

\[\begin{align*}
y=x^2
\\{dy \over dx}=2x
\end{align*}\]

<p>이런 해석적 미분은 전개식을 통해 미분하는데 프로그래밍 할 때는</p>

<ol>
  <li>함수를 정의했을 경우 그 함수의 전개식을 따로 저장해둬야 한다. (ex&gt; string)</li>
</ol>

<p>그래도 미분을 위해 전개식을 string으로 저장했다 하더라도 정규 문법을 정하기 어렵다.</p>

<ol>
  <li>다변수 함수까지 처리하려면 정규 문법을 정하는 것도 쉽지않다.</li>
</ol>

<p>때문에 오차 값을 감안하고 프로그래밍 할 때는 수치 미분을 한다.</p>

<p>해석적 미분을 하면 참값이 나오지만 수치 미분을 하면 오차가 생길 수밖에 없다.</p>

<p>그래서 그 오차를 최대한 줄이기 위해서는 차분의 간격을 최대한 좁히는 것이다.</p>

<p>그렇게해서 구현을 하게 되면 아래와 같은데 먼저 잘못된 예를 보면</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bad_numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
	<span class="n">h</span> <span class="o">=</span> <span class="mf">1e-50</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">h</span>
</code></pre></div></div>

<p>차분의 간격을 줄이려고 h값을 너무 작게 설정하면</p>

<p>python이 그 값을 반올림을 해서 그 값이 무시되는(0으로 되는) 반올림 오차$\mathsf{^{rounding\ error}}$가 발생한다.</p>

<p>따라서 h값은 $10^{-4}$정도의 값으로 사용한다.</p>

<p>그리고 차분은 두 점의 기울기 값 이기고 3개의 방법 중 중앙 차분을 이용해 구현해 보면 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">central_numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
	<span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># 0.0001
</span>	<span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="432-수치-미분의-예">4.3.2 수치 미분의 예</h2>

\[y=0.01x^2+0.1x\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">function_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">x</span>
</code></pre></div></div>

<h2 id="433-편미분">4.3.3 편미분</h2>

\[f(x_0,\ x_1)=x^2_0+x^2_1\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">function_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
	<span class="c1"># or return np.sum(x**2)
</span></code></pre></div></div>

\[{\partial f\over\partial x_0}=2x_0\ \ \ \ \ \ 
{\partial f\over\partial x_1}=2x_1\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># x0=3, x1=4 일 때, x0에 대한 편미분
</span><span class="k">def</span> <span class="nf">function_tmp1</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">x0</span><span class="o">*</span><span class="n">x0</span> <span class="o">+</span> <span class="mf">4.0</span><span class="o">**</span><span class="mf">2.0</span>

<span class="c1"># x1에 대한 편미분
</span><span class="k">def</span> <span class="nf">function_tmp2</span><span class="p">(</span><span class="n">x1</span><span class="p">):</span>
	<span class="k">return</span> <span class="mf">3.0</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">+</span> <span class="n">x1</span><span class="o">*</span><span class="n">x1</span>
</code></pre></div></div>

<p>편미분 하려는 변수를 제외한 나머지 변수는 고정값을 대입하여</p>

<p>새로운 함수를 정의한다.</p>

<h1 id="44-기울기">4.4 기울기</h1>

<p>한 함수를 통해 모든 변수에 대한 편미분 결과를 벡터로 정리한 것을 기울기$\mathsf{^{gradient}}$라고 한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
	<span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
	<span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	
	<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
		<span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
		
		<span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">+</span> <span class="n">h</span>
		<span class="n">fxh1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

		<span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span>
		<span class="n">fxh2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

		<span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
		<span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span>

	<span class="k">return</span> <span class="n">grad</span>
</code></pre></div></div>

<p>기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch4/ch4_1.png" alt="ch4_1" /></p>

<p>*잘못된 함수 구현</p>

<p>결과의 차이는 없지만 제대로 구하는 건 아니라 생각</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch4/ch4_2.png" alt="ch4_2" /></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch4/ch4_3.png" alt="ch4_3" /></p>

<h2 id="441-경사법-gradient-method-">4.4.1 경사법( Gradient method )</h2>

<p>광대한 매개변수 공간에서 어디가 손실함수를 최솟값으로 만드는 곳인지</p>

<p>기울기를 이용여 찾는 방법</p>

<p>기울기를 지표로 최적의 매개변수를 찾으러 움직이지만</p>

<p>기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지는 보장되지 않는다.</p>

<p>함수의 기울기가 0인 지점은 최솟값, 극솟값, 안장점이 될 수 있다.</p>

<p>복잡하고 찌그러진 모양의 함수라면 평평한 곳으로 파고들면서 고원$\mathsf{^{plateau,플라토}}$라 하는 학습 정체기에 빠질 수 있다.</p>

<p>기울기의 방향이 반드시 최솟값을 가리키지는 않지만</p>

<p>그 방향으로 가야 함수의 값을 줄일 수 있다.</p>

<p>경사법에는 경사 하강법$\mathsf{^{gradient\ descent\ method}}$과 경사 상승법$\mathsf{^{gradient\ ascent\ method}}$이 있는데</p>

<p>기울기가 가리키는 방향으로 일정 거리만큼 이동하면서 함수의 값을 점차 줄여나가는 방법을 경사 하강법이라 한다.</p>

\[\begin{align*}
x_0&amp;=x_0-\eta{\partial f\over\partial x_0}\\
x_1&amp;=x_1-\eta{\partial f\over\partial x_1}\\
\eta&amp;\ :\ \mathsf{learning\ rate}
\end{align*}\]

<p>에타는 학습률을 뜻하는데 기울기 방향으로 이동할 거리를 조절하고</p>

<p>에타가 너무 크면 빠른 학습을 하지만 최솟값에 수렴하지 못할 수도 있고</p>

<p>에타가 너무 작으면 학습 속도가 너무 느려 시간 비용이 크게 들어가서</p>

<p>조절하면서 학습을 진행한다.</p>

<h3 id="경사-하강법">경사 하강법</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_num</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">grad</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>f : 최적화 하려는 함수</p>

<p>init_x : 학습을 시작할 위치</p>

<p>step_num : 경사법 반복 수</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch4/ch4_4.png" alt="ch4_4" /></p>

<p>학습률 같은 매개변수를 하이퍼파라미터$\mathsf{^{hyper\ parameter,\ 초매개변수}}$라고 한다.</p>

<p>학습 과정에서 스스로 값이 설정되는 매개변수인 가중치와 편향과 달리</p>

<p>하이퍼파라미터는 사람이 직접 조절하면서 잘 맞는 값을 찾아야 한다.</p>

<h2 id="442-신경망에서의-기울기">4.4.2 신경망에서의 기울기</h2>

<p>가중치 $W$, 손실함수 $L$인 신경망에서</p>

\[\begin{align*}
W&amp;=\begin{pmatrix}
w_{11}&amp;w_{12}&amp;w_{13}\\
w_{21}&amp;w_{22}&amp;w_{23}
\end{pmatrix}\\
{\partial L\over\partial W}&amp;=
\begin{pmatrix}
{\partial L\over\partial w_{11}}&amp;{\partial L\over\partial w_{12}}&amp;{\partial L\over\partial w_{13}}\\{\partial L\over\partial w_{21}}&amp;{\partial L\over\partial w_{22}}&amp;{\partial L\over\partial w_{23}}
\end{pmatrix}
\end{align*}\]

<p>${\partial L\over\partial w_{11}}$는 $w_{11}$의 값을 변경했을 때 손실 함수 $L$이 얼마나 변화하는지를 나타낸다.</p>

<h1 id="45-학습-알고리즘-구현하기">4.5 학습 알고리즘 구현하기</h1>

<p>전체</p>

<p>신경망에는 적응 가능한 가중치와 편향이 있고</p>

<p>이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 ‘학습’이라고 한다.</p>

<p>신경망 학습은 4단계로 수행된다.</p>

<p>1단계 - 미니배치</p>

<p>훈련 데이터 중 일부를 무작위로 가져온다.</p>

<p>이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수를 줄이는것이 목표다.</p>

<p>2단계 - 기울기 산출</p>

<p>미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.</p>

<p>기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.</p>

<p>3단계 - 매개변수 갱신</p>

<p>가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.</p>

<p>4단계 - 반복</p>

<p>1~3단계를 반복한다.</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;DeepLearning from scratch&quot;]" /><summary type="html"><![CDATA[Ch4 신경망 학습]]></summary></entry><entry><title type="html">ResNet</title><link href="http://localhost:4000/paper%20review/2022/08/20/ResNet.html" rel="alternate" type="text/html" title="ResNet" /><published>2022-08-20T03:37:25+09:00</published><updated>2022-08-20T03:37:25+09:00</updated><id>http://localhost:4000/paper%20review/2022/08/20/ResNet</id><content type="html" xml:base="http://localhost:4000/paper%20review/2022/08/20/ResNet.html"><![CDATA[<h1 id="resnet">ResNet</h1>

<h1 id="deep-redisual-learning-for-image-recognition">Deep Redisual Learning for Image Recognition</h1>

<h1 id="1-introduction">1. Introduction</h1>

<p>깊은 CNN들은 이미지 분류의 돌파구를 만든다.</p>

<p>깊은 네트워크들은 end-to-end 다층 구조에서 low/mid/high 수준의 특징과 분류기들을 통합하는 본성이 있고
층이 쌓일수록(깊이가 깊어질수록) 특징의”단계”는 풍부해진다.</p>

<p>최근 밝혀진 바에 의하면 네트워크의 깊이는 아주 중요하고 까다로운 ImageNet 데이터셋의 선두 결과들은 모두 “매우 깊은” 모델들(16~30 깊이의 모델)을 활용한다고 한다.</p>

<p>다른 많은 nontrivial visual recognition 작업들도 매우 깊은 모델들로부터 좋은 결과를 얻을 수 있다.</p>

<p>깊이의 중요성으로부터 생긴 질문 : 
층을 더 많이 쌓는다고 네트워크가 더 좋은 학습을 하는가?</p>

<p>이 질문을 해결할 때 만나는 장애물은 유명한 문제인 vanishing/exploding gradients(처음부터 수렴을 방해하는 문제)이다.</p>

<p>그러나, normalize된 초기값과 네트워크 중간에 normalization 층을 끼워넣는 방법으로
(수십개의 층을 가진 네트워크가 역전파 방법을 통해 SGD optimizer로 수렴을 시작할 수 있도록 하는 방법)
이 문제는 크게 해결이 되어왔다.</p>

<p>깊은 네트워크들이 수렴하기 시작할 때,
degradation 문제가 생긴다 : 
네트워크의 깊이가 증가하면서 정확도가 (당연하게도)포화되는것인데
그렇게 되면 점점 정확도가 낮아진다.</p>

<p>뜻밖에도, 이러한 degradation은 과적합에 의한 문제가 아니고
적당히 깊은 모델이 더 깊어지도록 층을 추가하는 행위가 높은 training error 를 갖게하는 것이다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet0.png" alt="ResNet0" /></p>

<p>(학습 정확도의) degradation은 모든 시스템들이 optimize하기 쉬운건 아니라는 것을 보여준다.</p>

<p>우리는 얕은 구조와 그것을 복사하고 층을 몇개 추가한 모델을 고려했다.</p>

<p>구조에 있어서 깊은 모델에 solution이 존재한다 :
추가된 층들은 identity mapping이고
다른 층들은 학습된 얕은 모델을 복사하는 것이다.</p>

<p>이 구성으로 얻은 해결책에 의해
더 깊은 모델이 얕은 것보다 더 높은 훈련 오류를 발생해서는 안 된다는 것을 나타낸다.</p>

<p>하지만 실험에서 보여지듯 현재로써
이 구조로 인한 해결책과 비교해서 더 좋은 해결책은 찾지 못했다.</p>

<p>이 논문에서, degradation문제를 해결하기 위해 ‘deep residual learning framework’를 소개할 것이다.</p>

<p>쌓인 층들이 각각 직접 바라는 층과 mapping 되는것을 바라기 보다, 이 층들이 residual mapping에 fit 되도록 했다.</p>

<p>공식적으로, desired underlying mapping을 $H(x)$라고 하고, 비 선형 층들을 쌓은 묶음이 $F(x):=H(x)-x$와 mapping되게 했다.</p>

<p>원래 mapping은 $F(x)+x$로 변환된다.</p>

<p>residual mapping을 optimize하는 것이
original mapping보다 optimize하는 것 보다 더 쉽다.</p>

<p>극단적으로, 만약 하나의 identity mapping이 최적이라면, 비선형 층을 쌓은 identity mapping을 fit 시키는것보다 residual을 0으로 만드는게 더 쉬울 것이다.</p>

<p>$F(x)+x$라는 식은 “shortcut connections”를 통해 신경망의 feedforward에 의해 알 수 있을 것이다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet1.png" alt="ResNet1" /></p>

<p>Shortcut connections는 하나 이상의 층을 skip하는 것이다.</p>

<p>우리의 경우, shortcut connections는 단순히 identity mapping을 수행하고 그것들의 출력은 stacked layers의 출력에 더해진다.</p>

<p>Identity shortcut connections는 추가 매개변수도 없고 추가적인 계산복잡도도 없다.</p>

<p>전체 네트워크는 여전히 처음부터 끝까지 역전파와 SGD로 학습이 가능하고
라이브러리를 변형시키지 않고 쉽게 구현할 수 있다.</p>

<p>ImageNet에서 포괄적인 실험을 하여 degradation 문제도 증명하고 우리의 방법도 평가했다.</p>

<p>우리는 두가지를 증명한다</p>

<ol>
  <li>extremely deep residual net은 optimize가 쉽지만
단순히 층을 쌓아 만든 같은 네트워크는 깊이가 깊어지게 되면 학습오차가 커진다는 것</li>
  <li>deep residual net은 증가된 깊이에서 쉽게 정확도를 얻을 수 있어
이전 네트워크들과 비교해 더 나은 결과를 제공한다.</li>
</ol>

<p>비슷한 현상들을 CIFAR-10에서 볼 수 있다,
그러나 최적화 어려움과 우리 방법의 효과는 특정 데이터셋과 연관이 있지 않다.</p>

<p>100개 이상의 층을 가진 모델로 이 데이터를 학습시키고,
1000개 이상의 층을 가진 모델을 연구해 볼 것이다.</p>

<p>ImageNet 분류 데이터셋에서, extremely deep residual net을 통해 훌륭한 결과를 얻었다.</p>

<p>152층의 residual net은 VGG net보다 낮은 복잡도를 가지면서 ImageNet에 보고된 네트워크들 중에서 가장 깊은 네트워크이다.</p>

<p>ImageNet test set에서 조합을 통해 3.57%의 top-5 error를 얻었고 ILSVRC-2015 classification 대회에서 1등을 차지했다.</p>

<p>extremely deep한 표현은 generalization성능도 뛰어나다(다른 인식 작업에서도),
그리고 ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC &amp; COCO2015 competitions.</p>

<p>따라서 residual learning principle은 vision problem과 non-vision problem에 모두 일반적으로 사용될 수 있을것이다.</p>

<h1 id="2-related-work">2. Related Work</h1>

<h2 id="residual-representations">Residual Representations</h2>

<p>이미지 인식에서, VLAD는 Residual vector를 dictionary로 인코딩 하는 표현법이고
Fisher Vector는 VLAD를 확률적으로 공식화한 것이다.</p>

<p>둘 다 이미지 retrieval과 분류에서 강력한 얕은 표현 방법이다.</p>

<p>vector 양자화에서 residual vector의 인코딩은
원래 vector의 인코딩보다 더 효과적이다.</p>

<p>Partial Difference Equations 분야에 low-level vision과 computer graphics에서 널리 사용되는 Multigrid method는 문제를 시스템을 여러개의 크기의 subproblem으로 나눈다.</p>

<p>Multigrid의 대체 방안은 hierarchical basis preconditioning이다.(두 scale사이의 residual vector를 표현하는 변수에 의존하는 preconditioning)</p>

<p>이 방법은 표준적인 것과 비교해 더 빠르게 수렴한다.</p>

<p>이 방법들은 좋은 reformulation과 preconditioning은 optimization을 단순화 시킨다고 한다.</p>

<h2 id="shortcut-connections">Shortcut Connections</h2>

<p>shortcut connections를 말하는 실험들과 이론들은 오랫동안 연구되어왔다.</p>

<p>MLP의 초기 연구는 선형층이 네트워크의 입력부터 출력까지 구성하도록 하는 것이다.</p>

<p>몇개의 중간 층들은 vanishing/exploding gradient문제를 다루기 위해 보조 분류기에 직접 연결된다.</p>

<p>몇몇 논문들은 shortcut connections를 구현해서 레이어 반응, gradients, 전파 오류를 중심으로 하는 방법을 제안한다.</p>

<p>“inception” 층은 shortcut 가지와 몇개의 deeper 가지로 구성된다.</p>

<p>이 논문과 동시에, “highway networks”는 gating 기능이 있는 shortcut connections를 보여준다.</p>

<p>이 gate들은 데이터에 종속적이고 가중치가 있다,
반대로 identity shorcut은 가중치가 필요없다.</p>

<p>gated shortcut이 닫히면(0이되는 순간),
highway networks의 층들은 non-residual 기능을 보인다.</p>

<p>반대로, 우리의 식은 항상 residual 기능을 학습한다;
identity shortcut은 절대 닫히지 않는다,
그리고 모든 정보들은 항상 학습되어야 하는 추가적인 residual 함수들을 통과한다.</p>

<p>추가로, highway networks는 극도로 증가된 깊이(100이상 깊이)로 인한 정확도 증가를 증명하지 못했다.</p>

<h1 id="3-deep-residual-learning">3. Deep Residual Learning</h1>

<h2 id="31-residual-learning">3.1 Residual Learning</h2>

<p>몇개의 층이 쌓여 mapping되는 함수를 $H(x)$라고 하자,
$x$는 이 층들의 입력값이다.</p>

<p>만약 여러개의 비선형 층들로 복잡한 기능을 구현할 수 있다면,
마찬가지로 residual function으로도 복잡한 기능을 할 수 있을 것이다.  $H(x)-x$</p>

<p>그러니 쌓여있는 층들이 $H(x)$에 비슷해지는게 아니라
우리는 $F(x):=H(x)-x$가 되게 할 것이다.</p>

<p>그러므로 원래 함수는 $F(x)+x$가 될것이다.</p>

<p>두가지 형식 모두 요구되는 기능으로 근사될 것이지만,
학습 난이도는 다를 것이다.</p>

<p>이 식은 degradation문제에서 counterintuitive phenomena로부터 영감을 받은 것이다.</p>

<p>앞서 말했듯, 추가적인 층들이 identity mapping 구조를 띈다면,
더 깊은 모델은 복사본인 얕은 모델보다 학습오차율이 더 클 수 없다.</p>

<p>degradation문제는 solver가 여러개의 비선형층으로 이루어진 구조에서 identity mapping을 유추하는데에 어려움을 줄것이다.</p>

<p>residual learning reformulation에서
만약 identity mapping이 최적이라면,
solver들은 다층 비선형 구조가 identity mapping이 0을 향해 근접할 수 있도록 가중치를 움직일 것이다.</p>

<p>실제로는, identity mapping은 최적이지만 우리의 reformulation은 문제가 발생하도록 돕는 것이된다.</p>

<p>만약 최적의 함수가 zero mapping이 아니라 identity mapping이라면,
새로운 함수 하나를 학습하는 것 보다 identity mapping으로 방해요인을 찾는게 더 쉬울 것이다.</p>

<p>우리는 identity mappings가 합리적인 preconditioning을 제공하기 때문에
실험적으로 학습된 residual 함수들이 일반적으로 반응이 작다는 것을 증명했다,</p>

<h2 id="32-identity-mapping-by-shortcuts">3.2 Identity Mapping by Shortcuts</h2>

<p>우리는 모든 stacked layers에 residual learning을 도입했다.</p>

<p>공식적으로 이번 논문에서 우리가 만든 block은 아래와 같다</p>

\[y=F(x,\left\{W_i\right\})+x\]

<p>x랑 y는 각각 입력과 출력 벡터이다.</p>

<p>함수 F(x,{Wi})는 학습 되어야 하는 residual mapping을 의미한다.</p>

<p>예를들어</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet1.png" alt="ResNet1" /></p>

<p>이 구조는</p>

\[F=W_2\sigma(W_1\mathbf{x})\]

<p>가 되고 여기서 $\sigma$는 ReLU이고 편향은 표기의 단순화를 위해 제거했다.</p>

<p>수식 $F+x$는 shortcut connection에 의해 수ㅐㅎㅇ되고
element-wise addition이다.</p>

<p>두번째 비선형성은 $F+x$ 후에 계산된다.(i.e., $\sigma(y))$</p>

<p>앞에서 봤듯 shortcut connection은 계산 복잡도도 안올라가고 추가적인 가중치도 필요없다.</p>

<p>실로 매력적일 뿐 아니라 plain network와 residual network의 비교에 중요하다.</p>

<p>공정하게 plain과 residual net을 비교할 것이다.
같은 가중치 수, 깊이, 크기 그리고 element-wise addition을 제외한 계산비용까지 모두 같게하여 비교할 것이다.</p>

<p>x와 F의 차원은 반드시 같아야 한다.</p>

<p>만약 이런 경우가 아니라면(예를들어 입력과 출력의 채널을 변경하는 것),
우리는 shortcut connections의 차원을 맞추기 위해 $W_s$를 사용해 linear projection을 할 것이다.</p>

\[y=F(x,\{W_i\})+W_sx\]

<p>또한 정방행렬 $W_s$를 사용할 수도 있다.</p>

<p>하지만 우리는 실험적으로 identity mapping이 충분히 degradation문제를 다룰 수 있고 경제적임을 보일 것이다.(따라서 $W_s$는 차원을 맞추기 위해서만 사용될 것이다.)</p>

<p>residual function F는 유연하다.</p>

<p>이 논문의 실험에는 두세층이 포함된 F를 실험한다.
(더 많은 층들도 가능하다)</p>

<p>하지만 만약 F가 단층이라면</p>

\[y=F(x,\{W_i\})+x\]

<p>는 선형 식이 될것이다.</p>

\[y=W_1x+x\]

<p>위 식에서는 어떤 이점도 찾을 수 없다.</p>

<p>또한 우리는 단순함을 위해 전결합을 사용했지만,
residual은 합성곱 층에서도 사용 가능하다.</p>

<p>$F(z,{W_i})$는 여러 합성곱층을 표현할 수 있다.</p>

<p>element-wise addition은 두 feature map 사이에서 이루어지고
channel별로 이루어진다.</p>

<h2 id="33-network-architectures">3.3 Network Architectures</h2>

<p>다양한 plain/residual net들을 실험해보고
일관적인 현상들을 관찰했다.</p>

<p>사례를 위해 ImageNet을 위한 두가지 모델을 설명하겠다.</p>

<h2 id="plain-network">Plain Network</h2>

<p>VGG net의 이론에서 영감을 받아 plain 네트워크 구조를 만들었다.</p>

<p>합성곱층은 최대 3x3크기의 필터와 두가지 간단한 규칙을 따른다.</p>

<ol>
  <li>output feature map size가 동일하기 위해,
층들은 같은 수의 filter를 갖는다.</li>
  <li>만약 feature map size가 반으로 준다면,
filter의 수를 두배로 해서 층마다 시간 복잡도를 유지한다.</li>
</ol>

<p>downsampling은 합성곱층에서 stride를 2로 설정해서 수행한다.</p>

<p>네트워크는 global average pooling layer와 softmax를 사용하는 1000-way 전결합층으로 마무리된다.</p>

<p>가중치층의 총 수는 34개이다.</p>

<p>Plain Network는 VGG net보다 필터 수도 적고 복잡성도 낮다는것은 주목할 만하다.</p>

<p>34개의 층들은 36억개의 FLOPs(multiply-add)가 필요하고
이는 196억개의 FLOPs인 VGG-19의 18%에 해당하는 수치이다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet2.png" alt="ResNet2" /></p>

<h2 id="residual-network">Residual Network</h2>

<p>Plain net을 기반으로, shortcut connections를 추가해 Plain과 같은 부분을 residual 로 만들었다.</p>

<p>왼쪽에서 실선 shortcut connections처럼 입력과 출력의 차원이 같다면 identity shortcut 을 바로 사용할 수 있다.</p>

<p>차원이 증가하는 경우에는 점선으로된 shortcut connections처럼표현하고 두가지 경우를 고려한다.</p>

<ol>
  <li>shortcut은 여전히 identity mapping을 수행한다.
zero padding을 통해 차원을 늘려서 사용한다.
이 방법은 추가적인 매개변수가 없다</li>
  <li>projection shortcut은 차원을 일치시킬 때 사용된다.(1x1합성곱으로 완성)</li>
</ol>

<p>두개의 크기인 feature map을 통과할 때 두가지 경우 모두 stride=2로 수행한다.</p>

<h2 id="34-implementation">3.4 Implementation</h2>

<p>이미지는 scale augmentation을 위해 256~480에서 임의로 선택된 값으로 짧은 부분을 resize한다.</p>

<p>224,224 crop은 이미지로부터 임의로 sample을 구하거나
horizontal flip(각 pixel에 평균값을 빼는 것까지)을 통해서 도 sample을 취한다.</p>

<p>표준 color augmentation도 사용되었다.</p>

<p>convolution연산 뒤에 activation연산 전에 BatchNormalization(BN)을 사용했다.</p>

<p>가중치는 13번 논문을 따라서 초기화시켰고
네트워크는 처음부터 학습시켰다.</p>

<p>SGD를 mini-batch size 256으로 사용했다</p>

<p>Learning rate는 0.1부터 시작해서 error plateau일 때마다10씩 나눴다.</p>

<p>학습은 600000번만큼 iteration했다.</p>

<p>weight decay는 0.0001을 사용하고
momentum계수는 0.9로 했다.</p>

<p>Dropout층은 사용하지 않았다.</p>

<p>test에서는 연구의 비교를 위해 표준적으로 10-crop testing을 했다.</p>

<p>최고의 결과를 위해 fully-convolutional form을 사용했고
이미지의 짧은 축의 크기를 224,256,384,480,640으로 설정한 여러개의 크기에서 계산을 하고 결과를 평균내었다.</p>

<h1 id="4-experiments">4. Experiments</h1>

<h2 id="41-imagenet-classification">4.1 ImageNet Classification</h2>

<p>네트워크 평가를 위해 1000개의 클래스로 구성된 ImageNet 2012 classification dataset을 사용했다.</p>

<p>모델들은 128만개의 학습 이미지를 통해 학습하고
5만개의 validation 이미지로 평가됐다.</p>

<p>또한, 최종 결과는 10만개의 test image를 통해 얻었ek.</p>

<p>top-1, top-5 error rate를 계산했다.</p>

<h3 id="plain-networks">Plain Networks</h3>

<p>18-layer와 34-layer를 평가했다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet3.png" alt="ResNet3" /></p>

<p>Downsampling은 conv3_1,conv4_1,conv5_1에서도 stride 2로 진행된다</p>

<p>아래 결과를 보면 더 깊은 34-layer가 얕은 18-layer보다 validation error가 더 큰 것을 알 수 있다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet4.png" alt="ResNet4" /></p>

<p>얇은 선은 training error, 굵은 선은 validation error</p>

<p>이유를 알아보기 위해 학습과정에서 training/validation error를 확인해 보니 degradation 문제가 생긴것을 확인했다.</p>

<p>왜냐하면 학습 전체 과정에서 34-layer의 학습오차가 18-layer보다 항상 높았기 때문이다.
심지어 18-layer의 해공간이 34-layer의 해공간의 하위 집합이었음에도 이러한 결과가 나타났다.</p>

<p>이런 optimize difficulty는 불행하게도 vanishing gradient 때문에 발생한다.</p>

<p>plain net들은 전파 과정에서 0이 아닌 분산값을 갖게 하도록
BN을 사용했다.</p>

<p>또한, 역전파에서 gradient들은 BN을 통해 좋은 정규화가 이루어진 것을 증명할 수 있다.</p>

<p>따라서 순전파와 역전파 모두 신호를 소실시키지 않았다.</p>

<p>사실 34-layer plain net은 여전히 경쟁적인 정확도를 달성할 수 있다. (solver가 어느정도 작동한다면)</p>

<p>deep plain net들은 매우 낮은 비율로 수렴하기 때문에 training error를 낮추는데 영향을 준다.</p>

<p>이러한 optimization difficulties는 후에 연구될 것이다.</p>

<h3 id="residual-networks">Residual Networks</h3>

<p>다음으로 18-layer와 34-layer residual nets(ResNets)를 평가했다.</p>

<p>기본 구조는 plain net과 같고 3x3 filter쌍마다 shortcut connection이 추가 되어있다.</p>

<p>아래 비교에서 모든 shortcut 에 identity mapping을 적용했고
차원을 높이기 위해 zero-padding을 했다.</p>

<p>따라서 plain net과 비교해 추가적인 매개변수가 없다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet5.png" alt="ResNet5" /></p>

<p>얇은 선은 training error, 굵은 선은 validation error</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet6.png" alt="ResNet6" /></p>

<p>위 결과로부터 3가지 주요 관찰할 것을 얻었다.</p>

<h3 id="residual-학습을-통해-상황이-반대가-되었다">Residual 학습을 통해 상황이 반대가 되었다.</h3>

<p>residual 학습으로 34-layer의 결과가 18-layer의 결과보다 좋게 나왔다.</p>

<p>더 중요한 것은 34-layer ResNet은 상당히 낮은 training error를 보이고 validation data에 범용성을 갖췄다.</p>

<p>이것은 degradation문제가 이번 setting으로 잘 해결이 되고
또 깊은 구조의 네트워크로부터 높은 정확도를 얻었다는 것을 의미한다.</p>

<h3 id="plain-net과-resnet을-비교했을-때">plain net과 ResNet을 비교했을 때,</h3>
<p>34-layer ResNet은 top-1 error가 줄었다는 것이다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet7.png" alt="ResNet7" /></p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet8.png" alt="ResNet8" /></p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet9.png" alt="ResNet9" /></p>

<p>이러한 비교는 extremely deep system에서 residual learning의 효과를 입증한다.</p>

<p>마지막으로, 18-layer Plagin/Residual net들은 비슷한 정확도를 보이지만
18-layer ResNet이 더 빠르게 수렴한다.</p>

<p>network가 지나치게 깊지 않은 경우(여기서는 18-layer)
SGD는 계속해서 좋은 solution을 Plain net에 찾아줄 수 있다.</p>

<p>이 경우, ResNet은 더 빠른 단계에서 수렴을 하도록 하기 때문에 optimization이 쉬워진다.</p>

<h3 id="identity-vs-projection-shorcuts">Identity vs. Projection Shorcuts</h3>

<p>추가 매개변수가 없는것과 identity shorcuts는 학습을 돕는다고 증명했다.</p>

<p>다음으로는 projection shortcut을 살펴보겠다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet7.png" alt="ResNet7" /></p>

<p>Table 3에서 세가지 옵션들을 비교했다.</p>

<p>(A) 차원의 증가를 위해 zero-padding을 사용하고
모든 shortcut을 identity mapping으로 구현해 추가 매개변수가 없게 함.</p>

<p>(B) projection shortcut을 사용해 차원을 증가시키고
다른 shortcut들은 identity를 사용했다.</p>

<p>(C) 모든 shortcut들을 projection으로 사용했다.</p>

<p>Table 3에서 볼 수 있듯이 세가지 옵션 모두 plain net과 비교해 상당히 좋은 결과를 만든다.</p>

<p>B가 A보다 약간 더 좋은데
A에서 zero-padding으로 늘어난 차원이 사실 residual learning이 아니기 때문이다.</p>

<p>C는 B보다 조금 더 좋은데
projection shortcut에 의해 매개변수가 더 추가 되었기 때문이라고 생각한다.</p>

<p>하지만, A/B/C 사이에 차이가 작은 것은 projection shortcut이 degradation 문제를 다루는데 필수적인 것은 아니라는 것을 나타낸다.</p>

<p>따라서 이 논문의 나머지 부분에서는 C를 사용하지 않는다.
(메모리사용량과 시간 복잡도와 모델의 크기를 줄이기 위해)</p>

<p>Identity shortcut들은 bottleneck 구조의 복잡도를 증가시키지 않기 때문에 특히 중요하다.</p>

<h3 id="deeper-bootleneck-architectures">Deeper Bootleneck Architectures</h3>

<p>다음으로 더 깊은 구조를 보겠다.</p>

<p>사용할 수 있는 학습시간에 대한 염려로 인해,
building block을 bottleneck 구조로 변경했다.</p>

<p>(non-bottleneck ResNet도 좋은 정확도를 보이지만
경제적이지 못해 bottleneck ResNet을 사용한다.
따라서 bottleneck ResNet을 사용하는 이유는 practical consideration 때문인 것이다.)</p>

<p>각 residual function F에 층을 3개씩 샇았다.</p>

<p>3개의 층은 1x1,3x3,1x1 합성곱을 한다.</p>

<p>1x1 합성곱은 차원을 줄였다가 다시 키우는 역할을 하며
3x3 합성곱은 작은 차원의 입출력에 대해 병목현상을 일으킨다.</p>

<p>bottleneck과 non-bottleneck 모두 시간 복잡도는 비슷하다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet10.png" alt="ResNet10" /></p>

<p>매개변수가 추가되지 않는 identity shortcuts는 병목 구조에서
특히 더 중요하다.</p>

<p>만약 identity shortcut이 projection으로 대체된다면,
시간복잡도와 모델 크기가 두배가 될것이다.
마치 shortcut이 두개의 높은 차원과 연결된것처럼</p>

<p>따라서 identity shortcut은 bottleneck 구조를 더 효율적으로 만든다.</p>

<h3 id="50-layer-resnet">50-layer ResNet</h3>

<p>34-layer net에서 두개의 layer block을 3-layer bottleneck block으로 대체했다.(그 결과는 Table1의 50-layer ResNet이다.)</p>

<p>옵션B를 사용해 차원을 키웠고
이 모델의 FLOPs는 38억이다.</p>

<h3 id="101-layer-and-152-layer-resnets">101-layer and 152-layer ResNets</h3>

<p>101-layer과 152-layer ResNet을 만들었다(3-layer block을 더 사용하여)</p>

<p>깊이가 충분히 증가했지만,
152-layer ResNet(113억 FLOPs)은 여전히 VGG-16/19(153억/196억 FLOPs)보다 복잡도가 낮았다.</p>

<p>50/101/152-layer ResNet은 margin을 고려한 34-layer보다 더 정확하다.</p>

<p>degradation을 관찰하지 못했으므로
상당히 깊은 네트워크로부터 충분히 높은 정확도를 얻은 것이다.</p>

<p>모든 평가겨로가에서 깊이에 대한 이점이 발견되었다.</p>

<h3 id="comparisons-with-state-of-the-art-methods">Comparisons with State-of-the art Methods</h3>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet8.png" alt="ResNet8" /></p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet9.png" alt="ResNet9" /></p>

<p>위 결과에서 보듯 이전에 사용하던 최고의 단일-모델과 비교했다.</p>

<p>34-layer ResNet이 좋은 결과를 보였다.</p>

<p>152-layer ResNet은 단일-모델로써 top-5 validation error값이 4.49%가 나왔다.</p>

<p>테이블5에서 보이는 여러 모델을 조합한 결과들과 비교가 가능할 정도로
단일 모델인 152-layer ResNet은 매우 뛴어나다.</p>

<p>ensemble을 구성하기 위해 서로다른 깊이의 6개의 모델들을 조합했다.
(제출할 때에는 152-layer 두개만 조합했다.)</p>

<p>결과는 top-5 error값으로 3.57%가 나왔고
이 결과로 ILSVRC-2015 에서 1등을 차지했다.</p>

<h2 id="42-cifar-10-and-analysis">4.2 CIFAR-10 and Analysis</h2>

<p>CIFAR-10(10개의 클래스를 갖는 5만개의 학습 1만개의 테스트이미지)에서 더 많은 연구를 해보았다.</p>

<p>우리의 초점은 최첨단 구조에서 보이는 결과를 얻는 것이 아니기 때문에
extremely deep network의 구조를 간단하게 구성했다.</p>

<p>입력으로 32x32을 받는다(각 픽셀은 평균값들을 뺀 상태이다)</p>

<p>첫 번째 층은 3x3 합성곱층이다.</p>

<p>그리고나서 6n개의 3x3 합성곱 층을 쌓아 특징맵의 크기가 32,16,8에 맞게 할당을 해서
각 특징맵별로 2n개의 합성곱 계산을 해야한다.</p>

<p>subsampling은 stride2인 합성곱으로 진행했다.</p>

<p>네트워크의 마지막 부분은 global average pooling과 10-way fully-connected layer with softmax를 사용했다.</p>

<p>따라서 총 가중치층은 6n+2가 된다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet11.png" alt="ResNet11" /></p>

<p>shortcur connection이 사용되면,
그것들은 3x3 합성곱 두개에 하나씩 연결되어 총 3n개의 shortcut이 생긴다.</p>

<p>CIFAR-10에는 모든 경우에 identity shortcut(option A)을 사용해서
plain 모델과 비교해 깊이, 크기, 매개변수의 수가 모두 같다.</p>

<p>이 때 weight decay 는 0.0001
momentum계수는 0.9
그리고 dropout은 사용하지 않지만 BN을 사용했다.</p>

<p>batch-size는 128로하고
두개의 GPU에서 학습을 진행했다.</p>

<p>learning rate는 0.1로 초기 설정하고 32k와 48k에서 10씩 나눠서 적용했다
그리고 64k iteration에서 학습을 종료했다.(train/val의 크기를 45k/5k로 설정한 결과)</p>

<p>data augmentation은 간단하게하여 학습을 진행했다 :</p>

<p>각 모서리에 4pixel만큼씩 padding을 했고
padding한 이미지와 그것을 뒤집은 이미지로부터 32x32의 크기만큼 임의로 잘랐다.</p>

<p>test를 위해 32x32이미지의 single view만 평가했다.</p>

<p>n={3,5,7,9}를 적용해 20, 32, 44, 56-layer network를 만들었다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet12.png" alt="ResNet12" /></p>

<p>Highway network를 보면 증가된 깊이로부터 error가 증가하는 것을 볼 수 있다.</p>

<p>이 현상은 ImageNet과 MNIST에서 비슷하게 보여진다.
(모두 optimization difficulty가 문제다)</p>

<p>ImageNet에서와 비슷하게 ResNet은 optimization difficulty를 극복하고
깊어진 깊이로부터 높은 정확도를 얻는다고 증명했다.</p>

<p>게다가 n=18인 경우에 110-layer ResNet을 만드는데</p>

<p>이런 경우, 수렴을 시작하기에 앞서 learning rate가 0.1인 것은 다소 높다고 판단했다.</p>

<p>그래서 training error가 80%아래로 내려갈 때까지 0.01로 학습률을 설정했다.</p>

<p>그리고 나서 학습률을 다시 0.1로 설정하고 계속 학습했다.</p>

<p>110-layer도 잘 수렴했다.</p>

<p>110-layer는 최첨단 구조가 아님에도 다른 깊거나 얕은 구조들 (Fitnet and Highway)보다 더 매개변수가 적다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet13.png" alt="ResNet13" /></p>

<p>bold : training error, dashed : test error</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet14.png" alt="ResNet14" /></p>

<p>bold : training error, dashed : test error</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet15.png" alt="ResNet15" /></p>

<h3 id="analysis-of-layer-responses">Analysis of Layer Responses</h3>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet16.png" alt="ResNet16" /></p>

<p>위 그래프에서 표준편차를 확인할 수 있다.</p>

<p>반응들은 각 3x3 layer에 의한 결과이고 BN의 결과이고 activation함수를 통과하기 전의 값이다.</p>

<p>ResNet에서 이런 분석은 residual functions의 response strength를 표출한다.</p>

<p>위 그래프는 ResNet은 일반적으로 plain 구조에 비해 response가 작다고 보여준다.</p>

<p>이러한 결과는 우리의 기본 동기를 뒷받침해준다.</p>

<p>따라서 보통 residual function이 일반적으로 non-residual인것보다 0에 더 가깝다는 것이다.</p>

<p>또한, 깊은 ResNet은 더 작은 크기의 response를 보인다.</p>

<p>더 많은 층이 있을 경우, ResNet의 각 층은 신호를 덜 수정하는 경향이 있다.</p>

<h3 id="exploring-over-1000-layers">Exploring Over 1000 layers</h3>

<p>1000층이 넘는 아주 깊은 모델을 조사해보자.</p>

<p>n=200으로 설정을해서 1202-layer network를 만들었다.
학습은 위와 같은 방법으로 진행했다.</p>

<p>우리의 방법은 optimization difficulty를 보이지 않고
이 1000-layer network는 0.1미만의 학습 오차를 달성할 수 있다.</p>

<p>이것의 test error는 여전히 괜찮은 수준인 7.93%이다.</p>

<p>하지만 여전히 매우 깊은 모델이 갖는 문제는 해결되지 않았다.</p>

<p>1202-layer의 test 결과는 우리의 110-layer 네트워크보다 안좋다.
(비록 두 모델이 비슷한 학습에러를 결과를 보이지만)</p>

<p>이것은 과적합 때문인다.</p>

<p>1202-layer는 이 작은 데이터셋에 불필요하게 많은 층이 사용되었다.</p>

<p>강력한 regularization(e.g.,  maxout, dropout)은 이 데이터셋을 상대로 최고의 결과를 얻기 위해 사용되었다.</p>

<p>그러나 이번 연구에서 maxout과 dropout은 사용하지 않았고
구조적으로 깊고 얕음을 통해 regularization을 부과하고
optimization difficulty에는 초점을 두지 않았다.</p>

<p>하지만, 강력한 regularization의 조합은 결과를 향상시킬것이다.(앞으로 연구해볼 것이다.)</p>

<h2 id="43-object-detection-on-pascal-and-ms-coco">4.3 Object Detection on PASCAL and MS COCO</h2>

<p>우리의 방법은 다른 recognition 작업에서도 좋은 generalization 성능을 보인다.</p>

<p><img src="/assets/img/Paper_Review/ResNet/ResNet17.png" alt="ResNet17" /></p>

<p>Table7과 Table8 에서 볼 수 있듯이
object detection baseline은 PASCAL VOC 2007 and 2012 and COCO를 통해 평가했다.</p>

<p>detection 방법으로 Faster R-CNN을 채택했다.</p>

<p>우리는 VGG-16을 ResNet-101로 대체하여 개선하는 것에 관심이 있다.</p>

<p>두 모델을 사용하여 detection 구현하는 것은 같다,</p>

<p>따라서 더 좋은 네트워크를 기반으로 이점이 생긴다.</p>

<p>대부분 눈에 띄는 점은, COCO데이터셋에서 6.0% 상승한 값을 얻었다는 것이다.(상대적으로 28% 개선됨)</p>

<p>이런 이점은 단지 learned representation때문인 것이다.</p>

<p>Deep residual net을 기반으로 ILSVRC &amp; COCO 2015 competition에서 몇몇 부문에서 1등을 차지했다 :
ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[ResNet]]></summary></entry><entry><title type="html">VGGNet</title><link href="http://localhost:4000/paper%20review/2022/08/20/VGGNet.html" rel="alternate" type="text/html" title="VGGNet" /><published>2022-08-20T03:24:34+09:00</published><updated>2022-08-20T03:24:34+09:00</updated><id>http://localhost:4000/paper%20review/2022/08/20/VGGNet</id><content type="html" xml:base="http://localhost:4000/paper%20review/2022/08/20/VGGNet.html"><![CDATA[<h1 id="vggnet">VGGNet</h1>

<h1 id="1-introduction">1. Introduction</h1>

<p>합성곱 네트워크는 최근(2015) 대규모 공공 이미지 저장소인 ImageNet과 고성능 계산 시스템(GPU)와 대규모 cluster 덕분에 큰 규모의 이미지나 비디오 인식에 좋은 성과를 보인다.</p>

<p>특히, ILSVRC는 deep visual recognition 구조들의 진보에서 중요한 역할을 했다.</p>

<p>ILSVRC는 고차원 얕은 특징 인고딩에서 심층 ConvNet에 이르기까지 몇 세대에 걸친 대규모 이미지 분류 시스템의 테스트베드 역할을 했다.</p>

<p>ConvNet들이 computer vision 분야에서 점점 상품화 되면서 더 높은 정확도를 달성하기위해 Krizhevsky의 original 구조(AlexNet)를 개선하려고 많은 시도들이 있었다.</p>

<p>예를들어, ILSVRC-2013에서 최고의 성능은 Alexnet에서 receptive window의 크기를 줄이고 첫 번째 합성곱층에서 더 작은 stride를 사용해서 얻은 모델로부터 얻어졌다.</p>

<p>다른 라인에 대한 개선은 모델이 전체 이미지나 크기가 변형된 이미지를 더 조밀하게 학습하고 시험하게 했다.</p>

<p>이 논문에서, 우리는 ConvNet구조에서 또 다른 중요한 측면인 깊이에 대해 알아볼 것이다.</p>

<p>논문 끝에는, 구조의 다른 매개변수들을 고정시키고 모든 층에서 합성곱 필터의 크기를 3x3으로 작게 설정하게 해서 합성곱층을 더 많이 추가하여 깊이를 꾸준히 증가시켜볼 것이다.</p>

<p>결과적으로, ILSVRC classification과 localisation작업에서 최첨단 모델의 정확도를 달성할 뿐 아니라 간단한 파이프라인에 사용되더라도 다른 이미지 데이터에 대해 적용했을 때 최고의 성능을 보이는 상당히 정확한 ConvNet 구조를 만들었다.</p>

<p>우리는 연구를 통해 얻은 가장 좋은 성능의 두가지 모델을 후속연구를 위해 공개했다.</p>

<p>다음장에서는 ConvNet 구성을 묘사하고
3장에서는 image classification training과 evaluation에 대한 내용을 설명하고
4장에서는 ILSVRC classification작업에서 configuration들을 비교하고
5장에서는 결론을 내렸다.</p>

<p>ILSVRC-2014 object localisation system을 Appendix A에서 묘사하고 평가했다
Appendix B에서는 very deep features 의 generalisation에대해 논의하고
Appendix C에서는 주요 논문에 대한 개정 내용이 포함된다.</p>

<h1 id="2-convnet-configurations">2. ConvNet Configurations</h1>

<p>같은 조건에서 ConvNet depth의 증가가 가져오는 개선을 측정하려면 설계에 사용된 모든 ConvNet 층의 구성에 Ciresan과 Krizhevsky와 동일한 조건들을 적용해야 한다.</p>

<p>2장에서는 먼저 ConvNet 구성에서 일반적인 내용을 서술하고 그다음 평가에서 사용된 특정한 configuration에 대해 자세하게 서술한다.</p>

<p>마지막으로 설계할 때 했던 선택들을 이전 최신기술과 비교해볼것이다.</p>

<h2 id="21-architecture">2.1 Architecture</h2>

<p>학습하는 동안, ConvNet의 입력값은 224x224x3 형태이다.</p>

<p>각 픽셀로부터 training set에서 계산한 평균 RGB값을 빼주는 전처리 이외에 다른 전처리는 하지 않았다.</p>

<p>양옆과 위아래에 대한 정보를 담을 수 있는 가장 작은 사이즈인 3x3의 매우 작은 receptive field를 가지는 필터를 사용한 합성곱층의 stack에 이미지를 통과 시켰다.</p>

<p>우리는 non-linearity를 따르는 1x1 합성곱 필터 또한 사용했다.</p>

<p>합성곱의 stride는 1 pixel로 고정했다; convolution 연산 후에 입력과 같은 모양이 나오도록 padding은 1 pixel을 사용했다.</p>

<p>Spatial pooling은 합성곱 뒤에(모든 합성곱은 아님) 5개의 max-pooling층으로 수행된다.</p>

<p>Max-pooling은 2x2 pixel window가 stride=2의 값으로 연산을 한다.</p>

<p>모델별로 다른 깊이를 적용해서 만든 stack of convolutional layers 뒤에는 전결합층이 나온다.</p>

<p>처음 두개의 전결합층은 각각 4096개의 유닛을 가지고 세 번째 전결합층은 1000개의 유닛을 갖는다.</p>

<p>마지막층은 SoftMax층이다.</p>

<p>전결합층들의 구성은 모든 네트워크들이 같도록 설계했다.</p>

<p>모든 은닉층들은 ReLU non-linearity를 사용한다.</p>

<p>이번 연구의 네트워크들은 한 개를 제외하고 AlexNet에서 사용된 Local Response Normalisation(LRN) 을 사용하지 않았다 :
LRN은 ILSVRC 데이터 셋에서의 성능을 개선시키지 못하면서 메모리 사용량은 증가시키고 계산시간도 증가시켰다.</p>

<p>LRN이 적용된 경우 LRN층의 매개변수들은 AlexNet의 LRN층의 매개변수와 같다.</p>

<h2 id="22-configurations">2.2 Configurations</h2>

<p>이번 연구에서 평가할 ConvNet configuration들은 Table 1에 요약되어있고 column당 하나의 모델이다.</p>

<p>지금부터는 네트워크의 이름을 A-E사이로 이름을 부르겠다.</p>

<p>모든 configuration들은 2.1에서 설계한 구조를 따르고 서로 깊이만 다르게 한다: 11개의 가중치 층을 갖는 network A부터 19개의 가중치 층을 갖는 network E</p>

<p>합성곱층의 채널 수는 꽤 작다, 64부터 시작해서 512가 될 때까지 2를 곱한다.</p>

<p>Table 2에서는 각 configuration별로 매개변수의 수를 보여준다.</p>

<p>더 깊은 구조임에도 불구하고 가중치의 수는 우리의 network들이 더 깊이가 얕은 network와 비교했을 때 합성곱층의 채널 수도 적고 receptive field도 작아서 가중치가 더 많지 않다.</p>

<h2 id="23-discussion">2.3 Discussion</h2>

<p>ConvNet 구성들은 ILSVRC-2012와 ILSVRC-2013에서 입상한 가장 좋은 모델과 꽤 다르게 생겼다.</p>

<p>Krizhevsky의 11x11 크기의 필터를 stride 4만큼 씩 움직이는 합성곱층과 와 Zeiler&amp;Fergus의 7x7 크기의 필터를 stride 2만큼 씩 움직이는 합성곱층처럼 큰 receptive field를 사용하기 보다
우리는 전체 네트워크에서 3x3 크기의 receptive field를 사용하고 stride는 1씩 움직였다.</p>

<p>spatial pooling없이 두개의 3x3 크기의 필터를 사용하는 합성곱층을 사용하는 것은 5x5 크기의 receptive field를 사용하는 합성곱과 같다는 것을 쉽게 알 수 있다 ;
3x3 합성곱이 3개면 7x7 receptive field와 같은 효과를 보인다.</p>

<p>그렇다면 3x3 합성곱을 3개 연달아 사용하는 것이 7x7 합성곱을 1개 사용하는 것과 비교하여 우리가 어떤 이점을 얻는가?</p>

<p>첫째, 하나의 non-linear rectification layer를 사용하는거보다 3개의 non-linear rectification layers를 통합하여 사용하여 decision function을 더 차별적으로 만들 수 있다.</p>

<p>둘째, 매개변수의 수를 줄일 수 있다 :
만약 3개의 3x3 합성곱 stack의 입력과 출력이 C개의 채널을 갖는다면 stack의 매개변수는 $3(3^2C^2)=27C^2$ 개가 될것이다 ;
동시에 하나의 7x7 합성곱층은 $7^2C^2=49C^2$개의 가중치가 필요하다.
이전에 비해 가중치 수가 81% 증가한다.</p>

<p>이는 7x7 합성곱층에 regularization을 부과하여 사이사이 non-linearity가 주입된 3x3 필터들로 분해되도록 하는 것으로 보일 수 있다.</p>

<p>configuration C와 같이 1x1 합성곱층을 포함시키면 receptive field에 영향을 주지 않고 decision function에 non-linearity를 추가할 수 있다.</p>

<p>여기서 1x1 합성곱은 입력과 출력의 채널이 같아지도록 입력을 같은 차원에 선형 투영을 하지만
rectification 함수에 의해 추가된 non-linearity가 도입된다.</p>

<p>1x1 합성곱층은 최근 Lin의 “Network in Network” 구조에 활용되었다는 것을 알아야 한다.</p>

<p>Ciresan에 의해 이전부터 작은 크기의 합성곱 필터가 사용되었지만,
그들의 network들은 우리의 network보다 충분히 얕고 대규모 데이터셋 ILSVRC로 평가해보지 않았다.</p>

<p>Goodfellow가 11개의 가중치층을 갖는 깊은 ConvNet을 거리의 숫자 인식 작업에 적용했더니 깊이가 깊어질수록 더 좋은 성능을 보였다고 한다.</p>

<p>ILSVRC-2014 분류 작업에서 최고 실적을 보인 GoogLeNet은 우리와 따로 개발되었지만,
작은 필터를 이용한 합성곱과 매우 깊은 ConvNet(22 가중치 층)라는 점에서 유사하다.
(합성곱 필터는 3x3과 1x1 외에도 5x5를 사용했다.)</p>

<p>그러나 GoogLeNet의 topology는 우리가 설계한 구조보다 복잡하고 
feature map의 saptial resolution이 첫 번째 합성곱층에서 계산량을 줄이기 위해 더 공격적으로 줄어든다.</p>

<p>우리의 모델은 GooLeNet보다 single-network classification accuracy 측면에서 뛰어나다.</p>

<p><img src="/assets/img/Paper_Review/VGGNet/VGGNet0.png" alt="VGGNet0" /></p>

<h1 id="3-classification-framework">3. Classification Framework</h1>

<p>이전 장에서 우리의 네트워크의 구성을 자세하게 살펴보았다.</p>

<p>이번장에서는 classification ConvNet training과 evaluation을 자세하게 살펴보겠다.</p>

<h2 id="31-training">3.1 Training</h2>

<p>ConvNet 학습과정은 일반적으로 Krizhevsky를 따라했다.(나중에 설명하지만 학습 이미지의 크기를 키우고 잘라서 만든 샘플은 사용하지 않았다.)</p>

<p>즉, 학습은 multinomial logistic regression을 목적으로 mini-batch gradient descent(LeCun의 역전파를 기반으로)를 momentum과 함께 사용해 optimising 했다.</p>

<p>batch size는 256
momentum은 0.9으로 설정했다.</p>

<p>0.0005값을 weight decay로 설정한 L2 regularization과 dropout ratio를 0.5로 설정하여 첫 두개의 전결합층에 Dropout regularization을 적용했다.</p>

<p>학습률은 0.02로 초기설정을 했고 validation set accuracy가 높아지지 않을 때 10씩 나눠줬다.</p>

<p>전체적으로, 학습률은 3번 감소했고 학습은 370000iteration 뒤에 멈췄다(74epoch).</p>

<p>AlexNet과 비교해 우리 구조가 더 많은 매개변수와 더 깊은 네트워크임에도 더 적은 epoch수로 수렴할 수 있었던 이유는</p>

<ol>
  <li>더 깊지만 작은 합성곱 필터가 주는 암시적 정규화</li>
  <li>특정 층의 pre-initialisation</li>
</ol>

<p>네트워크 가중치의 초기설정은 중요하다,
왜냐하면 안좋은 초기설정은 deep net에서 gradient의 불안정한 성질 때문에 학습을 멈추게 하기 때문이다.</p>

<p>이 문제를 해결하기 위해 random 초기설정으로 학습하기에 충분히 얕은 configuration A부터 학습 시켰다.</p>

<p>그리고나서, 더 깊은 구조를 학습시킬 때, 첫 4개의 합성곱 층과 뒤에 3개의 전결합 층의 가중치를 A의 결과와 똑같이 설정해주고 중간 층은 random initialisation했다.</p>

<p>사전 초기설정된 층의 학습률을 줄이지 않고, 학습하는 동안 변경하도록 했다.</p>

<p>random initialisation의 경우, 가중치들이 평균이 0이고 표준편차가 0.01인 분포를 따르는 표본에서 추출했다.</p>

<p>편향들은 0으로 초기설정했다.</p>

<p>논문 출판 후에 우리는 Glorot&amp;Bengio(Xavier)의 random initialisation procedure를 통해 사전학습 없이 임의 설정된 가중치를 사용하는것이 가능하다는 것을 알았는데 이는 주목할만 하다.</p>

<p>224x224크기의 입력 이미지를 받기 위해서 학습 이미지들의 크기를 조절하고 임의로 잘랐다.(한번 crop하면 한 iteration동안 사용된다.)</p>

<p>학습 데이터를 더 증가시키기 위해서, crop하기 전에 임의로 horizontal flipping과 RGB colour shift를 했다.(AlexNet과 같이)</p>

<p>학습 이미지의 크기 조절은 밑에서 설명한다.</p>

<h3 id="학습-이미지-크기">학습 이미지 크기</h3>

<p>S를 학습이미지를 isotropically-rescaled 했을 때 짧은 쪽이라고 하자.(S를 training scale이라고도 한다)</p>

<p>자를 크기가 224x224로 고정된 반면,
원칙적으로 S는 224 이상의 값을 받을 수 있다. :</p>

<p>S=224라면 crop은 전체 이미지를 capture한다,  학습 이미지의 짧은 부분을 완전히 사용한다;
S»224라면 crop은 작은 객체나 객체의 일부를 포함하는 이미지의 작은 부분일 것이다.</p>

<p>training scale S를 정하기 위해 두가지 접근을 고려한다.</p>

<p>먼저 S를 single-scale training(한개의 값)과 같이 고정하는 것이다.</p>

<p>연구에서, AlexNet과 같이 256의 값으로도 해보고 384로도 고정해 보았다.</p>

<p>주어진 ConvNet 구성에서, 우리는 첫 번째 네트워크를 S=256으로 학습시켰다.</p>

<p>S=384인 네트워크의 학습 속도를 높이기 위해,
S=256일때 학습한 가중치로 초기설정을 하고 학습률을 0.001로 초기설정을 했다.</p>

<p>두번째 접근은 S를 multi-scale training(범위 지정)으로 설정하는 것이다.
각 이미지는 개별적으로 S의 범위에서 무작위로 한 값을 정해 크기조절을 진행한다.</p>

<p>S의 범위는 256~512로 정했다.</p>

<p>이미지 안에 객체들이 크기가 다를 수 있기 때문에,
학습 중에 이를 계산하는 것이 좋다.</p>

<p>이것은 하나의 모델이 넓은 범위에 걸쳐 객체를 인식하도록(scale jittering) 학습 데이터를 늘린 것처럼 보일 수 있다.</p>

<p>속도 문제 때문에 S=384인 single-scale model이 사전 학습한 모든 층의 가중치를 fine-tuning해서 같은 configuration인 multi-scale model을 학습 시켰다.</p>

<h2 id="32-testing">3.2 Testing</h2>

<p>test scale Q를 사용해 이미지를 isotropically rescale한다.</p>

<p>Q는 S와 같을 필요는 없다.</p>

<p>그리고나서, 네트워크를 조밀하게 rescale된 test image에 적용한다.</p>

<p>전결합 층들은 먼저 합성곱 층으로 전환된다.
첫번째 FC는 7x7 합성곱으로
두번째 FC는 1x1 합성곱으로 전환.</p>

<p>네트워크의 모든층을 합성곱층으로 만든 네트워크를 uncropped인 전체 이미지에 적용한다.</p>

<p>결과는 입력 이미지의 크기에 따른 variable spatial resolution과 class score map이고 채널의 수는 클래스의 수와 같다.</p>

<p>마지막으로, 이미지의 고정된 크기의 vector of class scores를 얻기 위해, class score map은 공간적 평균을 취한다. (sum-pooled)</p>

<p>또한, test set을 horizontal flipping으로 증가시킨다;
원본과 뒤집은 사진의 soft-max class posteriors를 평균내서 최종 수치를 얻는다.</p>

<p>합성곱 네트워크는 전체 이미지에 적용되기 때문에 test 때 여러개의 자른 sample은 필요없다.
각 crop별로 네트워크가 계산을 다시 하는것은 효율이 적다.</p>

<p>동시에, Szegedy에 의하면 큰 crop 집합을 사용하는 것은 입력 이미지를 미세하게 추출하기 때문에 합성곱 네트워크를 사용하는 것보다 정확도를 향상시킨다고 한다.</p>

<p>또한, multi-crop 평가는 합성곱의 결정 조건이 다르기 때문에 세밀한 평가가 가능하다:
ConvNet을 crop에 적용하면, padding을 하지 않는다, 반면 dense evaluation의 경우 같은 crop에 대한 padding은 이미지의 이웃 부분에서 합성곱과 공간 pooling으로 자연스럽게 생긴다, 그래서 실질적으로 전반적인 네트워크의 수용영역이 커지고 많은 context가 capture된다.</p>

<p>multiple crops로 인한 계산 시간 증가는 실제로 정확도를 올리지 못한다고 생각하지만,
추론을 위해 네트워크를 스케일당 50crop(5x5 regular grid with 2flips)을 사용해 평가했다.
이때 Szegedy의 4 스케일에 걸친 144crops에 비교할만한 3스케일에 걸쳐 150crops를 평가함.</p>

<h2 id="33-implementation-details">3.3 Implementation Details</h2>

<p>구현은 C++ Caffe toolbox를 사용했다.</p>

<p>대신 충분히 많은 변경사항을 적용했다.</p>

<p>여러개의 GPU를 하나의 시스템에 설치하여 학습과 평가를 수행하고
학습과 평가는 자르지 않은 전체 사이즈의 이미지를 여러개의 크기인 상태로 진행했다.</p>

<p>다중 GPU 학습은 데이터의 수평성을 이용하여 각 학습 이미지의 batch를 몇개의 GPU batch로 나누어 수행했다.</p>

<p>GPU batch gradient가 계산되고 난 뒤,
그들 전체 batch의 gradient를 얻기 위해 평균 계산을 했다.</p>

<p>Gradient계산은 GPU들 사이에서 동시에 일어나서 결과는 한개의 GPU를 사용한 경우와 같게 나온다.</p>

<p>ConvNet의 학습 속도를 높이는 많은 정교한 방법들이 제안되었지만(네트워크에서 다른 계층에 모델과 데이터의 병렬 처리),
개념적으로 더 간단한 내용인 우리 네트워크가 한개의 GPU보다 4개의 GPU로 학습해서 3.75배의 속도 향상을 시켰다.</p>

<p>4개의 NVIDIA Titan Black GPU를 장착한 시스템에서 하나의 네트워크를 학습 시키는 경우 구조에 따라 2-3주가 걸렸다.</p>

<h1 id="4-classification-experiments">4. Classification Experiments</h1>

<h3 id="dataset">Dataset</h3>

<p>4장에서는 이미지 분류 결과 얻기 위해 ILSVRC 2012-2014에 사용된 데이터를 사용했다.</p>

<p>데이터에는 1000개의 클래스가 있고
130만개의 학습 데이터셋과 validation을 위한 5만개 그리고 평가를 위한 10만개로 나눴다.</p>

<p>분류 성능 평가는 top-1과 top-5를 통해 이루어졌다.</p>

<p>top-1은 잘못 분류된 이미지의 비율을 구하고;
top-5는 ILSVRC에서 주로 사용하는 평가 기준으로 예측한 5개의 범주에 실제 class가 있는 비율을 계산한다.</p>

<p>대부분의 실험을 위해, validation set을 test set으로 사용했다.</p>

<p>특정 실험들은 test set을 사용했고
ILSVRC-2014에 “VGG”팀으로 입상하여 ILSVRC서버에 공식적으로 제출됐다.</p>

<h2 id="41-single-scale-evaluation">4.1 Single Scale Evaluation</h2>

<p>각 모델을 single scale로 성능을 평가한다.</p>

<p>test 이미지의 크기는 
$Q=S=0.5(S_{min}+S_{max})$로 설정했다.</p>

<p>먼저, local response normalisation은 normalisation층이 없는 모델 A를 향상시키지 못했기 때문에 다른 더 깊은 구조에도 normalisation을 적용하지 않았다.</p>

<p>다음으로, ConvNet이 깊어질수록 classification error가 감소하는 것을 관찰했다 :
A의 11층부터 E의 19층까지 실험했다.</p>

<p>같은 깊이임에도, C(1x1 conv)가 D(3x3 conv)보다 성능이 안좋았다.</p>

<p>C&gt;B : 추가적인 non-linearity가 도움이 된다</p>

<p>D&gt;C : trivial 하지않은(1보다 크기가 큰)합성곱 필터를 사용해 capture spatial context하는 것도 중요하다.</p>

<p>error rate는 모델의 깊이가 19층일 때 포화 되었지만 더 깊은 모델은 더 큰 데이터셋에서 효과적일지 모른다.</p>

<p>또한 B와 B의 3x3 필터를 쓰는 합성곱층의 쌍을 5x5 필터를 쓰는 합성곱으로 대체한 네트워크와 비교를 해보았다.</p>

<p>얕아진 네트워크의 top-1 error가 B보다 7% 높아진것으로 보아
깊고 작은 필터를 사용한 네트워크가 얕고 큰 필터를 사용한 네트워크보다 뛰어나다는 것을 확인했다.</p>

<p>마지막으로, 학습할 때 $S\in[256;512]$에서 scale jittering 하게 되면 test time에서 single scale을 사용하더라도 S가 256이나 384로 고정된 값을 갖는 것 보다 상당히 더 좋은 결과를 보인다.</p>

<p>따라서 training set augmentation by scale jittering은 capturing multi-scale image statistics에 도움을 준다는 것을 알 수 있다.</p>

<p><img src="/assets/img/Paper_Review/VGGNet/VGGNet1.png" alt="VGGNet1" /></p>

<h2 id="42-multi-scale-evaluation">4.2 Multi-Scale Evaluation</h2>

<p>ConvNet 모델을 single scale로 평가한 후,
scale jittering이 test time에 미치는 영향을 평가해보겠다.</p>

<p>이것은 모델을 Q를 다른값들로 바꿔보는 것과 같이 test image를 여러 크기의 버전으로 모델을 실행시킨다, 그 다음에 결과 클래스의 posterior에 대해 평균을 계산한다.</p>

<p>training고 testing scale의 차이가 성능저하를 일으킨다는 점을 고려하여 고정된 S로 학습한 모델들을 S와 비슷한 3개의 Q로 평가 했다 :
$Q=\left{S-32,\ S,\ S+32\right}$</p>

<p>동시에, 학습 때 scale jittering은 네트워크가 test time에서 광범위한 크기에 적용되도록 하기 때문에 모델은 $S\in\left[S_{min};S_{max}\right]$인 다양성으로 학습하고 
$Q=\left{S_{min},\ 0.5(S_{min}+S_{max}),\ S_{max}\right}$
로 더 큰 범위에 걸쳐 평가했다.</p>

<p>결과를 보면 test time에서 scale jittering은 더 좋은 성능으로 이끈다(같은 모델을 single scale에서 평가한것과 비교하여).</p>

<p>이전처럼, 더 깊은 네트워크(D and E)가 최고의 성능을 보여주고 scale jittering으로 학습한 것은 고정된 S로 학습한 모델보다 더 좋다.</p>

<p>가장 좋은 네트워크 성능은 validation에서 top-1 : 24.8% / top-5 : 7.5%이다.</p>

<p>test set에서 E 네트워크의 top-5 error는 7.3%가 나왔다.</p>

<p><img src="/assets/img/Paper_Review/VGGNet/VGGNet2.png" alt="VGGNet2" /></p>

<h2 id="43-multi-crop-evaluation">4.3 Multi-Crop Evaluation</h2>

<p>dense ConvNet evaluation과 mult-crop evaluation을 비교했다.</p>

<p>또한, 이 둘의 soft-max 결과를 평균내어 두가지 방법을 보완한 방법도 평가했다.</p>

<p>보이는 바와 같이 multiple crops를 사용해 평가하게 되면 dense evaluation보다 더 좋은 결과가 나오고 두가지를 조합해서 사용하면 더 좋은 결과가 나온다.</p>

<p>위에서 말했듯이 이것은 합성곱 결정 조건이 다르게 나타나기 때문이다.</p>

<p><img src="/assets/img/Paper_Review/VGGNet/VGGNet3.png" alt="VGGNet3" /></p>

<h2 id="44-convnet-fusion">4.4 ConvNet Fusion</h2>

<p>지금까지, 각 ConvNet별로 성능을 평가했다.</p>

<p>이번에는 몇가지 모델의 soft-max class posteriors를 평균내어 결과를 조합해보겠다.</p>

<p>이 과정은 모델들을 보완해주기 때문에 성능을 개선시키고 ILSVRC-2012에서 Krizhevsky가 사용했었고 2013에는 Zeiler&amp;Fergus가 사용했다.</p>

<p>ILSVRC 제출 당시 우리는 single-scale 네트워크와 multi-scale model D(전결합 층만 미세조정을 한)만 학습 시켰다.</p>

<p>7개의 네트워크를 조합한 결과 ILSVRC test error는 7.3%였다.</p>

<p>그 뒤, multi-scale 모델중 최고 성능을 보이는 D와 E만 조합했더니 test error는 dense evaluation에서 7.0%이고 dense와 multi-crop을 합쳐서 평가하니 6.8%였다.</p>

<p>참고로 가장 좋은 single model 은 7.1%의 error를 보였다.</p>

<p><img src="/assets/img/Paper_Review/VGGNet/VGGNet4.png" alt="VGGNet4" /></p>

<h2 id="45-comparison-with-the-state-of-the-art">4.5 Comparison with the State-of-the-art</h2>

<p>마지막으로, 우리의 결과를 최신 기술과 비교해 보겠다.</p>

<p>ILSVRC-2014때 분류작업에서, VGG팀은 7개의 모델을 조합해서 7.3%의 test error를 나타내며 2등을 차지했다.</p>

<p>그 뒤, 우리는 D&amp;E의 조합으로 error를 6.8까지 낮췄다.</p>

<p>결과에서 보이듯 이전 세대(ILSVRC-2012,2013)의 최고 모델과 비교해 상당히 좋은 결과를 우리 very deep ConvNet이 달성했다.</p>

<p>우리의 결과는 분류작업 우승자인 GoogLeNet(6.7%)과 비교할 수 있을정도이고
실제로 ILSVRC-2013의 우승자 Clarifai(외부 학습 데이터로 11.2% 외부학습 데이터 없이 11.7%)를 능가했다.</p>

<p>ILSVRC에 제출된 대부분의 모델들보다 상당히 적은 양인 2개의 모델을 조합해서 얻은 최고의 결과라는 점에서 매우 놀랍다.</p>

<p>single-net performance를 보면, 우리의 구조가 최고의 결과를 달성한다(test error 7.0%), single GoogLeNet를 0.9% 능가하는 수치다.</p>

<p>참고로, 우리는 LeCun의 구조에서 많이 벗어나지 않고 실질적으로 깊이만 증가시켰다.</p>

<p><img src="/assets/img/Paper_Review/VGGNet/VGGNet5.png" alt="VGGNet5" /></p>

<h1 id="5-conclusion">5. Conclusion</h1>

<p>이번 연구에서 우리는 매우 깊은 신경망(최대 19의 가중치 층)을 대규모 이미지 분류를 통해 평가했다.</p>

<p>이것으로 표현깊이는 분류 정확도에 이점이 된다는 것과
ImageNet challenge dataset에서 최첨단 모델의 성능은 ConvNet구조의 실질적 깊이를 깊게 함으로써 얻을 수 있다는 것을 확인했다.</p>

<p>부록에서, 우리는 또한 우리의 모델이 광범위한 작업과 data set으로 잘 일반화되어 덜 깊은 구조로 구축된 더 복잡한 인식 파이프라인과 성능이 같거나 능가한다는 것을 보였다.</p>

<p>이 결과로 visual representation에서 depth의 중요성을 다시한번 확인했다.</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[VGGNet]]></summary></entry><entry><title type="html">AlexNet</title><link href="http://localhost:4000/paper%20review/2022/08/20/AlexNet.html" rel="alternate" type="text/html" title="AlexNet" /><published>2022-08-20T03:18:35+09:00</published><updated>2022-08-20T03:18:35+09:00</updated><id>http://localhost:4000/paper%20review/2022/08/20/AlexNet</id><content type="html" xml:base="http://localhost:4000/paper%20review/2022/08/20/AlexNet.html"><![CDATA[<h1 id="alexnet">AlexNet</h1>

<h1 id="imagenet-classification-with-deep-convolutional-neural-networks">ImageNet Classification with Deep Convolutional Neural Networks</h1>

<h1 id="1-introduction">1. Introduction</h1>

<p>최근 객체 인식은 기계학습 방법을 필수적으로 동반한다.</p>

<p>성능 개선을 위해 데이터를 더많이 모으고, 강력한 모델을 학습 시키고, 그리고 overfitting을 방지하기 위해 더 좋은 기술을 사용한다.</p>

<p>최근까지, 라벨된 이미지데이터는 수만개로 비교적 적은 양이었다.</p>

<p>이정도 양의 데이터 집합이라도 label-preserving transformation으로 증가된 상태라면 간단한 인식 문제가 꽤 잘 해결된다.</p>

<p>예를 들어 최근 숫자 인식에 대한 error rate는 인간의 성능과 비슷하다.</p>

<p>하지만 실제의 객체는 상당한 다양성을 보이기 때문에,
그들을 학습하기 위해서는 더 많은 학습 데이터가 필요하다.</p>

<p>사실, 작은 양의 데이터 셋의 단점은 널리 알려져있었지만, 이 문제에 대한 해결은 수백만개의 라벨링된 이미지 데이터셋을 최근(2012)에야 준비할 수 있었기 때문에 해결이 되었다.</p>

<p>새로운 큰 데이터셋에는 LabelMe와 ImageNet이 있다.</p>

<p>수백만 이미지를 통해 수천개의 객체를 학습하려면, large learning capacity인 모델이 필요하다.</p>

<p>그러나, 거대한 객체인식 작업의 복잡성은 ImageNet만틈 큰 데이터의 양으로도 해결할 수 없다,
그래서 모델이 가지고 있지 않은 데이터에 대해 올바른 loss값을 도출하기 위해서는 prior knowledge가 필요하다.</p>

<p>CNN의 capacity는 깊이와 크기를 다양하게 하여 조절할 수 있고 그들은 강력하고 대부분 정확한 가정들(고정적인 통계량과 지역적 픽셀 종속성)을 만들어낸다.</p>

<p>그러므로, 비슷한 크기의 층을 가진 표준 feedforward 신경망과 CNN을 비교하면 CNN의 이론적 최고의 성과는 약간 더 안좋은 반면 CNN은 더 적은 연결수와 매개변수의 수를 가지기 때문에 학습하기 더 쉽다.</p>

<p>CNN의 매력적인 성능과 local구조의 효율에도, 그들은 여전히 대규모 고해상도 이미지에 적용하기에는 비용적으로 매우 비싸다.</p>

<p>운좋게도, 2D 합성곱에 매우 최적화된 최근의(2012)GPU들은 매우 큰 CNN을 학습 시키기에 충분하고, ImageNet 같은 최근 데이터들은 과적합 없이 모델을 학습 시키기 위해서 충분한 학습 샘플을 가지고 있다.</p>

<p>이 논문의 주요 내용은 다음과 같다 :
지금까지(2012) 가장 큰 CNN중 하나를 ILSVRC2010과 2012에 사용된 ImageNet의 부분을 학습시키고 이 데이터에 대해 지금까지 보고된 내용중 최고의 결과를 얻었다.</p>

<p>2D 합성곱에 최적화된 GPU와 CNN을 학습 시키는 법을 기술했다.</p>

<p>성능을 개선하고 학습 시간을 줄이는 새롭고 평범하지 않은 feature들을 많이 사용한다.</p>

<p>이 네트워크의 크기는 120만개의 데이터 수로도 과적합을 일으키기 때문에, 과적합을 막기 위해 효과적인 방법을 사용했다.</p>

<p>5개의 합성곱층과 3개의 전결합층을 포함하고, 이 depth는 중요해 보인다 : 어떤 층을 빼는것이 성능 저하를 일으킨다는 것을 확인했다.</p>

<h1 id="2-the-dataset">2. The Dataset</h1>

<p>ImageNet은 22000개의 범주를 가지는 1500만개의 labeled high-resolution image를 가지고 있다.</p>

<p>이미지들은 웹과 Amazon’s Mechanical Turk crowd-sourcing tool을 통해 사람이 직접 라벨링 했따.</p>

<p>2010부터, Pascal Visual Object Challenge 중 하나인 ILSVRC가 열렸다. ILSVRC는 ImageNet의 부분인 1000개의 각 범주가 1000개 정도의 이미지를 가지는 데이터를 사용한다.</p>

<p>전체적으로, 120만개의 학습 이미지, 5만개의 validation 이미지와 15만개의 test 이미지로 구성된다.</p>

<p>ILSVRC-2010은 ILSVRC 중에서 test set label들을 사용할 수 있는 유일한 버전이기 때문에, 이번 실험에서 대부분 이 버전을 사용했다.</p>

<p>ILSVRC-2012에 참가했기 때문에 test set label은 사용할 수 없지만 이 버전에 대해서도 성능 결과를 볼것이다.</p>

<p>ImageNet은 관습적으로 두가지 error rate를 계산한다.
top-1과 top-5 error rate이다.</p>

<p>우리 시스템은 입력차원이 정해져있지만, ImageNet은 다양한 해상도의 이미지로 구성된다.</p>

<p>그러므로, 우리는 이미지들을 256x256 크기의 해상도로 전처리 한다.</p>

<p>직사각형 이미지가 주어진다면, 짧은 길이를 256으로 rescale하고 중심에 위치한 256x256 크기만큼 자를 것이다.</p>

<p>각 픽셀에 평균값을 빼는것을 제외하고 이미지에 다른 전처리는 안했다.</p>

<p>그래수 우리의 network는 RGB이미지의 중심을 학습했다.</p>

<h1 id="3-the-architecture">3. The Architecture</h1>

<p>구조는 8개의 학습층으로 5개의 합성곱층과 3개의 전결합 층으로 구성된다.</p>

<p>이제 네트워크 구조에서 사용된 새롭고 평범하지 않은 feature에 대해 설명한다.</p>

<p>제일 중요한것부터 순서대로 설명하겠다.</p>

<h2 id="31-relu-nonlinearity">3.1 ReLu Nonlinearity</h2>

<p>모델이 사용하는 표준적인 뉴런의 결과 함수는 tanh와 sigmoid이다.</p>

<p>경사하강법에서 학습 시간동안, 이러한 saturating nonlinearities들은 non-saturating nonlinearities보다 속도가 더 느려진다.</p>

<p>Nair와 Hinton에 따라 우리는 Rectified Linear Units(ReLUs)를 통해 non-saturating nonlinearities를 사용한다.</p>

\[f \mathsf{\ \ is\ \ non-saturating}\\
\mathsf{iff}\ \ (|\lim_{z\rightarrow-\infty}f(z)|=+\infty)\lor(|\lim_{z\rightarrow+\infty}f(z)|=+\infty)\\
\mathsf{others,\ }f\mathsf{\ is\ saturating}\]

<p>tanh와 비교해 ReLU를 사용한 Deep CNN은 몇배 더 빠르게 학습한다.</p>

<p><img src="/assets/img/Paper_Review/AlexNet/AlexNet0.png" alt="AlexNet0" /></p>

<p>A four-layer CNN with ReLUs(solid line) 가 CIFAR-10을 사용했을 때 학습에러 25%에 도달하기 까지 걸리는 시간은  tanh(dashed line)를 사용할 때보다 6배 빠르다.</p>

<p>위 결과에서 보이는 것과 같이, 이번 실험에서 큰 신경망에 기존에 사용하던 saturating neuron model을 사용했다면 실험을 할 수 없었을 것이다.</p>

<p>우리가 전통적인 neuron model을 바꾸려고 생각한 첫번째가 아니다.</p>

<p>적용할 데이터에 맞게 neuron model을 바꾼 사람들이 많았다.</p>

<p>하지만, 이 데이터셋에서 과적합을 방지를 최 우선으로 여겨서, 그들이 관찰한 가속화 능력은 ReLUs를 사용한 이 보고서와 맞지 않는다.</p>

<p>빠른 학습은 큰 데이터를 학습하는 큰 모델의 성능에 중요한 영향을 미친다.</p>

<h2 id="32-training-on-multiple-gpus">3.2 Training on Multiple GPUs</h2>

<p>GTX 580 GPU 한개는 3GB 메모리만 사용할 수 있고 이 메모리량은 네트워크의 최대 학습량을 제한한다.</p>

<p>120만개의 학습 샘플들은 하나의 GPU에서 학습하기에 너무 크다.</p>

<p>따라서, GPU를 2개를 사용한다.
최근(2012) GPU는 주메모리에 접근하지 않고 서로 직접 읽고 쓰는 능력이 있기 때문에 병렬연결 처리가 잘된다.</p>

<p>parallelization scheme은 뉴런의 절반을 각 GPU에 담는 것이다.(단, GPU간의 통신은 특정 층에서만 이루어진다.)</p>

<p>패턴의 연결을 고르는 것은 cross-validation에서 문제가 되지만, 이런 연결 방식은 계산량을 해결 가능한 수준이 되도록 튜닝하는것이 가능하게 한다.</p>

<p>이 구조는 column들이 종속적이라는것을 빼면Ciresan의 “columnar” CNN과 약간 비슷하다.</p>

<p>이 내용은 kernel을 반으로 줄인 네트워크를 하나의 GPU로 학습한 것과 비교하여 top-1과 top-5 error rate를 1.7%와 1.2%씩 낮춘다.</p>

<p>두 개의 GPU는 학습 시간이 하나일 때보다 약간 빠르다.</p>

<h2 id="33-local-response-normalization">3.3 Local Response Normalization</h2>

<p>ReLUs는 뉴런의 saturating을 막기위해 입력을 normalization하지 않아도 된다는 좋은 특성이 있다.</p>

<p>만약 어떤 학습 샘플이 양의 입력을 ReLU에 제공한다면, 학습은 해당 뉴런에서 진행될 것이다.</p>

<p>그러나, 우리는 local normalization이 generalization을 돕는다고 여긴다.</p>

<p>response-normalized는 다음과 같이 계산한다.</p>

\[b^i_{x,y}={a^i_{x,y}\over \left(k+\alpha\sum\limits^{\min(N-1,i+n/2)}_{j=\max(0,i-n/2)}(a^j_{x,y})^2\right)^\beta}\]

<p>kernel의 순서는 학습 전에 임의로 배정된다.</p>

<p>이런 reponse normalization은 실제 뉴런에서 영감을 받아 만들어져 다른 커널을 사용해서 얻은 뉴런 출력값들 중 큰 활성값들 사이에 경쟁을 만드는 lateral inhibition 기능을 한다.</p>

<p>$k,n,\alpha,\beta$ 는 validation set을 통해 결정되는 하이퍼파라미터들이다; 실험적으로</p>

<p>$k=2,n=5,\alpha=10^{-4},\beta=0.75$ 를 사용했다.</p>

<p>이 normalization을 ReLU를 적용한 이후에 적용했다.</p>

<p>이 내용은 local contrast normalization내용과 유사함을 보이지만, response-normalized는 평균값을 빼지 않아 “brightness normalization”이라 명명된다.</p>

<p>Response normalization은 top-1과 top-5 error rate를 각각 1.4%와 1.2%씩 낮춘다.</p>

<p>또한, CIFAR-10에 효과적인것을 확인했다 : 4개의 CNN에 normalization을 적용하면 11%의 test error가 나오고
without normalization인 경우는 13%의 test error가 나온다.</p>

<h2 id="34-overlapping-pooling">3.4 Overlapping Pooling</h2>

<p>CNN에서 Pooling layer는 같은 kernel map에서 이웃한 뉴런 그룹들의 결과를 요약해준다.</p>

<p>전통적으로, 인접한 pooling unit들로 인해 요약된 neighborhood 들은 overlap되지 않는다.</p>

<p>더 정확하게는 pooling unit 하나당 pooling layer는 zxz 크기의 neighborhood를 요약하는데 이 pooling unit이 grid의 형태로 가로와 세로가 각각 s 간격만큼 움직이는 층을 pooling layer라고 한다.</p>

<p>만약 s와 z를 같은 값으로 설정한다면, 우리는 CNN에서 옛날부터 쓰이는 전통적인 local pooling layer를 얻게 된다.</p>

<p>s&lt;z인 경우에는 overlapping pooling을 한다.</p>

<p>이 overlapping pooling이 우리 network에 $s=2,z=3$의 형태로 쓰인다.</p>

<p>이 내용은 top-1과 top-5 error rate를 각각 0.4%와 0.3%씩 줄인다. ($s=2,z=2$로 설정한 non overlapping pooling과 비교하여)</p>

<p>일반적으로 overlapping pooling을 사용하면 과적합이 좀 덜 일어나게 되는걸 관찰했다.</p>

<h2 id="35-overall-architecture">3.5 Overall Architecture</h2>

<p>전반적인 구조는 5개의 합성곱층과 3개의 전결합층으로</p>

<p>총 8개의 가중치를 갖는 층으로 이루어져있다.</p>

<p>마지막 전결합층의 결과는 1000-way softmax에 입력된다.</p>

<p>우리 network는 multinomial logistic regression을 최대화하는데 이것은 예측 결과에서 맞게 labeling한 log-probability를 모든 학습 case값들의 평균을 극대화하는 것과 같다.</p>

<p>2,4, 그리고 5번째 합성곱층은 같은 GPU에 속하는 이전층의 kernel map들과만 연결되어있다.</p>

<p>3번째 합성곱층의 kernel들은 2번째 층의 모든 kernel map들과 연결했다.</p>

<p>Response-normalization layer들은 첫번째와 두번째 합성곱층 뒤에 적용한다.</p>

<p>Max-pooling layer들은 두 response-normalization 층 뒤와 5번째 합성곱층 뒤에 위치시킨다.</p>

<p>ReLU non-linearity는 모든 합성곱층과 전결합층 뒤에 적용한다.</p>

<p>C1</p>

<p>227x227x3 — (96, 11, 11, 3), p=0, s=4 → 96x55x55</p>

<p>S2</p>

<p>96x55x55 —(3, 3), p=0, s=2 →96x27x27</p>

<p>C3</p>

<p>96x27x27 —(256, 5, 5, 96), p=2, s=1 → 256x27x27</p>

<p>S4</p>

<p>256x27x27 —(3,3), p=0, s=2 →256x13x13</p>

<p>C5</p>

<p>256x13x13 —(384, 3, 3, 256), p=1, s=1 →384x13x13</p>

<p>C6</p>

<p>384x13x13 —(384, 3, 3, 384), p=1, s=1 →384x13x13</p>

<p>C7</p>

<p>384x13x13 —(256, 3, 3, 384), p=1, s=1 →256x13x13</p>

<p>S8</p>

<p>256x13x13 —(3x3), p=0, s=2 → 256x6x6</p>

<p>Flatten : 256x6x6 = 9216</p>

<p>FC8</p>

<p>9216 → 4096</p>

<p>FC9</p>

<p>4096 → 4096</p>

<p>SOFTMAX10</p>

<p>4096 → 1000</p>

<h1 id="4-reducing-overfitting">4 Reducing Overfitting</h1>

<p>6000만개의 매개변수가 있다. (60Mb)</p>

<p>1000개의 class를 갖는 ILSVRC가 각 샘플에 이미지를 라벨링할 때 10bits로 제약을 준다 하더라도 과적합 없이 이 많은 매개변수를 학습시키기에는 공간이 부족하다.
labeling에만 75Mb</p>

<p>이제 과적합을 없애기 위한 두가지 주요 방법을 말하겠다.</p>

<h2 id="41-data-augmentation">4.1 Data Augmentation</h2>

<p>가장 쉽고 일반적으로 데이터를 통해 과적합을 줄이는 방법은 인공적으로 데이터를 label-preserving transformation을 통해 확대하는 것이다.</p>

<p>원본 이미지에 계산을 조금 적용해서 변형을 주기 때문에 디스크에 결과를 저장할 필요는 없다.</p>

<p>GPU로 학습을 하는 동안 CPU로 data augmentation을 하기 때문에 실질적으로 computationally free하다.</p>

<p>첫번째 data augmentation은 translation과 horizontal reflection이다. 256x256 크기의 이미지에서 227x227을 임의로 추출하여 reflection 한다.</p>

<p>이것은 데이터 샘플의 크기를 키워주고 데이터 샘플들은 interdependent해진다.</p>

<p>이런 과정없다면, 이 네트워크는 과적합을 일으켜 우리가 더 작은 네트워크를 사용하게 한다.</p>

<p>test 할 때에는, 227x227를 5장을 추출한다(4개의 코너와 중앙).
그리고 그것의 horizontal reflection까지 총 10장에 대해 예측을 하고 10개의 예측값의 평균을 구한다.</p>

<p>두번째 data augmentation은 학습 이미지에서 RGB채널들의 밝기값을 변경하는 것이다.</p>

<p>특히, 학습 이미지의 RGB픽셀값 집합에 PCA를 진행한다.</p>

<p>각 이미지에, eigenvalue에 비례하는 크기를  평균이 0이고 표준편차가 0.1인 가우시안 분포에서 random variable을 선택하여 곱하고 그렇게 만든 여러 주성분을 더한다.</p>

<p>그러므로 각 RGB 이미지 픽셀의 값이 변하게 된다.</p>

\[\begin{align*}
I_{xy}&amp;=\left[I^R_{xy},I^G_{xy},I^B_{xy}\right]^T\\
I'_{xy}&amp;=\left[\mathbf{p}_1,\mathbf{p}_2,\mathbf{p}_3\right]\left[\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3\right]^T
\end{align*}\]

<p>$\mathbf{p}_i, \lambda_i$ 는 i번째 eigenvector와 eigenvalue이다.</p>

<p>$\alpha_i$ 는 위에 말했던 random variable이고 한 이미지가 다시 학습되기 위해 들어올때까지 그 값을 유지한다.
이미지가 다시 학습되기 위해 입력되면 그 값이 다시 설정된다.</p>

<p>이 내용은 원본 이미지의 중요한 특성을 잡아낸다.</p>

<p>그 특성은 밝기값과 조명값에 대해 불변성을 가진다.</p>

<p>이 내용은 top-1과 top-5 error rate를 1% 줄인다.</p>

<h2 id="42-dropout">4.2 Dropout</h2>

<p>여러 종류의 모델들의 예측을 조합하는 것은 test error를 줄이는 매우 성공적인 방법이지만 큰 신경망의 학습 시간에 대한 비용이 매우 크다.</p>

<p>그러나 모델을 조합하는 매우 효율적인 방법이 있다.</p>

<p>“dropout”은 최근(2012) 소개된 기술로 각 은닉 뉴런의 결과를 0.5의 확률로 0의 값을 출력하는 것이다.</p>

<p>“dropped out”된 뉴런들은 forward pass와 back-propagation 진행시 참여하지 않는다.</p>

<p>그래서 입력을 표현할 때마다 신경망은 다른 architecture를 사용하고 모든 architecture들은 가중치를 공유한다.</p>

<p>이렇게 하면 뉴런이 다른 뉴런의 존재에 의존하지 않게 되므로 뉴런의 complex co-adaptation을 줄인다.</p>

<p>그러므로 다양한 뉴런들의 부분집합들과의 결합은 model이robust한 feature를 학습하게 한다.</p>

<p>test에서 우리는 모든 뉴런을 사용하지만 출력에 0.5를 곱하며, 이는 기하급수적으로 많은 dropout network에 의해 생성된 예측 분포의 기하학적 평균을 취하는 데 합리적인 근사치이다.</p>

<p>첫 두 전결합층에서 dropout을 사용한다.
dropout을 사용하지 않으면, 우리 네트워크는 상당한 과적합을 보일 것이다.</p>

<p>Dropout은 수렴에 필요한 반복수를 거의 두배로 만든다.</p>

<h1 id="5-details-of-learning">5. Details of learning</h1>

<p>모델 최적화에는 SGD를 사용하고 batch size 는 128이다
momentum은 0.9로 하고 weight decay는 0.0005로 설정 했다.</p>

<p>모델이 학습하기 위해 weight decay가 중요하다는 것을 발견했다.</p>

<p>다시말해, weight decay는 단지 regularizer가 아니라 model의 학습 error를 줄여준다는 것이다.</p>

<p>가중치 update 방식은 아래와 같다.</p>

\[\begin{align*}
v_{i+1}&amp;:=0.9\cdot v_i-0.0005\cdot\epsilon\cdot w_i-\epsilon\cdot\left\langle{\partial L\over\partial w}|_{w_i}\right\rangle_{D_i}\\
w_{i+1}&amp;:=w_i+v_{i+1}
\end{align*}\]

<p>각 층의 가중치를 평균이 0이고 표준편차가 0.01인 가우시안 분포를 따르도록 초기설정 했다.</p>

<p>2,4,5 번째 합성곱층과 전결합 층들의 편향을 1로 설정했다.</p>

<p>그리고 나머지 층의 편향은 0이다.</p>

<p>이런 초기 설정은 ReLUs에 양의 입력을 제공함으로써 학습의 초기 단계를 가속화 한다.</p>

<p>모든 층에 동일한 학습률을 적용했다.
이 학습률은 학습동안 수동으로 조정했다.</p>

<p>heuristic은 validation error가 개선되지 않는 경우 현재 학습률을 10으로 나눠서 적용한 것이다.</p>

<p>학습률은 0.01로 초기설정을 했고 종료되기 전에 3번 줄었다.</p>

<p>120만개의 학습 데이터를 90 epochs로 학습했더니 NVIDIA GTX 580 3GB GPU두개로 5~6일이 걸렸다.</p>

<h1 id="6-results">6. Results</h1>

<h2 id="61-qualitative-evalutaions">6.1 Qualitative Evalutaions</h2>

<p><img src="/assets/img/Paper_Review/AlexNet/AlexNet1.png" alt="AlexNet1" /></p>

<p>제한된 연결성을 가지는 구조로 인해 GPU1에 있는 kernel 들은 color-agnostic(색상에 구애받지 않는)하고</p>

<p>GPU2에 있는 kernel 들은 color-specific하다.</p>

<p>이 특징은 매 학습마다 발견되고 가중치 초기화와 관련이 없다.</p>

<p><img src="/assets/img/Paper_Review/AlexNet/AlexNet2.png" alt="AlexNet2" /></p>

<p>사진에서 객체가 중심에 없더라도 사진에서 mite처럼 network에 의해 검출이 될 수 있다.</p>

<p>대부분의 top-5 label들은 합리적이다.</p>

<p>예를들어, 레오파드의 top-5 label들은 고양이과의 다른 종류들이다.</p>

<p>grille과 cherry같은 경우는 사진에서 의도된 초점에 대한 진정한 모호함이 있다.</p>

<p><img src="/assets/img/Paper_Review/AlexNet/AlexNet3.png" alt="AlexNet3" /></p>

<p>네트워크의 visual knowledge를 살피는 다른 방법은 마지막 4096차원의 은닉층으로부터 유도된 feature activation을 생각하는 것이다.</p>

<p>만약 작은 Euclidean separation인 두개의 이미지가 feature activation vector에 입력되면, 높은 수준의 신경망이 이 사진들을 비슷하다고 여긴다고 말할 수 있다.</p>

<p>위 사진의 첫번째 column은 test set에서 고른 5개의 사진인고 row는 그 test 사진들을 query image로해서 Euclidean separation이 작은 이미지들을 training set에서 고른 것이다.</p>

<p>pixel 수준에서 선택된 training set의 사진들은 일반적으로 qeury image와 Euclidean separation이 L2만큼 가깝지 않다.</p>

<p>예를 들어 강아지와 코끼리를 보면 그들은 다양한 자세를 보이는것을 알 수 있다.</p>

<p>4096차원인 두 은닉층 사이의 Euclidean distance 계산할 때 실수값을 가지는 vector들은 비효율적이지만 이러한 vector를 짧은 binary 코드로 압축하도록 auto-encoder를 학습시킴으로써 이를 효율적으로 만들 수 있다.</p>

<p>이미지 label을 사용하지 않아서 의미상 같은 이미지이던 아니던 비슷한 edge 패턴을 가진 이미지를 도출하는 이 방법은 auto-encoder를 raw pixel에 적용해  이미지를 도출하는 방법보다 더 좋다.</p>

<h1 id="7-discusstion">7. Discusstion</h1>

<p>크고 깊은 합성곱 신경망은 매우 도전적인 데이터셋을 supervised learning 했을 때 좋은 결과를 보인다.</p>

<p>우리의 네트워크는 합성곱층을 하나 빼더라도 성능이 저하된다는 것을 알았다.</p>

<p>예를들어, 중간에 어느 층을 제거하는 것은 top-1 성능에 2% 저하를 가져온다.</p>

<p>따라서 깊이는 우리 결과를 만드는데 매우 중요하다.</p>

<p>실험을 단순화하기 위해, 특히 라벨이 붙은 데이터의 양을 크게 늘리지 않고 네트워크의 크기를 크게 늘리기에 충분한 계산 능력이 있는 경우에 도움이 될 것으로 예상되더라도 unsupervised 사전 훈련을 사용하지 않았습니다.</p>

<p>지금까지, 우리는 네트워크의 크기를 키우거나 더 오래 학습시키면 성능이 향상되는것을 알고있지만 인간의 시각 시스템 방법인 아래관자피질(Inferior temporal cortex,IT-cortex 또는 IT)과 대응 되려면 아직 해야하는 일들이 많다.</p>

<p>궁극적으로 우리는 정적인 이미지에서 누락된 부분이나 덜 명백한 매우 유용한 정보를 제공하는 시간적 구조인 video sequences에 크고 깊은 합성곱 네트워크를 사용하고 싶다.</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[AlexNet]]></summary></entry><entry><title type="html">LeNet-5</title><link href="http://localhost:4000/paper%20review/2022/08/20/LeNet-5.html" rel="alternate" type="text/html" title="LeNet-5" /><published>2022-08-20T02:50:58+09:00</published><updated>2022-08-20T02:50:58+09:00</updated><id>http://localhost:4000/paper%20review/2022/08/20/LeNet-5</id><content type="html" xml:base="http://localhost:4000/paper%20review/2022/08/20/LeNet-5.html"><![CDATA[<h1 id="lenet-5">LeNet-5</h1>

<h1 id="i-introduction">I. Introduction</h1>

<p>learning techniques의 사용은 최근 성공적인 패턴인식 적용에 중요한 요인이다.</p>

<p><img src="/assets/img/Paper_Review/LeNet-5/LeNet_0.png" alt="LeNet0" /></p>

<p>Feature Extractor( 특징 추출기 )는 사전지식이 많이 필요하고 진행중인 작업에 따라 다르다(ex&gt; A와 B작업을 위한 특징 추출기는 다르다)</p>

<p>특징 추출기는 거의 전체를 대부분 사람이 직접 설계하기 때문에 설계에서 많은 집중이 필요하게 된다.</p>

<p>반면, Classifier( 분류기 )는 종종 범용적이고 학습 가능하다.</p>

<p>때문에 주요 문제점중 하나는 설계자의 적절한 특징 set을 적용하는 능력은 인식 정확도에 큰 영향을 주는 것이다.</p>

<p>각 새로운 문제를 해결하려 할 때 매번 다시 특징 추출기를 새로 설계해야 한다면 매우 힘들 것이다.</p>

<p>learning techniques 가 쉽게 분리되도록 저차원 공간으로 한정된 분류기에 의해 사용되기 때문에 적절한 특징 추출기의 필요했었다.</p>

<p>세 요인들이 뭉치면서 지난 위와 같은 수십년동안의 관점이 바뀌었다.</p>

<ol>
  <li>
    <p>낮은 비용으로 수학적 계산이 빠른 컴퓨터를 사용</p>

    <p>때문에 알고리즘적 개선보다 무차별적 수치대입 방법에 의존하게 되었다.</p>
  </li>
  <li>
    <p>큰 시장과 폭넓은 관심이 있는 문제에 대한 대규모 데이터베이스의 가용성</p>

    <p>때문에 인식 시스템 설계에서 직접 만든 특징 추출기보다 실제 데이터에 의존하게 되었다.</p>
  </li>
  <li>
    <p>높은 차원의 입력 처리 능력과 큰 데이터를 통해 뒤얽힌 결정 함수를 만드는 능력을 가진 강력한 기계학습의 사용</p>
  </li>
</ol>

<p>때문에 음성과 필기 인식 시스템에 대한 정확도의 최근 진보는 학습 기술과 큰 학습 데이터에서 크게 기인되었다고 할 수 있다.</p>

<h2 id="a-larning-from-data">A. Larning from Data</h2>

<p>자동 기계학습에 대하 접근중 가장 성공적인 접근은 ‘numerical’ 또는 gradient-based learning이라 불린다.</p>

\[Y^p=F(Z^p,W)\\
E^p=\mathcal{D}(D^p,F(Z^p,W))\\
E_{train}(W)=Average(E^p)\]

<p>간단히 보면 학습은 $E_{train}(W)$을 낮추는 $W$를 찾는 것이지만</p>

<p>사실, 학습 데이터에서의 성능은 별로 중요하지 않다.</p>

<p>더 중요한 지표는 실제로 사용될 분야에서 그 시스템이 보이는 ‘오차율’이다.</p>

<p>이 ‘오차율’은 학습 데이터와 분리된 테스트 데이터에서의 정확도를 계산하여 평가한다.</p>

<p>테스트 데이터 오차율과 학습 데이터 오차율의 차이는 보통 다음과 같이 정의된다.</p>

\[E_{test}-E_{train}=k(h/P)^\alpha\]

<p>P가 학습 데이터 수, h는 ‘effective capacity’로 기계의 복잡도를 나타낸다.</p>

<p>따라서 학습 데이터 수가 많아지면 오차율 gap이 항상 줄고</p>

<p>그리고 h값을 높이면 $E_{train}$값이 줄지만 $E_{test}-E_{train}$값인 오차율의gap이 커진다.</p>

<p>대부분 학습 알고리즘들은 $E_{train}$와 $E_{test}-E_{train}$을 최소화 하려고 한다.</p>

<p>이러한 것을 공식적으로 Structural Risk Minimization(구조적 결함 최소화)라고 부르고</p>

<p>각 하위 집합이 이전 하위 집합의 상위 집합이 되도록 매개 변수 공간의 하위 집합을 만드는것 처럼 기계가 학습을 하도록 capacity를 높이는 시퀀스를 기반으로 정의한다.</p>

<p>실제로 구조적 결함 최소화는</p>

\[E_{train}+\beta H(W)\]

<p>를 최소화 시키도록 구현한다.</p>

<p>$H(W)$는 Regularization function(정규화 함수)이라 하고</p>

<p>$\beta$는 상수다</p>

<p>정규화 함수는 매개변수 공간의 하위집합중 high-capacity인 매개변수들 W에서 큰 값을 취하도록 선택된다.</p>

<p>H(W)의 최소화는 capacity가 접근 가능한 매개변수공간의 하위집합을 제한 한다.</p>

<p>그렇게 함으로써 학습 오차율과 두 오차율간의 차이의 tradeoff를 조절한다.</p>

<h2 id="b-gradient-based-learning">B. Gradient-Based Learning</h2>

<p>매개변수 집합을 사용하는 함수의 값을 최소화 하는 문제는 컴퓨터 과학의 많은 문제의 근원이다.</p>

<p>손실함숫값은 매개변수의 작은 변화가 손실함수에 끼치는 영향을 측정하여 최소화 한다.( 손실함수를 매개변수에 대해 미분한 기울기를 통해 손실함숫값을 측정 )</p>

<p>해석적 미분이 가능할 때 효율적인 학습 알고리즘을 고안할 수 있다.( 반대는 perturbation을 통해 수치적으로 한다 )</p>

<p>최소화하는 가장 간단한 방법은 Gradient Descent 알고리즘이다.</p>

\[W_k=W_{k-1}-\epsilon\dfrac{\partial E(W)}{\partial W}\]

<p>유명한 최소화 과정은 SGD(stochastic gradient descent, also called on-line update)이다.</p>

\[W_k=W_{k-1}-\epsilon\dfrac{\partial E^{p_k}(W)}{\partial W}\]

<p>SGD는 기울기 평균의 근사치를 통해 매개변수를 업데이트한다.</p>

<p>가장 흔한 경우로 매개변수는 단일 샘플을 기반으로 업데이트 된다.</p>

<p>때문에 W는 평균 궤도에서 변동되지만
대개는 빅데이터에서 일반적인 경사하강법과 second-order방법보다 상당히 빠르게 수렴한다.</p>

<h2 id="c-gradient-back-propagation">C. Gradient Back-Propagation</h2>

<p>경사 하강 학습은 1950년대 부터 사용 됐지만,</p>

<p>대부분 선형 시스템으로 제한되었다.</p>

<p>복잡한 기계학습 작업에 대한 경사하강법의 놀라운 유용성은 아래 세가지 사건이 발생할 때까지 널리 실현되지 않았었다.</p>

<ol>
  <li>극솟값에 대한 존재가 실제로는 큰 문제가 되지 않는 다는 것을 깨달음
    <ul>
      <li>사실 극솟값이 다층 신경망에서 문제가 되지 않는다는 것이 이론적으로 미스테리하다.</li>
      <li>It is conjectured that if the network is oversized for the task ( as is usually the case in practice ), the presence of “extra dimensions” in parameter space reduces the risk of unattainable regions.</li>
      <li>추측해보자면 네트워크는 실제 문제에서 필요한 경우보다 oversize되어있다면(대개 실제문제에서 그러함), 매개변수 공간에서 “extra dimensions”의 존재는 global minima에 도달하지 못하더라도 oversize된 parameter space에서 도달한 local minima가 tight한 parameter space에서 도달 해야하는 global minima일 확률이 올라가기 때문</li>
    </ul>
  </li>
  <li>비선형 시스템에서 간단하고 효율적으로 gradient를 구하기 위해 Back-Propagation이 대중화 되었다.</li>
  <li>sigmoid를 통해 다층신경망에 적용된 backpropagation이 복잡한 학습 문제를 해결한것이 증명되었기 때문이다.</li>
</ol>

<p>제어이론에서 Lagrange formalism이 back-propagation을 유도하는 최고의 방법이고 다시 나타나거나 새로 등장하는 네트워크를 유도할 때에도 좋다.</p>

<h2 id="d-learning-in-real-handwriting-recognition-systems">D. Learning in Real Handwriting Recognition Systems</h2>

<p>최고의 neural network인 Convolution Network는 연관있는 feature를 이미지 픽셀로부터 바로 추출 하도록 학습하게 디자인 되었다.</p>

<p>필기체 인식 문제중 가장 어려운 문제중 하나는 문자 하나하나를 인식하는것 뿐만 아니라, 단어나 문장으로부터 이웃한 문자들을 서루 분리하는 것도 어렵다. ( a process known as “segmentation”)</p>

<p>모델에서 시스템의 정확도는 “heuristic에 의해 segmentation된 quality”와 인식기의 문자나 여러 문자로 잘 분할되거나 잘 분할되지 못한 문자들을 “구별해내는 능력”에 달려있다.</p>

<p>잘못 분해된 문자들을 라벨된 데이터로 만들어야 하는 어려움 때문에 인식기가 잘 작동하도록 학습시키는 것은 큰 도전이 된다.</p>

<h2 id="e-globally-trainable-systems">E. Globally Trainable Systems</h2>

<p>대부분의 실제 패턴인식 시스템들은 multiple modules로 구성되어 있다.</p>

<p>ex&gt; 문서인식 시스템은</p>

<ul>
  <li>field locator( extract ROI )</li>
  <li>field segmentor</li>
  <li>recognizer</li>
  <li>contextual post-processor</li>
</ul>

<p>로 구성된다.</p>

<p>각 모듈을 수동적으로 최적화하고 또는 학습시킨다.</p>

<p>각 모듈들을 조립하여 시스템을 완성시킨 다음</p>

<p>각 모듈별로 성능을 최대로 올리기 위해 파라미터를 조정하는데 이것은 별로 좋지 않다.</p>

<p>더 좋은 방안은 global error를 최소화 하기 위해 전체 시스템을 한번에 학습시키는 것이다.</p>

<p>만약 성능을 측정하는 손실함수 E가 tunable한 파라미터W에 대해 미분가능하게 만들어진다면, Gradient-Based Learningdmf 통해 극솟값을 구할 수 있다.</p>

<p>global loss function이 미분 가능하게 하기 위해</p>

<p>전체적인 시스템을 미분 가능한 모듈들로 feed-forward 네트워크로 설계한다.</p>

\[\begin{align*}
Z^p&amp;=X_0\\
X_n&amp;=F(W_n,X_{n-1})\\
\dfrac{\partial E^p}{\partial W_n}&amp;=\dfrac{\partial F}{\partial W}(W_n,X_{n-1})\dfrac{\partial E^p}{\partial X_n}\\
\dfrac{\partial E^p}{\partial X_{n-1}}&amp;=\dfrac{\partial F}{\partial X}(W_n,X_{n-1})\dfrac{\partial E^p}{\partial X_n}
\end{align*}\]

<h1 id="ii-convolutional-neural-networks-for-isolated-character-recognition">II. Convolutional Neural Networks for Isolated Character Recognition</h1>

<p>복잡하고 다차원이고 비선형인 큰 데이터들을 경사 하강법으로 학습한 다층 네트워크의 능력은 이미지 인식 작업에 명백한 후보자가 된다.</p>

<p>CNN의 흥미로운 점은 특징 추출기가 스스로 학습한다는 것이다.</p>

<p>문자인식에서 전결합의 feed-forward 네트워크가 성공적으로 이루어지기위해 몇가지 문제점들을 해결해야 한다.</p>

<h3 id="1-일반적인-이미지는-수백가지의-픽셀들로-이루어져-있다">1. 일반적인 이미지는 수백가지의 픽셀들로 이루어져 있다.</h3>

<p>100개의 hidden unit을 가진 전결합층은 수만가지의 가중치를 갖게 된다.</p>

<p>매개변수의 증가는 모델의 capacity를 높이고 많은 양의 학습 데이터를 필요로 한다.</p>

<p>게다가 많은 가중치를 저장하기 위해 메모리 수요가 증가해 하드웨어의 능력을 뛰어넘을 수 있다.
translate나 loacl distortion 같은 변형을 FC Layer가 충분한 크기의 unit들로 해결할 수 있지만,</p>

<p>비슷한 패턴의 매개변수들이 여러 뉴런에서 확인될 것이다.</p>

<p>또한, 충분한 크기의 unit들을 제대로 학습 시키려면 많은 데이터도 필요하다.</p>

<p>하지만, CNN에서는 매개벼수 공간에서 가중치 구성의 복제를 억제함으로써 shift 불변성이 자동적으로 적용된다.</p>

<h3 id="2-전결합-구조의-결핍은-입력의-topology를-완전히-무시한다">2. 전결합 구조의 결핍은 입력의 topology를 완전히 무시한다.</h3>

<p>입력 변수는 학습에서 결과에 영향을 주지 않고 많은 순서로 입력될 수 있다.</p>

<p>하지만, 이미지나 time-frequency같은 음성 파일은 근처에 있으면 높은 correlation을 보인다.</p>

<p>Local Correlation은 local feature 추출하기에 좋다고 잘 알려져있다.</p>

<p>가까운 변수들은 edge나 corner같은 작은 범주에 구성원이 되기 때문이다.</p>

<p>CNN은 은닉 뉴런들의 receptive field를 지역적이 되도록 제한함으로써 local feature들을 추출한다.</p>

<h2 id="a-convolutional-networks">A. Convolutional Networks</h2>

<dl>
  <dt>Convolutional Netwrok는 shift, scale, distortion에 불변을 보장하기 위해 세가지 구조를 조합한다</dt>
  <dd>local receptive fields, shared weights( weight replication ), and spatial or temporal sub-sampling</dd>
</dl>

<p>각 층의 유닛들은 이전 층의 이웃된 작은 영역의 유닛들의 집합을 입력으로 받는다.</p>

<p>Local receptive fields에서 뉴런들은 기초적인 visual feature들을 추출한다.(ex&gt; edge, end-point, corner,…)</p>

<p>distortion이나 shift는 다양한 눈에 띄는 feature들을 발생시킨다.</p>

<p>그리고 이미지의 한 부분에 유용한 elementary feature detector들은 전체 이미지에 유용할 가능성이 있다.</p>

<p>이런 내용을 접목시키기 위해 이미지에서 다른 위치에 있는 receptive field들을 가진 유닛들의 집합이 고유의 가중치 벡터를 갖도록 한다.</p>

<p>한 층의 유닛들은 평면을 구성하고 그 안에 유닛들은 같은 가중치 집합을 공유한다.</p>

<p>이러한 유닛들로 만든 평면을 feature map이라 한다.</p>

<p>feature map의 모든 유닛들은 이미지에서 각각 다른 부분에 같은 연산을 하게 된다.</p>

<p>완성된 convolutional layer는 몇개의 feature map으로 구성되고 여러개의 feature map은 각 위치에서 여러개의 특징을 추출한다.</p>

<p>LeNet-5의 첫번째 층은 6개의 feature map을 가지고
각 feature map은 5x5형태로 25개의 입력값(receptive field)을 받는다.</p>

<p>따라서 25개의 입력을 받는 각 유닛은 25개의 학습가능한 계수와 하나의 편향을 갖는다.</p>

<p>convolutional layer의 흥미로운 점은 입력 이미지가 shift되면, feature map out도 같은 양 만큼 shift된다는 것이다.</p>

<p>이러한 convolutional network의 특성이 shift와 distortion에 robust하게되는 이유이다.</p>

<p>일단 특징이 감지되면, 그것의 정확한 위치는 덜 중요해진다.</p>

<p>그 특징과 연관된 다른 특징의 대략적인 위치가 더 중요하다.</p>

<p>각 특징들의 정확한 위치가 패턴을 구분짓는 것과는 연관이 없을 뿐 아니라,</p>

<p>다른 문자 예시에서 다양한 위치를 갖기 때문에 오히려 해롭다.</p>

<p>특징의 정확한 위치정보를 제거하기 위해 sub-sampling layer를 사용한다.</p>

<p>sub-sampling layer를 사용하면 shift와 distortion에 대한 민감성도 줄어든다.</p>

<p>LeNet-5의 두번째 층은 sub-sampling layer이다.</p>

<p>2x2의 receptive field에서 평균을 취하고 편향을 더한다.</p>

<p>결과적으로 크기가 이전 층보다 반으로 줄어든다.</p>

<p>sub-sampling에서 가중치 파라미터가 작으면, 준선형 모드의 작동을 하고</p>

<p>단지 입력을 blur처리할 뿐이다.</p>

<p>sub-sampling에서 가중치 파라미터가 크면, 편향에 따라 “noisy OR” 또는 “noisy AND” 함수 기능을 수행한다.</p>

<p>convolution과 sub-sampling을 연달아 두면 “bi-pyramid” 결과를 만든다.</p>

<p>feature map은 늘어나고 spatial resolution은 줄어든다.</p>

<p>weight sharing 기술의 흥미로운 부가 효과는 자유 매개변수를 줄인다는 점이다.</p>

<p>자유매개변수가 줄면 기계의 capacity도 줄고 test error와 training error의 차이도 줄어든다.</p>

<h2 id="b-lenet-5">B. LeNet-5</h2>

<p>LeNet은 입력층 빼고 가중치 있는 7층의 구조로 되어있다.</p>

<p>입력은 32x32 pixel 이미지이다.(데이터베이스에서 가장 큰 문자의 크기보다 충분히 큰 사이즈이다.)</p>

<p>highest-level feature detector에서 보는 receptive field의 중앙에
stroke end-point나 corner같은 잠재적 특징이 될만한 것들이 나타나기 때문이다.</p>

<p>마지막 convolutional layer(C3)의 receptive field들의 중앙은 32x32인 입력의 중앙에 20x20인 영역을 만든다.</p>

<p>입력값은 normalized를 통해 -0.1~1.175로 변형해서 거의 평균이 0, 분산이 1로 만들어 학습을 가속화 했다.</p>

<h3 id="c1">C1</h3>
<p>32x32 → feature maps 6@28x28</p>

<p>5x5 크기의 필터 6개를 통해 연산을 하고 input이 32x32이므로</p>

<p>padding=0, stride=1에 대한 결과로</p>

<p>feature maps 6@28x28이 나온다.</p>

<p>가중치 수 : $(필터크기+편향)\times특징맵수 = 156$</p>

<p>연결 수 : $(필터크기+편향)\times convolution연산수\times 특징맵수=122304$</p>

<h3 id="s2">S2</h3>
<p>feature maps 6@28x28 → feature maps 6@14x14</p>

<p>14x14크기인 6개의 feature map으로 출력된다.</p>

<p>2x2의 입력을 받고 4개의 값을 더하고 학습가능한 계수를 곱하고 편향을 더한다.</p>

<p>그 결과는 sigmoidal 함수로 전해진다.</p>

<p>각 receptive field는 겹치는 부분이 없기 때문에 C1의 출력을 입력으로 받은 S2의 출력은 C1의 출력의 반절이 된다.</p>

<p>가중치 수 : 12 = (1+1)*6</p>

<p>연결 수 : 5880 = (3+1+1)<em>14</em>14*6</p>

<h3 id="c3">C3</h3>
<p>feature maps 6@14x14 → feature maps 16@10x10</p>

<p><img src="/assets/img/Paper_Review/LeNet-5/LeNet_1.png" alt="LeNet1" /></p>

<p>S2와 C3의 연결을 보면 S2의 모든 feature map이 C3의 모든 feature map에 연결되지 않는다.</p>

<p>첫 번째 이유</p>

<ul>
  <li>완전한 연결을 하지 않으면 연결의 수를 합리적인 범위로 제한할 수 있다.</li>
  <li>더 중요한 것은 균형을 깬다는 것이다.
균형이 깨지면 서로 다른 입력을 받게 될 것이고 다른 특징들의 조합으로 서로 다른 상향된 특징을 얻을 수 있기 때문이다.</li>
</ul>

<p>가중치 수 : 1516</p>

<p>연결 수 : 151600</p>

<h3 id="s4">S4</h3>
<p>feature maps 16@10x10 → feature maps 16@5x5</p>

<p>가중치 수 : 32 = (1+1)*16</p>

<p>연결 수 : 2000 = (3+1+1)<em>25</em>16</p>

<h3 id="c5">C5</h3>
<p>feature maps 16@5x5 → feature maps 120@1x1</p>

<p>필터의 크기가 그대로 5x5이고 feature map은 120개 이다.</p>

<p>따라서 출력의 모양이 120개의 feature map이고 그 크기가 1x1이 된다.</p>

<p>C5는 C3와 다르게 이전 층(S4)과 완전 연결을 한다.</p>

<p>이런 결과를 얻게 되는데도 C5가 Fully Connected Layer가 아니라 convolutional layer인 이유는
입력의 크기가 더 커지게 되면 C5의 결과는 1x1보다 커지기 때문이다.</p>

<p>가중치 수 : 48120 = (25<em>16+1)</em>120</p>

<p>연결 수 : 48120 = (25<em>16+1)</em>120</p>

<h3 id="f6">F6</h3>
<p>120(feature maps 120@1x1) → 84</p>

<p>C5와 전결합을 이루는 84개의 유닛을 가진 층</p>

<p>가중치 수 : 10164 = (120+1)*84</p>

<p>연결 수 : 10164 = (120+1)*84</p>

<p>전통적인 신경망처럼, F6층까지는 입력벡터와 가중치벡터의 dot연산을 하고 bias를 더했다.</p>

<p>그리고 그 결과를 sigmoid squashing function에 입력했다.</p>

\[x_i=f(a_i)\\
f(a)=A\ tanh(Sa)\]

<p>sigmoid squashing function으로 scaled hyperbolic tangent함수를 사용했다.</p>

<p>A는 함수의 amplitude이고 S는 원점에서의 경사를 결정한다.</p>

<p>따라서 함수 f는 +A와 -A에 수평 점근선이 있는 기함수이다.</p>

<p>A는 1.7159로 했다.</p>

<h3 id="output">output</h3>

<p>출력층은 Euclidean Radial Basis Function (RBF)으로 구성된다.</p>

\[y_i=\sum_j(x_j-w_{ij})^2\]

<p>입력 패턴과 RBF와 연관된 class의 적합도를 측정한다.</p>

<p>값이 작을수록 관련있는 class</p>

<p>RBF는 분류할 클래스 수가 많아지더라도 잘 작동한다.</p>

<p>sigmoid와 달리 RBF는 원하는 특징이 아닌 영역을 보더라도 계산을 수행하기 때문에 목적이 아닌 class를 거부하는 용도로도 쓸 수 있다.</p>

<p>가중치의 범위를 1과 -1로만 구성하게 되면
sigmoid가 치솟는것을 막는데
그렇게 되면 손실함수가 천천히 수렴하는것을 방지한다.</p>

<h2 id="c-loss-function">C. Loss Function</h2>

<p>LeNet-5에는 Maximum Likelihood Estimation criterion(MLE)가 사용될 수 있고
위에서는 Minimum Mean Squared Error( MSE )를 사용했다.</p>

\[E(W)=\dfrac{1}{P}\sum^P_{p=1}y_{D^p}(Z^p,W)\]

<p>$y_{D^p}$ 는 $D_p$번째 RBF 결과이다.</p>

<p>이 손실 함수는 대부분의 경우에 적절하지만,
중요한 3가지가 빠져있다.</p>

<ol>
  <li>RBF 매개변수를 adaptive하게 한다면, E(W)는 trivial solution을 갖는다 (Collapsing Effect)
하지만 이 문제는 RBF를 adaptive하게 안하면 일어나지 않게 해결할 수 있다.</li>
  <li>
    <p>competition이 없는 것이 두번째 문제이다.
더 차별적인 학습 기준에서 경쟁이 있을 수 있는데,ex&gt; MAP(maximum a posteriori) criterion,
경쟁은 입력 이미지가 분류된 클래스이거나 어디에도 분류가 안되는 클래스일 수가 있다는 점에서 올바른 클래스일 확률을 최대화하는 것과 같다.
penalty의 관점에서 보면, MSE는 올바른 클래스의 penalty를 작게 했다면 아래는 잘못된 클래스의 penalty를 끌어 올린다.</p>

\[E(W)=\dfrac{1}{P}\sum^P_{p=1}(y_{D^p}(Z^p,W)+\log(e^{-j}+\sum_ie^{-y_i(Z^p,W)}))\]

    <p>log항의 -값이 “competitive” 역할을 한다.</p>

    <p>그 값은 손실 함수가 양수이기 위해 반드시 첫번째 항보다 작거가 같아야한다.</p>

    <p>상수 j는 양수이고 class의 penalty가 이미 큰것을 더 커지지 않게 막는다.</p>

    <p>쓰레기 클래쓰의 posterior probability는 $e^{-j}$와 $e^{-j}+\sum\limits_ie^{-y_i(Z^p,W)}$의 비율이 될것이다.</p>

    <p>이 차별적인 기준은 RBF의 중심을 서로 떨어져 유지 시킴으로써 “collapsing effect”를 막는다.</p>
  </li>
</ol>

<h1 id="iii-results-and-comparison-with-other-methods">III. Results and Comparison with other methods</h1>

<p>개별 숫자를 인식하는 것은 실용적인 인식 시스템에 속한 많은 문제들중 하나지만, 모양을 비교하는 인식 방법의 성능을 아는데에는 훌륭한 기준이다.</p>

<p>많은 방법들이 직접 만든 특징 추출기와 학습 가능한 분류기의 조합으로 구성되지만, 이 논문은 normalized된 크기의 이미지에 바로 사용할 수 있는 적용가능한 방법에 집중한다.</p>

<h2 id="a-database-the-modified-nist-set">A. Database: the Modified NIST set</h2>

<p>NIST’s Special Database 3와 Special Database 1을 사용한다.</p>

<p>SD3는 학습데이터이고 SD1은 시험데이터이고 SD3가 SD1보다 clean 한다.</p>

<p>SD3는 Census Bureau(미국 인구조사국)에서 얻은 것이고
SD1은 고등학교 학생들로 부터 얻었기 때문이다.</p>

<p>학습 경험으로부터 현명한 결론을 도출하려면 전체 샘플에서 학습과 시험 데이터중 어느 것을 선택했는지와 무관하게 결과가 나와야 한다. ( 따라서 둘을 섞는다 )</p>

<p>SD1에서 첫 250명의 데이터는 학습데이터로
SD1에서 나머지 250명의 데이터는 테스트데이터로
각 데이터가 약 30000개가 되었고 SD3를 통해 각 데이터를 60000개로 만들어 줬다.</p>

<p>실험에는 10000개의 테스트데이터만 사용한다.(SD1과 SD3로부터 5000개씩)
그리고 학습데이터는 60000개 모두 사용한다.</p>

<p>세가지의 데이터베이스가 사용되었다.</p>

<ol>
  <li>regular database
28x28 사이즈이고 가운데에 위치하게 함
어떤 경우에는 배경과 함께 32x32인 것도 있다.</li>
  <li>deslanted database
deslanting과 cropped down으로 20x20으로 함
deslanting은 pixel inertia의 second moment를 계산(counting a foreground as 1 and a background as 0)
그리고 line의 shear 변환을 통해 principal axis가 수직을 이루게 한다.</li>
  <li>옛날에 썼던 데이터로 크기가 16x16이다.</li>
</ol>

<h2 id="b-results">B. Results</h2>

<p>LeNet-5는 regular database를 사용
전체 학습데이터 학습을 20번 수행
학습률 → ~2 : 0.0005 → ~5 : 0.0002 → ~8 : 0.0001 → ~12 : 0.00005 → ~20 : 0.00001</p>

<p>각 iteration전에 diagonal Hessian 은 500개 샘플로 재평가하고 iteration 끝날때까지 유지</p>

<p>매개변수들 사이에서 효과적인 학습률은 0.00007~0.016 사이이다.</p>

<p>10 iter까지 test error는 0.95%이고</p>

<p>19 iter후에는 0.35%였다.</p>

<p>많은 저자들이 NN을 다른 adaptive 알고리즘을 다양한 작업에서 학습 시키는 경우 over-training 현상을 관찰했다.</p>

<p>over-training이 발생하면, training error는 계속 감소하고 test error는 작아지다가 커진다.</p>

<p>이런 현상이 일반적이지만, LeNet-5에서는 볼 수 없었다.</p>

<p>가능한 이유로는 학습률이 비교적 컸기 때문이라고 할 수 있다.</p>

<p>가중치가 local minima에 수렴하지 않고 무작위로 진동하는 것이 over-training의 현상이다.</p>

<p>이러한 진동 때문에 그 평균 비용은 broader minimum에서 더 낮아지기 때문이다.</p>

<p>그래서, Stochastic gradient는 regularization에서 broader minima를 선호하는 비슷한 현상을 보일것이다.</p>

<p>Broader minima는 generalization error가 높아지는 매개변수 분포의 entropy가 커지는 경우에 solution과 일치한다.</p>

<p>학습데이터의 수가 많아지면 정확도가 향상된다.</p>

<p>왜곡된 데이터를 학습한 네트워크는 20번의 iter동안 두번만 각 개별 샘플을 효과적으로 본다는 것이 흥미롭다.</p>

<p>비록 나타나지 않았던 형태로 쓰여졌지만 어떤 것은 정말 애매하더라도 어떤 것은 사람이 완벽하게 분류할 수 있다.</p>

<p>때문에 더 많은 학습 데이터를 통해 개선 가능하다고 기대된다.</p>

<h2 id="c-comparison-with-other-classifiers">C. Comparison with Other Classifiers</h2>

<h3 id="c1-linear-classifer-and-pairwise-linear-classifier">C.1 Linear Classifer, and Pairwise Linear Classifier</h3>

\[\mathsf{Linear&lt;[deslant]Linear&lt;Pairwise\ Linear}\]

<h3 id="c2-baseline-nearest-neighbor-classifier">C.2 Baseline Nearest Neighbor Classifier</h3>

<p>K-nearest neighbor 분류기는 입력 이미지 사이에 Euclidean 거리를 측정한다.</p>

<p>이것의 이점은 학습시간이 필요없고, 설계자의 뇌가 필요없다는 것이다.</p>

<p>하지만, 메모리 수요가 크고 인식하는데 시간이 오래 걸린다.</p>

<p>1pixel = 1 byte → 20x20 pixel = 400byte → 60000x(20x20) pixel = 24000000byte = 24Megabyte</p>

<p>24Megabyte가 run time에서 사용가능해야함</p>

<p>많은 표현이 들어가면 error rate가 조금 올라갈 수 있다.</p>

<p>원래 실제 유클리드 거리 NN시스템은 특징 벡터들 사이의 연산을 하지만, 이 논문에 모든 시스템들은 픽셀들 사이에 연산을 하기 때문에 이 결과는 baseline comparison에 유용하다.</p>

<h3 id="c3-principal-component-analysis-pca-and-polynomial-classifier">C.3 Principal Component Analysis (PCA) and Polynomial Classifier</h3>

<p>40개의 principal component를 사용</p>

<p>821개의 입력을 받는 선형 분류기로 보일수 있다.</p>

<h3 id="c4-radial-basis-function-network">C.4 Radial Basis Function Network</h3>

<p>첫 층은 28x28의 입력을 받는 1000개 가우시안 RBF유닛이다.</p>

<p>두번째 층은 1000개의 입력을 받고 10개를 출력하는 선형 분류기이다.</p>

<p>RBF 유닛들은 100개씩 10개의 그룹으로 나뉘고 K-means알고리즘을 통해 각 그룹은 학습을 한다.</p>

<p>두번째 층에서 가중치들은 pseudo-inverse 방법으로 regularized를 통해 계산된다.</p>

<h3 id="c5-one-hidden-layer-fully-connected-multilayer-neural-network">C.5 One-Hidden Layer Fully Connected Multilayer Neural Network</h3>

<p>또다른 분류기는 하나의 전결합층이 은닉층 전부이고
하나의 출력층을 갖는 역전파 알고리즘을 이용한 다층 신경망이다.</p>

<p>regular test data를 사용하면 300개의 은닉뉴런을 사용할 때가 1000개의 은닉뉴런을 사용할 때보다 test error가 높았다.</p>

<p>distortion이 적용된 데이터로 학습하면 전체적으로 개선이 되고
300개의 은닉뉴런일 때가 1000개의 은닉뉴런일 때보다 test error가 작았다.</p>

<p>deslanted data를 학습하면 300개의 은닉뉴런일 때 test error가 1.6%까지 떨어졌다.</p>

<p>자유 매개변수가 많을 수록  test error가 낮게 나온다.</p>

<p>→ self-regularization을 한다고 추측한다. 가중치 공간에서 원점은 saddle point이기 때문에 어느 방향으로든 SGD가 보기에는 매력적으로 보이기 때문이다. 첫 몇번에 epoch에서만 가중치가 줄지 않음.</p>

<p>작은 가중치는 sigmoid를 준선형으로 작동하게 하여 network는 low-capacity가 되어 마치 sigle layer network처럼 된다.</p>

<h3 id="c6-two-hidden-layer-fully-connected-multilayer-neural-network">C.6 Two-Hidden Layer Fully Connected Multilayer Neural Network</h3>

<p>이론상 어떤 기능도 하나의 은닉층 신경망으로 근사화될 수 있다.</p>

<p>그러나, 두 은닉층 신경망이 가끔 실제 상황에서 더 좋은 성능을 보였다.</p>

<p>여기서도 그 현상을 관찰했는데, 28x28-300-100-10 network에서 test error가 3.05%로 은닉층이 하나인 경우보다 뛰어났다.</p>

<p>network 크기를 28x28-1000-150-10으로 늘린 경우에는 조금 개선된 test error가 2.95% 였다.
28x28-300-100-10가 training with distorted pattern으로 된 경우 약간 개선된 2.50%다.</p>

<p>그리고 크기를 키워서 28x28-1000-150-10일 때는 2.45%였다.</p>

<h3 id="c7-a-small-convolutional-network-lenet-1">C.7 A Small Convolutional Network: LeNet-1</h3>

<p>Convolutional network는 딜레마를 풀려고 한다.</p>

<p>작은 network는 학습을 할 수 없고 큰 network는 매개변수가 너무 많은 딜레마</p>

<p>이미지는 16x16으로 줄이고 28x28크기의 가운데에 위치 시켰다.</p>

<p>100000번의 계산이 필요하지만 convolution의 특징으로 자유 매개변수는 2600개 뿐이다.</p>

<p>LeNet-1은 USPS 데이터 형식에 만들어졌고 입력을 위해 다시 조절했다.</p>

<p>test error는 1.7%가 나왔다.</p>

<p>적은 수의 매개변수로 error가 작게 나오는 신경망은 그 구조가 적용될 작업에 적합하다는 것이다.</p>

<h3 id="c8-lenet-4">C.8 LeNet-4</h3>

<p>LeNet-1에서 convolutional network가 크면 큰 사이즈의 학습 데이터에 적합해진다고 보았다.</p>

<p>LeNet-4와 LeNet-5는 이 문제를 해결하기 위해 고안되었다.</p>

<p>LeNet-4는</p>

<p>Conv1(4)→Sub2(8)→Conv2(16)→Sub3(16)→FC4(120)→output</p>

<p>free parameters : 17000</p>

<p>connections : 260000</p>

<p>test error : 1.1%</p>

<p>실험에서 분류기는 유클리드 NN 분류기를 사용했다.</p>

<h3 id="c9-boosted-lenet-4">C.9 Boosted LeNet-4</h3>

<p>여러개의 분류기를 조합해 “boosting” 방법을 만들었다.</p>

<p>3개의 LeNet-4를 조합했다</p>

<ol>
  <li>하나는 일반적 방법으로 학습시켰고</li>
  <li>두 번째는 첫번째 네트워크의 분류 결과를 가지고 학습했다.</li>
  <li>세 번째 네트워크는 앞에 네트워크들과 일치하지 않는 새로운 패턴에 대해 훈련했다.</li>
</ol>

<p>test하는 도안, 세개의 네트워크의 결과를 단순히 더했다.</p>

<p>왜냐하면 LeNet-4의 error는 괸장히 작기 때문이다.</p>

<p>그리고 두 번째와 세 번째 네트워크를 학습 시키기 위해 인공적으로 왜곡시킨 학습 데이터를 사용했다.</p>

<p>test error : 0.7 %</p>

<p>boosing이 3배 더 비용이 들어 보이지만
사실, 첫 번재 네트워크가 정답에 높은 자신을 보이면 두 번째와 세 번째 네트워크는 호출되지 않는다.</p>

<p>따라서 계산 비용은 LeNet-4가 하나인 경우보다 boosting에 경우 1.75배만 증가했다.</p>

<h3 id="c10-tangent-distance-classifier--tdc-">C.10 Tangent Distance Classifier ( TDC )</h3>

<p>TDC는 입력 이미지의 작은 왜곡과 이동에 민감한 거리측정 함수가 사용된 nearest-neighbor 방법이다.</p>

<p>만약 고차원 공간에서 이미지를 점 하나로 생각할 때,
문자의 왜곡은 픽셀 공간에서 곡선을 따라간다.</p>

<p>같이 취해진 이 모든 왜곡들은 픽셀 공간에서 낮은 차원의 다양성을 정의한다.</p>

<p>원본 이미지에 가까운 작은 왜곡들로인한 다양성은 tangent plane으로 추측될 수 있다.</p>

<p>여러 왜곡들을 사용해 만든 tangent plane들 과의 거리를 통해 “closeness”를 측정한다.</p>

<p>test error : 1.1% (입력 16x16)</p>

<p>유클리드 거리를 통해 여러 결과들을 사전에 필터링하면 tangent 거리 계산을 줄일 수 있다.</p>

<h3 id="c11-support-vector-machine-svm">C.11 Support Vector Machine (SVM)</h3>

<p>Polynomial 분류기들은 복잡한 결정 평면을 만들 때 잘 학습되는 방법이다.</p>

<p>하지만, 제공되는 항의 수가 제한적이라 고차원 문제를 풀기에는 실용적이지 못하다.</p>

<p>SVM은 고차원 공간에서 다항식이나 많은 평면을 포함하는 복잡한 평면을 나타내는 매우 경제적인 방법이다.</p>

<p>regular SVM test error : 1.4%</p>

<p>Cortes and Vapnik test error : 1.1%</p>

<p>계산 비용이 매우 높다.</p>

<p>V-SVM test error : 1.0%</p>

<p>modified V-SVM test error : 0.8%</p>

<p>하지만, V-SVM은 regular SVM의 2배 만큼 비용이 높다.</p>

<p>Burges는 Reduced Set Support Vector technique(RS-SVM)을 제안하고 1.1%의 test error를 얻었고</p>

<p>계산 비용은 LeNet-5보다 60%만큼만 높았다.</p>

<h3 id="d-discussion">D. Discussion</h3>

<p>Boosted LeNet-4의 성능이 제일 좋고 그 다음이 LeNet-5이다.</p>

<p>신경망은 memory 기반 방법들보다 계산 수가 적다.</p>

<p>일반적인 구조와 그들의 적은 메모리 수요 때문에 CNN들은 하드웨어 구현에 특히 적합하다.</p>

<p>LeNet-5이전 analog와 digital을 섞어서 구현한 네트워크는 초당 1000자를 넘는 속도로 계산해왔다.</p>

<p>그러나, 컴퓨터 주요 기술의 급진적 진보로 인해 섞어서 만든 네트워크들을 쓸모없게 만들었다.</p>

<p>메모리 기반 기술들은 메모리와 컴퓨팅에 대한 수요가 커서 비용 효과적 구현에서는 피한다.</p>

<p>KNN과 TDC는 학습 시간이 없다고
단층 네트워크, pairwise 네트워크, PCA+quadratic 네트워크 1시간 내로 학습하고
다층 네트워크는 더 오래 걸린다.</p>

<p>CPU로 LeNet-5를 학습시키는데에는 2~3일이 걸렸다.</p>

<p>학습 시간은 설계자가 고려할 사항이고 최종 시스템 사용자와는 별로 관계가 없다는 것을 알아야 한다.</p>

<p>현존하는 기술과 그것보다 학습시간이 늘고 조금 상향된 정확도의 새로운 기술중에 최종 사용자는 새로운 기술을 택할 것이다.</p>

<p>대부분의 방법들은 적절한 성능을 위해 1byte만 필요하지만 Nearest-Neighbor방법은 pixel당 4bits만 필요하다.</p>

<p>게다가, 신경망은 메모리 기반 방법보다 더 적은 메모리를 수요한다.</p>

<p>전반적인 성능은 많은 요인에 기인한다(정확도, 실행시간,메모리 수요).</p>

<p>컴퓨터 기술이 상향되면, larger-capacity recognizers도 가능해진다.</p>

<p>larger recognizer는 더 큰 학습 데이터를 필요로 한다.</p>

<p>LeNet-1은 1989에 사용하기 적절했고 LeNet-5는 1998에 사용하기 적절하다.</p>

<p>1989년도에는 LeNet-5 정도의 복잡한 인식기는 학습하는데 몇주가 걸리고 더 많은 데이터가 필요했기 때문에 사용하려고 생각하지도 않았다.</p>

<p>boosting이 메모리와 계산비용에 적당한 증가를 주지만 많은 개선력을 준다는걸 알았다.</p>

<p>도한, 왜곡을 통해 데이터의 양을 효과적으로 늘려서 실제로 많은 데이터를 모아야 될 필요가 없어졌다.</p>

<p>SVM은 훌륭한 정확도를 갖지만 사용에 대한 비용이 높아 Reduced된 형태로 사용했더니 CNN의 error rate와 비슷했다.</p>

<p>신경망은 메모리 기반 방법보다 더 빠르고 더 적은 공간을 필요로하기 때문에 데이터의 사이즈가 커질수록 더 뛰어난 성능을 보인다.</p>

<h3 id="e-invariance-and-noise-resistance">E. Invariance and Noise Resistance</h3>

<p>convolutional network는 특히 다양한 사이즈, 위치, 방향, 사람에 의한 분할같은 형태를 거부하거나 인식하는데 적합하다.</p>

<p>글자 분할기는 완벽하지않아서 기울어지거나 중심에 있지 않거나 크기가 다르거나 할 수 있어서 분류기의 invariance는 중요하다.</p>

<p>완벽한 불변성을 가지는 분류기를 만드는 목표는 해결하기 어렵지만
CNN은 그 문제에 대한 어느정도 해답이 된다.</p>]]></content><author><name>Chang Hun Kang</name></author><category term="[&quot;Paper Review&quot;]" /><summary type="html"><![CDATA[LeNet-5]]></summary></entry></feed>