<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Ch5 오차역전파법 | No Free Knowledge</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Ch5 오차역전파법" />
<meta name="author" content="Chang Hun Kang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Ch5 오차역전파법" />
<meta property="og:description" content="Ch5 오차역전파법" />
<link rel="canonical" href="http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html" />
<meta property="og:url" content="http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html" />
<meta property="og:site_name" content="No Free Knowledge" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-21T14:55:27+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Ch5 오차역전파법" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Chang Hun Kang"},"dateModified":"2022-08-21T14:55:27+09:00","datePublished":"2022-08-21T14:55:27+09:00","description":"Ch5 오차역전파법","headline":"Ch5 오차역전파법","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html"},"url":"http://localhost:4000/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="No Free Knowledge" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">No Free Knowledge</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>
        
        <a class="page-link" href="">Jump to</a>

        <!-- <div class="trigger"><a class="page-link" href="/categories/DeepLearning%20from%20scratch.html">DeepLearning from scratch</a><a class="page-link" href="/categories/Math%20Note.html">Math Note</a><a class="page-link" href="/categories/Paper%20Review.html">Paper Review</a><a class="page-link" href="/categories/Project.html">Project</a><a class="page-link" href="/categories/RNN%20Note.html">RNN Note</a><a class="page-link" href="/">All Documents</a></div> -->
      </nav></div>
</header>
<div class="container">
      <div>
        


<div class="category_box">
    <h2> CATEGORY </h2>
    <ul>
          
                
                <li><a href=" http://localhost:4000/categories/DeepLearning from scratch "> DeepLearning from scratch     </a></li>
                <br/>
          
                
                <li><a href=" http://localhost:4000/categories/Math Note "> Math Note     </a></li>
                <br/>
          
                
                <li><a href=" http://localhost:4000/categories/Paper Review "> Paper Review     </a></li>
                <br/>
          
                
                <li><a href=" http://localhost:4000/categories/RNN Note "> RNN Note     </a></li>
                <br/>
        
    </ul>
</div>
      </div>
      <main class="page-content" aria-label="Content">
        <div class="wrapper">
          <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML">
  MathJax.Hub.Config({
    "HTML-CSS": {
      availableFonts: ["TeX"],
    },
    tex2jax: {
      inlineMath: [['$','$'],["\\(","\\)"]]},
      displayMath: [ ['$$','$$'], ['\[','\]'] ],
    TeX: {
      extensions: ["AMSmath.js", "AMSsymbols.js", "color.js"],
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    showProcessingMessages: false,
    messageStyle: "none",
    imageFont: null,
    "AssistiveMML": { disabled: true }
  });
</script>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Ch5 오차역전파법</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-08-21T14:55:27+09:00" itemprop="datePublished">Aug 21, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="ch5-오차역전파법">Ch5 오차역전파법</h1>

<p>수치 미분을 통해 기울기를 구하는 방법은</p>

<p>구현이 쉽다는 장접이 있지만 시간이 오래 걸린다는 단점도 있다.</p>

<p>오차역전파법을 사용해 효율적으로 계산하겠다.</p>

<h1 id="51-계산-그래프">5.1 계산 그래프</h1>

<h2 id="511-계산-그래프로-풀다">5.1.1 계산 그래프로 풀다.</h2>

<p>흐름</p>

<ol>
  <li>계산 그래프를 구성한다.</li>
  <li>그래프에서 계산을 왼쪾에서 오른쪽으로 진행한다. (순전파$\mathsf{^{forward\ propagation}}$)</li>
</ol>

<h2 id="512-국소적-계산">5.1.2 국소적 계산</h2>

<p>계산 그래프의 특징은 ‘국소적 계산’을 전파하여 최종 결과를 얻는다.</p>

<p>따라서 전체 계산이 복잡하더라도 각 단계에서 하는 일은 해당 노드의 ‘국소적 계산’이기 때문에</p>

<p>계산이 간단하지만 그것들이 모여 복잡한 계산을 해낸다.</p>

<h2 id="513-왜-계산-그래프">5.1.3 왜 계산 그래프?</h2>

<ol>
  <li>국소적 계산을 통해 문제를 단순화할 수 있다.</li>
  <li>중간 계산 결과를 모두 보관할 수 있다.</li>
  <li>역전파( back propagation )를 통해 ‘미분’을 효율적으로 계산할 수 있다.</li>
</ol>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_0.jpg" alt="ch5_0" /></p>

<h1 id="52-연쇄법칙">5.2 연쇄법칙</h1>

<h2 id="521-계산-그래프의-역전파">5.2.1 계산 그래프의 역전파</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_1.jpg" alt="ch5_1" /></p>

<p>신호 E에 노드의 국소적 미분 $\partial y\over \partial x$를 곱한 후 다음 노드로 전달한다.</p>

<h2 id="522-연쇄법칙이란">5.2.2 연쇄법칙이란</h2>

<p>합성 함수 : 여러 함수로 구성된 함수.</p>

\[z=(x+y)^2\\
\downarrow\\
z=t^2\\
t=x+y\]

<p>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</p>

\[{\partial z\over \partial x}={\partial z\over \partial t}{\partial t\over \partial x}\]

\[{\partial z\over \partial x}={\partial z\over \cancel{\partial t}}{\cancel{\partial t}\over \partial x}\]

<ol>
  <li>국소적 미분(편미분) 계산</li>
</ol>

\[{\partial z\over \partial t}=2t\]

\[{\partial t\over \partial x}=1\]

<ol>
  <li>연쇄법칙$\mathsf{^{chain rule}}$ 적용</li>
</ol>

\[{\partial z\over \partial x}={\partial z\over \partial t}{\partial t\over \partial x}=
2t\cdot1=2(x+y)\]

<h2 id="523-연쇄법칙과-계산-그래프">5.2.3 연쇄법칙과 계산 그래프</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_2.jpg" alt="ch5_2" /></p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_3.jpg" alt="ch5_3" /></p>

<p>역전파 방향으로 국소적 미분을 곱하면 입력에 대한 출력의 미분값을 계산한것과 같다.</p>

<h1 id="53-역전파">5.3 역전파</h1>

<h2 id="531-덧셈-노드의-역전파">5.3.1 덧셈 노드의 역전파</h2>

\[z=x+y\]

\[{\partial z\over\partial x}=1\]

\[{\partial z\over\partial y}=1\]

<p>x, y : 덧셈노드의 입력</p>

<p>z : 덧셈노드의 출력</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_4.jpeg" alt="ch5_4" /></p>

<p>앞에 임의의 계산이 있더라도 연쇄법칙에 의해
덧셈노드의 상류에서 흘러온 미분 값은 $\partial L\over\partial z$가 된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_5.jpeg" alt="ch5_5" /></p>

<h2 id="532-곱셈-노드의-역전파">5.3.2 곱셈 노드의 역전파</h2>

\[z=xy\]

\[{\partial z\over\partial x}=y\]

\[{\partial z\over\partial y}=x\]

<p>곱셈노드의 각 입력에 의한 미분 값은 자신이 아닌 다른 입력값</p>

<p>따라서 입력 x에 의한 출력의 미분 값은 ${\partial L\over\partial z}\cdot y$</p>

<p>입력y에 의한 출력의 미분 값은 ${\partial L\over\partial z}\cdot x$</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_6.jpeg" alt="ch5_6" /></p>

<h2 id="533-사과-쇼핑의-예">5.3.3 사과 쇼핑의 예</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_7.jpeg" alt="ch5_7" /></p>

<h1 id="54-단순한-계층-구현하기">5.4 단순한 계층 구현하기</h1>

<p>계산 그래프를 파이썬으로 구현해보는 절</p>

<h2 id="541-곱셈-계층">5.4.1 곱셈 계층</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MulLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span>
        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span>
        
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</code></pre></div></div>

<h2 id="542-덧셈-계층">5.4.2 덧셈 계층</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span><span class="o">*</span><span class="mi">1</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span><span class="o">*</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</code></pre></div></div>

<h1 id="55-활성화-함수-계층-구현하기">5.5 활성화 함수 계층 구현하기</h1>

<p>활성함수 ReLu와 sigmoid 계층 구현</p>

<h2 id="551-relu-계층">5.5.1 ReLu 계층</h2>

\[y=\begin{cases}
x &amp; (x&gt;0)\\
0 &amp; (x\le0)
\end{cases}\]

\[{\partial y\over\partial x}=\begin{cases}
1 &amp; (x&gt;0)\\
0 &amp; (x\le0)
\end{cases}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_8.jpeg" alt="ch5_8" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReLu</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dout</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<p>ReLu 계층은 전기 회로의 ‘스위치’에 비유할 수 있다.</p>

<p>순전파 때 전류가 흐르고 있으면$(x&gt;0)$ 스위치를 ON으로 하고, 흐르지 않으면$(x\le0)$ OFF로 한다.</p>

<p>역전파 때는 스위치가 ON이라면 전류가 그대로 흐르고, OFF면 더 이상 흐르지 않는다.</p>

<h2 id="552-sigmoid-계층">5.5.2 Sigmoid 계층</h2>

\[y={1\over1+e^{-x}}\]

<p>아래 sigmoid 함수의 계산 그래프를 보면</p>

<p>덧셈 노드, 곱셈 노드, ‘exp’ 노드와 ‘ / ’ 노드가 사용된다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_9.jpeg" alt="ch5_9" /></p>

<h3 id="sigmoid-함수의-역전파-과정">sigmoid 함수의 역전파 과정</h3>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_10.png" alt="ch5_10" /></p>

<h3 id="step-1-결과---partial-loverpartial-ycdot-y2">step 1 결과 : $-{\partial L\over\partial y}\cdot y^2$</h3>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;={1\over x}\\
{\partial y\over\partial x}&amp;=-{1\over\ x^2}\\
&amp;=-y^2
\end{align*}\]

<h3 id="step-2-결과---partial-loverpartial-ycdot-y2">step 2 결과 : $-{\partial L\over\partial y}\cdot y^2$</h3>

<p>덧셈 노드는 상류의 값을 여과 없이 하류로 내보낸다.</p>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;=x+1\\
{\partial y\over\partial x}&amp;=1
\end{align*}\]

<h3 id="step-3-결과---partial-loverpartial-ycdot-y2cdot-e-x">step 3 결과 : $-{\partial L\over\partial y}\cdot y^2\cdot e^{-x}$</h3>

<p>exp 노드의 편미분값은 원래 exp와 같다.</p>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;=e^x\\
{\partial y\over\partial x}&amp;=e^x
\end{align*}\]

<p>주의할 점은 step 3에서 exp노드의 입력값이 $e^{-x}$이므로</p>

<p>역전파 계산시 exp노드를 통과하면 국소적 미분값으로 $e^{-x}$를 곱해야 한다.</p>

<h3 id="step-4-결과--partial-loverpartial-ycdot-y2cdot-e-x">step 4 결과 : ${\partial L\over\partial y}\cdot y^2\cdot e^{-x}$</h3>

<p>곱셈 노드의 역전파 계산은 순전파 때의 값을 서로 바꿔 곱한다.</p>

<p>ex&gt;</p>

\[\begin{align*}
y&amp;=x\cdot c\\
{\partial y\over\partial x}&amp;=c
\end{align*}\]

<h3 id="최종-결과">최종 결과</h3>

\[\begin{align*}
{\partial L\over\partial y}\cdot y^2\cdot e^{-x}=&amp;{\partial L\over\partial y}{1\over (1+e^{-x})^2}e^{-x}\\
=&amp;{\partial L\over\partial y}{1\over1+e^{-x}}{e^{-x}\over1+e^{-x}}\\
=&amp;{\partial L\over\partial y}\cdot y(1-y)\\
&amp;(\because y={1\over1+e^{-x}})
\end{align*}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_11.jpeg" alt="ch5_11" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h1 id="56-affinesoftmax-계층-구현">5.6 Affine/Softmax 계층 구현</h1>

<h2 id="벡터의-미분"><a href="http://kchanghun.github.io/math%20note/2022/08/21/벡터의-미분.html">*벡터의 미분</a></h2>

<h2 id="561-affine-계층">5.6.1 Affine 계층</h2>

<p>신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 어파인 변환$\mathsf{^{affine\ transformation}}$이라고 한다.</p>

<p>Affine 계층의 계산 그래프 : 변수가 행렬임에 주의, 각 변수의 형상을 변수명 위에 표기했다.</p>

<p>이제부터는 노드 사이에 벡터와 행렬도 흐른다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_12.png" alt="ch5_12" /></p>

<ol>
  <li></li>
</ol>

\[\begin{align*}{\partial L\over\partial X}=&amp;{\partial L\over\partial Y}{\partial Y\over\partial X}\\{\partial Y\over\partial X}=&amp;{\partial (X\cdot W+B)\over\partial X}={\partial (X^TW)\over\partial X}=W^T\\&amp;(\ \because Y=X\cdot W+B,\ X\mathsf{\ is\ vector,\ }W\ \mathsf{is\ matrix\ },\ B\ \mathsf{is\ constant\ })\\\therefore {\partial L\over\partial X}=&amp;{\partial L\over\partial Y}\cdot W^T\end{align*}\]

<ol>
  <li></li>
</ol>

\[\begin{align*}{\partial L\over\partial X}=&amp;
{\partial L\over\partial Y}
{\partial (X\cdot W)\over\partial X}={\partial (X^TW)\over\partial X}=
W^T\\&amp;
(\ \because Y=X\cdot W+B,\ X\mathsf{\ is\ vector,\ }W\ \mathsf{is\ matrix\ },\ B\ \mathsf{is\ constant\ })\\\therefore {\partial L\over\partial X}=&amp;{\partial L\over\partial Y}\cdot W^T\end{align*}\]

<p>dot노드의 역전파는 곱셈 노드의 역전파와 개념이 같은데 계산되는 변수가 다차원 배열이기 때문에 서로 도트곱을 할 수 있도록 차원을 맞춰줘야 한다.</p>

\[\begin{align*}
&amp;{\partial L\over\partial X}=&amp;
{\partial L\over\partial Y}\cdot &amp;W^T\\
&amp;(2,)&amp;(3,)\cdot&amp;(3,2)\\&amp;\\
&amp;{\partial L\over\partial W}=&amp;
X^T\cdot&amp;{\partial L\over\partial Y}\\
&amp;(2,3)&amp;(2,1)\cdot&amp;(,3)
\end{align*}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_13.jpg" alt="ch5_13" /></p>

<h2 id="562-배치용-affine-계층">5.6.2 배치용 Affine 계층</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_14.jpg" alt="ch5_14" /></p>

<p>순전파의 편향 덧셈은 각각의 데이터( 1 번째 데이터, 2번째 데이터, …)에 더해진다.</p>

<p>그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Affine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">original_x_shape</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">original_x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">original_x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h2 id="563-softmax-with-loss-layer">5.6.3 Softmax-with-Loss Layer</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_15.jpg" alt="ch5_15" /></p>

<p>손글씨 숫자 인식에서의 softmax 계층의 출력의 모습인데</p>

<p>0~9까지 10개의 숫자를 분류하기 때문에 입력도 10개고 출력도 10개다</p>

<p>softmax계층을 cross entropy error계층과 함께 구현한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_16.png" alt="ch5_16" /></p>

<h2 id="forward-propagation">Forward Propagation</h2>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_17.png" alt="ch5_17" /></p>

<h3 id="1-softmax">1. Softmax</h3>

\[\begin{align*}
y_k&amp;={e^{a_k}\over\sum\limits_i^ne^{a_i}}\\
S&amp;=e^{a_1}+e^{a_2}+e^{a_3}\\
y_1&amp;={e^{a_1}\over S}\quad y_2={e^{a_2}\over S}\quad y_3={e^{a_3}\over S}
\end{align*}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_18.png" alt="ch5_18" /></p>

<h3 id="2-cross-entropy-error">2. Cross Entropy Error</h3>

\[L=-\sum\limits_kt_k\log(y_k)\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_19.png" alt="ch5_19" /></p>

<h2 id="back-propagation">Back Propagation</h2>

<h3 id="1-cross-entropy-error">1. Cross Entropy Error</h3>

\[y=\log x\\
{\partial y\over\partial x}={1\over x}\]

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_20.png" alt="ch5_20" /></p>

<h3 id="2-softmax">2. Softmax</h3>

<p>step 1</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_21.png" alt="ch5_21" /></p>

<p>step 2</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_22.png" alt="ch5_22" /></p>

<p>step 3</p>

<p>순전파 때 여러 갈래로 나뉘어 흘렸다면 역전파 때는 그 반대로 흘러온 여러 값을 더한다.</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_23.png" alt="ch5_23" /></p>

<p>step 4</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_24.png" alt="ch5_24" /></p>

<p>step 5</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_25.png" alt="ch5_25" /></p>

<p>step 6</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_26.png" alt="ch5_26" /></p>

<p>결과</p>

<p><img src="/assets/img/DeepLearning_from_scratch/Ch5/ch5_27.png" alt="ch5_27" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SoftmaxWithLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">t</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h1 id="57-오차역전파법-구현하기">5.7 오차역전파법 구현하기</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>\
                    <span class="n">weight_init_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> \
                            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> \
                            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Relu1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ReLu</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span> <span class="o">=</span> <span class="n">SoftmaxWithLoss</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">accuracy</span>
    
    <span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">loss_W</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">grads</span>
    
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">dout</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">dout</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
        
        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">layers</span><span class="p">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">dout</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
            
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">].</span><span class="n">dW</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">].</span><span class="n">db</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">].</span><span class="n">dW</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">].</span><span class="n">db</span>
        
        <span class="k">return</span> <span class="n">grads</span>
</code></pre></div></div>

<h2 id="573-오차역전파법으로-구한-기울기-검증하기">5.7.3 오차역전파법으로 구한 기울기 검증하기</h2>

<p>기울기를 구하는 방법으로</p>

<ol>
  <li>수치 미분</li>
  <li>해석적 미준</li>
</ol>

<p>두가지를 알았다.</p>

<p>계산 그래프를 통해 해석적 미분을 계산하면서</p>

<p>느린 수치 미분보다 효율적 결과를 얻을 수 있다.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>수치 미분</th>
      <th>해석적 미분</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>속도</td>
      <td>느림</td>
      <td>빠름</td>
    </tr>
    <tr>
      <td>구현 난이도</td>
      <td>쉬움</td>
      <td>어려움</td>
    </tr>
  </tbody>
</table>

<p>때문에 수치 미분 값과 오차역전파법의 결과를 비교하여 제대로 구현 되었는지 확인한다. → 기울기 확인$\mathsf{^{gradient\ check}}$</p>

  </div><a class="u-url" href="/deeplearning%20from%20scratch/2022/08/21/Ch5-BackPropagation.html" hidden></a>
</article>

        </div>
      </main>
    </div><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Humans think like Machines, Machines think like Humans</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Chang Hun Kang</li><li><a class="u-email" href="mailto:abcd877287@gmail.com">abcd877287@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/Kchanghun"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Kchanghun</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>There is No Free Knowledge when we study something.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
