---
layout: post
title: "ResNet"
date: 2022-08-20 03:37:25 +0900
category: [Paper Review]
---
# ResNet

# Deep Redisual Learning for Image Recognition

# 1. Introduction

깊은 CNN들은 이미지 분류의 돌파구를 만든다.

깊은 네트워크들은 end-to-end 다층 구조에서 low/mid/high 수준의 특징과 분류기들을 통합하는 본성이 있고
층이 쌓일수록(깊이가 깊어질수록) 특징의"단계"는 풍부해진다.

최근 밝혀진 바에 의하면 네트워크의 깊이는 아주 중요하고 까다로운 ImageNet 데이터셋의 선두 결과들은 모두 “매우 깊은" 모델들(16~30 깊이의 모델)을 활용한다고 한다.

다른 많은 nontrivial visual recognition 작업들도 매우 깊은 모델들로부터 좋은 결과를 얻을 수 있다.

깊이의 중요성으로부터 생긴 질문 : 
층을 더 많이 쌓는다고 네트워크가 더 좋은 학습을 하는가?

이 질문을 해결할 때 만나는 장애물은 유명한 문제인 vanishing/exploding gradients(처음부터 수렴을 방해하는 문제)이다.

그러나, normalize된 초기값과 네트워크 중간에 normalization 층을 끼워넣는 방법으로
(수십개의 층을 가진 네트워크가 역전파 방법을 통해 SGD optimizer로 수렴을 시작할 수 있도록 하는 방법)
이 문제는 크게 해결이 되어왔다.

깊은 네트워크들이 수렴하기 시작할 때,
degradation 문제가 생긴다 : 
네트워크의 깊이가 증가하면서 정확도가 (당연하게도)포화되는것인데
그렇게 되면 점점 정확도가 낮아진다.

뜻밖에도, 이러한 degradation은 과적합에 의한 문제가 아니고
적당히 깊은 모델이 더 깊어지도록 층을 추가하는 행위가 높은 training error 를 갖게하는 것이다.

![ResNet0](/assets/img/Paper_Review/ResNet/ResNet0.png)

(학습 정확도의) degradation은 모든 시스템들이 optimize하기 쉬운건 아니라는 것을 보여준다.

우리는 얕은 구조와 그것을 복사하고 층을 몇개 추가한 모델을 고려했다.

구조에 있어서 깊은 모델에 solution이 존재한다 :
추가된 층들은 identity mapping이고
다른 층들은 학습된 얕은 모델을 복사하는 것이다.

이 구성으로 얻은 해결책에 의해
더 깊은 모델이 얕은 것보다 더 높은 훈련 오류를 발생해서는 안 된다는 것을 나타낸다.

하지만 실험에서 보여지듯 현재로써
이 구조로 인한 해결책과 비교해서 더 좋은 해결책은 찾지 못했다.

이 논문에서, degradation문제를 해결하기 위해 ‘deep residual learning framework’를 소개할 것이다.

쌓인 층들이 각각 직접 바라는 층과 mapping 되는것을 바라기 보다, 이 층들이 residual mapping에 fit 되도록 했다.

공식적으로, desired underlying mapping을 $H(x)$라고 하고, 비 선형 층들을 쌓은 묶음이 $F(x):=H(x)-x$와 mapping되게 했다.

원래 mapping은 $F(x)+x$로 변환된다.

residual mapping을 optimize하는 것이
original mapping보다 optimize하는 것 보다 더 쉽다.

극단적으로, 만약 하나의 identity mapping이 최적이라면, 비선형 층을 쌓은 identity mapping을 fit 시키는것보다 residual을 0으로 만드는게 더 쉬울 것이다.

$F(x)+x$라는 식은 “shortcut connections”를 통해 신경망의 feedforward에 의해 알 수 있을 것이다.

![ResNet1](/assets/img/Paper_Review/ResNet/ResNet1.png)

Shortcut connections는 하나 이상의 층을 skip하는 것이다.

우리의 경우, shortcut connections는 단순히 identity mapping을 수행하고 그것들의 출력은 stacked layers의 출력에 더해진다.

Identity shortcut connections는 추가 매개변수도 없고 추가적인 계산복잡도도 없다.

전체 네트워크는 여전히 처음부터 끝까지 역전파와 SGD로 학습이 가능하고
라이브러리를 변형시키지 않고 쉽게 구현할 수 있다.

ImageNet에서 포괄적인 실험을 하여 degradation 문제도 증명하고 우리의 방법도 평가했다.

우리는 두가지를 증명한다

1. extremely deep residual net은 optimize가 쉽지만
단순히 층을 쌓아 만든 같은 네트워크는 깊이가 깊어지게 되면 학습오차가 커진다는 것
2. deep residual net은 증가된 깊이에서 쉽게 정확도를 얻을 수 있어
이전 네트워크들과 비교해 더 나은 결과를 제공한다.

비슷한 현상들을 CIFAR-10에서 볼 수 있다,
그러나 최적화 어려움과 우리 방법의 효과는 특정 데이터셋과 연관이 있지 않다.

100개 이상의 층을 가진 모델로 이 데이터를 학습시키고,
1000개 이상의 층을 가진 모델을 연구해 볼 것이다.

ImageNet 분류 데이터셋에서, extremely deep residual net을 통해 훌륭한 결과를 얻었다.

152층의 residual net은 VGG net보다 낮은 복잡도를 가지면서 ImageNet에 보고된 네트워크들 중에서 가장 깊은 네트워크이다.

ImageNet test set에서 조합을 통해 3.57%의 top-5 error를 얻었고 ILSVRC-2015 classification 대회에서 1등을 차지했다.

extremely deep한 표현은 generalization성능도 뛰어나다(다른 인식 작업에서도),
그리고 ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO2015 competitions.

따라서 residual learning principle은 vision problem과 non-vision problem에 모두 일반적으로 사용될 수 있을것이다.

# 2. Related Work

## Residual Representations

이미지 인식에서, VLAD는 Residual vector를 dictionary로 인코딩 하는 표현법이고
Fisher Vector는 VLAD를 확률적으로 공식화한 것이다.

둘 다 이미지 retrieval과 분류에서 강력한 얕은 표현 방법이다.

vector 양자화에서 residual vector의 인코딩은
원래 vector의 인코딩보다 더 효과적이다.

 Partial Difference Equations 분야에 low-level vision과 computer graphics에서 널리 사용되는 Multigrid method는 문제를 시스템을 여러개의 크기의 subproblem으로 나눈다.

Multigrid의 대체 방안은 hierarchical basis preconditioning이다.(두 scale사이의 residual vector를 표현하는 변수에 의존하는 preconditioning)

이 방법은 표준적인 것과 비교해 더 빠르게 수렴한다.

이 방법들은 좋은 reformulation과 preconditioning은 optimization을 단순화 시킨다고 한다.

## Shortcut Connections

shortcut connections를 말하는 실험들과 이론들은 오랫동안 연구되어왔다.

MLP의 초기 연구는 선형층이 네트워크의 입력부터 출력까지 구성하도록 하는 것이다.

몇개의 중간 층들은 vanishing/exploding gradient문제를 다루기 위해 보조 분류기에 직접 연결된다.

몇몇 논문들은 shortcut connections를 구현해서 레이어 반응, gradients, 전파 오류를 중심으로 하는 방법을 제안한다.

“inception” 층은 shortcut 가지와 몇개의 deeper 가지로 구성된다.

이 논문과 동시에, “highway networks”는 gating 기능이 있는 shortcut connections를 보여준다.

이 gate들은 데이터에 종속적이고 가중치가 있다,
반대로 identity shorcut은 가중치가 필요없다.

gated shortcut이 닫히면(0이되는 순간),
highway networks의 층들은 non-residual 기능을 보인다.

반대로, 우리의 식은 항상 residual 기능을 학습한다;
identity shortcut은 절대 닫히지 않는다,
그리고 모든 정보들은 항상 학습되어야 하는 추가적인 residual 함수들을 통과한다.

추가로, highway networks는 극도로 증가된 깊이(100이상 깊이)로 인한 정확도 증가를 증명하지 못했다.

# 3. Deep Residual Learning

## 3.1 Residual Learning

몇개의 층이 쌓여 mapping되는 함수를 $H(x)$라고 하자,
$x$는 이 층들의 입력값이다.

만약 여러개의 비선형 층들로 복잡한 기능을 구현할 수 있다면,
마찬가지로 residual function으로도 복잡한 기능을 할 수 있을 것이다.  $H(x)-x$ 

그러니 쌓여있는 층들이 $H(x)$에 비슷해지는게 아니라
우리는 $F(x):=H(x)-x$가 되게 할 것이다.

그러므로 원래 함수는 $F(x)+x$가 될것이다.

두가지 형식 모두 요구되는 기능으로 근사될 것이지만,
학습 난이도는 다를 것이다.

이 식은 degradation문제에서 counterintuitive phenomena로부터 영감을 받은 것이다.

앞서 말했듯, 추가적인 층들이 identity mapping 구조를 띈다면,
더 깊은 모델은 복사본인 얕은 모델보다 학습오차율이 더 클 수 없다.

degradation문제는 solver가 여러개의 비선형층으로 이루어진 구조에서 identity mapping을 유추하는데에 어려움을 줄것이다.

residual learning reformulation에서
만약 identity mapping이 최적이라면,
solver들은 다층 비선형 구조가 identity mapping이 0을 향해 근접할 수 있도록 가중치를 움직일 것이다.

실제로는, identity mapping은 최적이지만 우리의 reformulation은 문제가 발생하도록 돕는 것이된다.

만약 최적의 함수가 zero mapping이 아니라 identity mapping이라면,
새로운 함수 하나를 학습하는 것 보다 identity mapping으로 방해요인을 찾는게 더 쉬울 것이다.

우리는 identity mappings가 합리적인 preconditioning을 제공하기 때문에
실험적으로 학습된 residual 함수들이 일반적으로 반응이 작다는 것을 증명했다,

## 3.2 Identity Mapping by Shortcuts

우리는 모든 stacked layers에 residual learning을 도입했다.

공식적으로 이번 논문에서 우리가 만든 block은 아래와 같다

$$
y=F(x,\left\{W_i\right\})+x
$$

x랑 y는 각각 입력과 출력 벡터이다.

함수 F(x,{Wi})는 학습 되어야 하는 residual mapping을 의미한다.

예를들어 

![ResNet1](/assets/img/Paper_Review/ResNet/ResNet1.png)

이 구조는

$$
F=W_2\sigma(W_1\mathbf{x})
$$

가 되고 여기서 $\sigma$는 ReLU이고 편향은 표기의 단순화를 위해 제거했다.

수식 $F+x$는 shortcut connection에 의해 수ㅐㅎㅇ되고
element-wise addition이다.

두번째 비선형성은 $F+x$ 후에 계산된다.(i.e., $\sigma(y))$ 

앞에서 봤듯 shortcut connection은 계산 복잡도도 안올라가고 추가적인 가중치도 필요없다.

실로 매력적일 뿐 아니라 plain network와 residual network의 비교에 중요하다.

공정하게 plain과 residual net을 비교할 것이다.
같은 가중치 수, 깊이, 크기 그리고 element-wise addition을 제외한 계산비용까지 모두 같게하여 비교할 것이다.

x와 F의 차원은 반드시 같아야 한다.

만약 이런 경우가 아니라면(예를들어 입력과 출력의 채널을 변경하는 것),
우리는 shortcut connections의 차원을 맞추기 위해 $W_s$를 사용해 linear projection을 할 것이다.

$$
y=F(x,\{W_i\})+W_sx
$$

또한 정방행렬 $W_s$를 사용할 수도 있다.

하지만 우리는 실험적으로 identity mapping이 충분히 degradation문제를 다룰 수 있고 경제적임을 보일 것이다.(따라서 $W_s$는 차원을 맞추기 위해서만 사용될 것이다.)

residual function F는 유연하다.

이 논문의 실험에는 두세층이 포함된 F를 실험한다.
(더 많은 층들도 가능하다)

하지만 만약 F가 단층이라면 

$$
y=F(x,\{W_i\})+x
$$

는 선형 식이 될것이다.

$$
y=W_1x+x
$$

위 식에서는 어떤 이점도 찾을 수 없다.

또한 우리는 단순함을 위해 전결합을 사용했지만,
residual은 합성곱 층에서도 사용 가능하다.

$F(z,\{W_i\})$는 여러 합성곱층을 표현할 수 있다.

element-wise addition은 두 feature map 사이에서 이루어지고
channel별로 이루어진다.

## 3.3 Network Architectures

다양한 plain/residual net들을 실험해보고
일관적인 현상들을 관찰했다.

사례를 위해 ImageNet을 위한 두가지 모델을 설명하겠다.

## Plain Network

VGG net의 이론에서 영감을 받아 plain 네트워크 구조를 만들었다.

합성곱층은 최대 3x3크기의 필터와 두가지 간단한 규칙을 따른다.

1. output feature map size가 동일하기 위해,
층들은 같은 수의 filter를 갖는다.
2. 만약 feature map size가 반으로 준다면,
filter의 수를 두배로 해서 층마다 시간 복잡도를 유지한다.

downsampling은 합성곱층에서 stride를 2로 설정해서 수행한다.

네트워크는 global average pooling layer와 softmax를 사용하는 1000-way 전결합층으로 마무리된다.

가중치층의 총 수는 34개이다.

Plain Network는 VGG net보다 필터 수도 적고 복잡성도 낮다는것은 주목할 만하다.

34개의 층들은 36억개의 FLOPs(multiply-add)가 필요하고
이는 196억개의 FLOPs인 VGG-19의 18%에 해당하는 수치이다.

![ResNet2](/assets/img/Paper_Review/ResNet/ResNet2.png)

## Residual Network

Plain net을 기반으로, shortcut connections를 추가해 Plain과 같은 부분을 residual 로 만들었다.

왼쪽에서 실선 shortcut connections처럼 입력과 출력의 차원이 같다면 identity shortcut 을 바로 사용할 수 있다.

차원이 증가하는 경우에는 점선으로된 shortcut connections처럼표현하고 두가지 경우를 고려한다.

1. shortcut은 여전히 identity mapping을 수행한다.
zero padding을 통해 차원을 늘려서 사용한다.
이 방법은 추가적인 매개변수가 없다
2. projection shortcut은 차원을 일치시킬 때 사용된다.(1x1합성곱으로 완성)

두개의 크기인 feature map을 통과할 때 두가지 경우 모두 stride=2로 수행한다.

## 3.4 Implementation

이미지는 scale augmentation을 위해 256~480에서 임의로 선택된 값으로 짧은 부분을 resize한다.

224,224 crop은 이미지로부터 임의로 sample을 구하거나
horizontal flip(각 pixel에 평균값을 빼는 것까지)을 통해서 도 sample을 취한다.

표준 color augmentation도 사용되었다.

convolution연산 뒤에 activation연산 전에 BatchNormalization(BN)을 사용했다.

가중치는 13번 논문을 따라서 초기화시켰고
네트워크는 처음부터 학습시켰다.

SGD를 mini-batch size 256으로 사용했다

Learning rate는 0.1부터 시작해서 error plateau일 때마다10씩 나눴다.

학습은 600000번만큼 iteration했다.

weight decay는 0.0001을 사용하고
momentum계수는 0.9로 했다.

Dropout층은 사용하지 않았다.

test에서는 연구의 비교를 위해 표준적으로 10-crop testing을 했다.

최고의 결과를 위해 fully-convolutional form을 사용했고
이미지의 짧은 축의 크기를 224,256,384,480,640으로 설정한 여러개의 크기에서 계산을 하고 결과를 평균내었다.

# 4. Experiments

## 4.1 ImageNet Classification

네트워크 평가를 위해 1000개의 클래스로 구성된 ImageNet 2012 classification dataset을 사용했다.

모델들은 128만개의 학습 이미지를 통해 학습하고
5만개의 validation 이미지로 평가됐다.

또한, 최종 결과는 10만개의 test image를 통해 얻었ek.

top-1, top-5 error rate를 계산했다.

### Plain Networks

18-layer와 34-layer를 평가했다.

![ResNet3](/assets/img/Paper_Review/ResNet/ResNet3.png)

Downsampling은 conv3_1,conv4_1,conv5_1에서도 stride 2로 진행된다

아래 결과를 보면 더 깊은 34-layer가 얕은 18-layer보다 validation error가 더 큰 것을 알 수 있다.

![ResNet4](/assets/img/Paper_Review/ResNet/ResNet4.png)

얇은 선은 training error, 굵은 선은 validation error

이유를 알아보기 위해 학습과정에서 training/validation error를 확인해 보니 degradation 문제가 생긴것을 확인했다.

왜냐하면 학습 전체 과정에서 34-layer의 학습오차가 18-layer보다 항상 높았기 때문이다.
심지어 18-layer의 해공간이 34-layer의 해공간의 하위 집합이었음에도 이러한 결과가 나타났다.

이런 optimize difficulty는 불행하게도 vanishing gradient 때문에 발생한다.

plain net들은 전파 과정에서 0이 아닌 분산값을 갖게 하도록
BN을 사용했다.

또한, 역전파에서 gradient들은 BN을 통해 좋은 정규화가 이루어진 것을 증명할 수 있다.

따라서 순전파와 역전파 모두 신호를 소실시키지 않았다.

사실 34-layer plain net은 여전히 경쟁적인 정확도를 달성할 수 있다. (solver가 어느정도 작동한다면)

deep plain net들은 매우 낮은 비율로 수렴하기 때문에 training error를 낮추는데 영향을 준다.

이러한 optimization difficulties는 후에 연구될 것이다.

### Residual Networks

다음으로 18-layer와 34-layer residual nets(ResNets)를 평가했다.

기본 구조는 plain net과 같고 3x3 filter쌍마다 shortcut connection이 추가 되어있다.

아래 비교에서 모든 shortcut 에 identity mapping을 적용했고
차원을 높이기 위해 zero-padding을 했다.

따라서 plain net과 비교해 추가적인 매개변수가 없다.

![ResNet5](/assets/img/Paper_Review/ResNet/ResNet5.png)

얇은 선은 training error, 굵은 선은 validation error

![ResNet6](/assets/img/Paper_Review/ResNet/ResNet6.png)

위 결과로부터 3가지 주요 관찰할 것을 얻었다.

### Residual 학습을 통해 상황이 반대가 되었다.

residual 학습으로 34-layer의 결과가 18-layer의 결과보다 좋게 나왔다.

더 중요한 것은 34-layer ResNet은 상당히 낮은 training error를 보이고 validation data에 범용성을 갖췄다.

이것은 degradation문제가 이번 setting으로 잘 해결이 되고
또 깊은 구조의 네트워크로부터 높은 정확도를 얻었다는 것을 의미한다.

### plain net과 ResNet을 비교했을 때,
34-layer ResNet은 top-1 error가 줄었다는 것이다.

![ResNet7](/assets/img/Paper_Review/ResNet/ResNet7.png)

![ResNet8](/assets/img/Paper_Review/ResNet/ResNet8.png)

![ResNet9](/assets/img/Paper_Review/ResNet/ResNet9.png)

이러한 비교는 extremely deep system에서 residual learning의 효과를 입증한다.

마지막으로, 18-layer Plagin/Residual net들은 비슷한 정확도를 보이지만
18-layer ResNet이 더 빠르게 수렴한다.

network가 지나치게 깊지 않은 경우(여기서는 18-layer)
SGD는 계속해서 좋은 solution을 Plain net에 찾아줄 수 있다.

이 경우, ResNet은 더 빠른 단계에서 수렴을 하도록 하기 때문에 optimization이 쉬워진다.

### Identity vs. Projection Shorcuts

추가 매개변수가 없는것과 identity shorcuts는 학습을 돕는다고 증명했다.

다음으로는 projection shortcut을 살펴보겠다.

![ResNet7](/assets/img/Paper_Review/ResNet/ResNet7.png)

Table 3에서 세가지 옵션들을 비교했다.

(A) 차원의 증가를 위해 zero-padding을 사용하고
모든 shortcut을 identity mapping으로 구현해 추가 매개변수가 없게 함.

(B) projection shortcut을 사용해 차원을 증가시키고
다른 shortcut들은 identity를 사용했다.

(C) 모든 shortcut들을 projection으로 사용했다.

Table 3에서 볼 수 있듯이 세가지 옵션 모두 plain net과 비교해 상당히 좋은 결과를 만든다.

B가 A보다 약간 더 좋은데
A에서 zero-padding으로 늘어난 차원이 사실 residual learning이 아니기 때문이다.

C는 B보다 조금 더 좋은데
projection shortcut에 의해 매개변수가 더 추가 되었기 때문이라고 생각한다.

하지만, A/B/C 사이에 차이가 작은 것은 projection shortcut이 degradation 문제를 다루는데 필수적인 것은 아니라는 것을 나타낸다.

따라서 이 논문의 나머지 부분에서는 C를 사용하지 않는다.
(메모리사용량과 시간 복잡도와 모델의 크기를 줄이기 위해)

Identity shortcut들은 bottleneck 구조의 복잡도를 증가시키지 않기 때문에 특히 중요하다.

### Deeper Bootleneck Architectures

다음으로 더 깊은 구조를 보겠다.

사용할 수 있는 학습시간에 대한 염려로 인해,
building block을 bottleneck 구조로 변경했다.

(non-bottleneck ResNet도 좋은 정확도를 보이지만
경제적이지 못해 bottleneck ResNet을 사용한다.
따라서 bottleneck ResNet을 사용하는 이유는 practical consideration 때문인 것이다.)

각 residual function F에 층을 3개씩 샇았다.

3개의 층은 1x1,3x3,1x1 합성곱을 한다.

1x1 합성곱은 차원을 줄였다가 다시 키우는 역할을 하며
3x3 합성곱은 작은 차원의 입출력에 대해 병목현상을 일으킨다.

bottleneck과 non-bottleneck 모두 시간 복잡도는 비슷하다.

![ResNet10](/assets/img/Paper_Review/ResNet/ResNet10.png)

매개변수가 추가되지 않는 identity shortcuts는 병목 구조에서
특히 더 중요하다.

만약 identity shortcut이 projection으로 대체된다면,
시간복잡도와 모델 크기가 두배가 될것이다.
마치 shortcut이 두개의 높은 차원과 연결된것처럼

따라서 identity shortcut은 bottleneck 구조를 더 효율적으로 만든다.

### 50-layer ResNet

34-layer net에서 두개의 layer block을 3-layer bottleneck block으로 대체했다.(그 결과는 Table1의 50-layer ResNet이다.)

옵션B를 사용해 차원을 키웠고
이 모델의 FLOPs는 38억이다.

### 101-layer and 152-layer ResNets

101-layer과 152-layer ResNet을 만들었다(3-layer block을 더 사용하여)

깊이가 충분히 증가했지만,
152-layer ResNet(113억 FLOPs)은 여전히 VGG-16/19(153억/196억 FLOPs)보다 복잡도가 낮았다.

50/101/152-layer ResNet은 margin을 고려한 34-layer보다 더 정확하다.

degradation을 관찰하지 못했으므로
상당히 깊은 네트워크로부터 충분히 높은 정확도를 얻은 것이다.

모든 평가겨로가에서 깊이에 대한 이점이 발견되었다.

### Comparisons with State-of-the art Methods

![ResNet8](/assets/img/Paper_Review/ResNet/ResNet8.png)

![ResNet9](/assets/img/Paper_Review/ResNet/ResNet9.png)

위 결과에서 보듯 이전에 사용하던 최고의 단일-모델과 비교했다.

34-layer ResNet이 좋은 결과를 보였다.

152-layer ResNet은 단일-모델로써 top-5 validation error값이 4.49%가 나왔다.

테이블5에서 보이는 여러 모델을 조합한 결과들과 비교가 가능할 정도로
단일 모델인 152-layer ResNet은 매우 뛴어나다.

ensemble을 구성하기 위해 서로다른 깊이의 6개의 모델들을 조합했다.
(제출할 때에는 152-layer 두개만 조합했다.)

결과는 top-5 error값으로 3.57%가 나왔고
이 결과로 ILSVRC-2015 에서 1등을 차지했다.

## 4.2 CIFAR-10 and Analysis

CIFAR-10(10개의 클래스를 갖는 5만개의 학습 1만개의 테스트이미지)에서 더 많은 연구를 해보았다.

우리의 초점은 최첨단 구조에서 보이는 결과를 얻는 것이 아니기 때문에
extremely deep network의 구조를 간단하게 구성했다.

입력으로 32x32을 받는다(각 픽셀은 평균값들을 뺀 상태이다)

첫 번째 층은 3x3 합성곱층이다.

그리고나서 6n개의 3x3 합성곱 층을 쌓아 특징맵의 크기가 32,16,8에 맞게 할당을 해서
각 특징맵별로 2n개의 합성곱 계산을 해야한다.

subsampling은 stride2인 합성곱으로 진행했다.

네트워크의 마지막 부분은 global average pooling과 10-way fully-connected layer with softmax를 사용했다.

따라서 총 가중치층은 6n+2가 된다.

![ResNet11](/assets/img/Paper_Review/ResNet/ResNet11.png)

shortcur connection이 사용되면,
그것들은 3x3 합성곱 두개에 하나씩 연결되어 총 3n개의 shortcut이 생긴다.

CIFAR-10에는 모든 경우에 identity shortcut(option A)을 사용해서
plain 모델과 비교해 깊이, 크기, 매개변수의 수가 모두 같다.

이 때 weight decay 는 0.0001
momentum계수는 0.9
그리고 dropout은 사용하지 않지만 BN을 사용했다.

batch-size는 128로하고
두개의 GPU에서 학습을 진행했다.

learning rate는 0.1로 초기 설정하고 32k와 48k에서 10씩 나눠서 적용했다
그리고 64k iteration에서 학습을 종료했다.(train/val의 크기를 45k/5k로 설정한 결과)

data augmentation은 간단하게하여 학습을 진행했다 :

각 모서리에 4pixel만큼씩 padding을 했고
padding한 이미지와 그것을 뒤집은 이미지로부터 32x32의 크기만큼 임의로 잘랐다.

test를 위해 32x32이미지의 single view만 평가했다.

n={3,5,7,9}를 적용해 20, 32, 44, 56-layer network를 만들었다.

![ResNet12](/assets/img/Paper_Review/ResNet/ResNet12.png)

Highway network를 보면 증가된 깊이로부터 error가 증가하는 것을 볼 수 있다.

이 현상은 ImageNet과 MNIST에서 비슷하게 보여진다.
(모두 optimization difficulty가 문제다)

ImageNet에서와 비슷하게 ResNet은 optimization difficulty를 극복하고
깊어진 깊이로부터 높은 정확도를 얻는다고 증명했다.

게다가 n=18인 경우에 110-layer ResNet을 만드는데

이런 경우, 수렴을 시작하기에 앞서 learning rate가 0.1인 것은 다소 높다고 판단했다.

그래서 training error가 80%아래로 내려갈 때까지 0.01로 학습률을 설정했다.

그리고 나서 학습률을 다시 0.1로 설정하고 계속 학습했다.

110-layer도 잘 수렴했다.

110-layer는 최첨단 구조가 아님에도 다른 깊거나 얕은 구조들 (Fitnet and Highway)보다 더 매개변수가 적다.

![ResNet13](/assets/img/Paper_Review/ResNet/ResNet13.png)

bold : training error, dashed : test error

![ResNet14](/assets/img/Paper_Review/ResNet/ResNet14.png)

bold : training error, dashed : test error

![ResNet15](/assets/img/Paper_Review/ResNet/ResNet15.png)

### Analysis of Layer Responses

![ResNet16](/assets/img/Paper_Review/ResNet/ResNet16.png)

위 그래프에서 표준편차를 확인할 수 있다.

반응들은 각 3x3 layer에 의한 결과이고 BN의 결과이고 activation함수를 통과하기 전의 값이다.

ResNet에서 이런 분석은 residual functions의 response strength를 표출한다.

위 그래프는 ResNet은 일반적으로 plain 구조에 비해 response가 작다고 보여준다.

이러한 결과는 우리의 기본 동기를 뒷받침해준다.

따라서 보통 residual function이 일반적으로 non-residual인것보다 0에 더 가깝다는 것이다.

또한, 깊은 ResNet은 더 작은 크기의 response를 보인다.

더 많은 층이 있을 경우, ResNet의 각 층은 신호를 덜 수정하는 경향이 있다.

### Exploring Over 1000 layers

1000층이 넘는 아주 깊은 모델을 조사해보자.

n=200으로 설정을해서 1202-layer network를 만들었다.
학습은 위와 같은 방법으로 진행했다.

우리의 방법은 optimization difficulty를 보이지 않고
이 1000-layer network는 0.1미만의 학습 오차를 달성할 수 있다.

이것의 test error는 여전히 괜찮은 수준인 7.93%이다.

하지만 여전히 매우 깊은 모델이 갖는 문제는 해결되지 않았다.

1202-layer의 test 결과는 우리의 110-layer 네트워크보다 안좋다.
(비록 두 모델이 비슷한 학습에러를 결과를 보이지만)

이것은 과적합 때문인다.

1202-layer는 이 작은 데이터셋에 불필요하게 많은 층이 사용되었다.

강력한 regularization(e.g.,  maxout, dropout)은 이 데이터셋을 상대로 최고의 결과를 얻기 위해 사용되었다.

그러나 이번 연구에서 maxout과 dropout은 사용하지 않았고
구조적으로 깊고 얕음을 통해 regularization을 부과하고
optimization difficulty에는 초점을 두지 않았다.

하지만, 강력한 regularization의 조합은 결과를 향상시킬것이다.(앞으로 연구해볼 것이다.)

## 4.3 Object Detection on PASCAL and MS COCO

우리의 방법은 다른 recognition 작업에서도 좋은 generalization 성능을 보인다.

![ResNet17](/assets/img/Paper_Review/ResNet/ResNet17.png)

Table7과 Table8 에서 볼 수 있듯이
object detection baseline은 PASCAL VOC 2007 and 2012 and COCO를 통해 평가했다.

detection 방법으로 Faster R-CNN을 채택했다.

우리는 VGG-16을 ResNet-101로 대체하여 개선하는 것에 관심이 있다.

두 모델을 사용하여 detection 구현하는 것은 같다,

따라서 더 좋은 네트워크를 기반으로 이점이 생긴다.

대부분 눈에 띄는 점은, COCO데이터셋에서 6.0% 상승한 값을 얻었다는 것이다.(상대적으로 28% 개선됨)

이런 이점은 단지 learned representation때문인 것이다.

Deep residual net을 기반으로 ILSVRC & COCO 2015 competition에서 몇몇 부문에서 1등을 차지했다 :
ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.